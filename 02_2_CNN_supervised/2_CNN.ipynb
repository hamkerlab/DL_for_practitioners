{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdc71d8-931d-41cf-bdc3-6adb1b015006",
   "metadata": {},
   "source": [
    "# Tutorial 2. How to create a convolutional neural network (CNN)\n",
    "\n",
    "You will notice a big overlap with the implementation from tutorial 1 since the basics are always the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995c7e0",
   "metadata": {},
   "source": [
    "Check and set the available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aa2346-2ee1-4160-a9c3-5aaef5694b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Device is ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c6802",
   "metadata": {},
   "source": [
    "Load and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f154009-dfa2-4537-aae8-d4b055c48b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:00<00:00, 104MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./mnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 1.89MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 33.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 18.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/FashionMNIST/raw\n",
      "\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5),(0.5))])\n",
    "batch_size=4\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./mnist/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./mnist/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# list of class labels\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)\n",
    "# classes = ['0','1','2','3','4','5','6','7','8','9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11015f13-ed06-415c-bfcd-ed8ffe74de09",
   "metadata": {},
   "source": [
    "Plot images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c2cdb6-f1f1-4219-b54f-f34d51fbf1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes are: \n",
      "Shirt Sneaker Sneaker Pullover\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(imges):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        img = imges[i,0]\n",
    "        img = img / 2 + 0.5 #unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(npimg, cmap='gray')\n",
    "    plt.show\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images)\n",
    "\n",
    "print('Classes are: ')\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72614289-c286-49d7-a705-3f86701de016",
   "metadata": {},
   "source": [
    "## Create the deep neural network\n",
    "\n",
    "### The network class\n",
    "\n",
    "As before, we create a class object which inherits from the `torch.nn.Module`class to build our neural network. This includes the layer objects and the feedforward pass.\n",
    "\n",
    "#### Define the network structure\n",
    "\n",
    "We add to two convolutional layers before the neural network from tutorial 1.\n",
    "\n",
    "To add a 2D-convolutional layer, we use the `nn.Conv2D()` object, which needs at least the following three parameters:\n",
    "1. Number of input channels/ feature maps (corresponds to color channels, e.g. RGB)\n",
    "2. Number of output channels / feature maps\n",
    "3. Size of the convolutional kernel as single integer or tuple.\n",
    "(for more information look in the [official PyTorch documentation](ttps://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "\n",
    "As we want to process gray-scaled images, the input only has a single color channel. Therefore, the number of input channels into the first convolutional layer is $1$. We want $6$ feature maps and a kernel size of $5 \\times 5$. In contrast to defining fully-connected layers, the pixel dimensions of the image is not relevant.\n",
    "\n",
    "Creating our first convolutional layer leads to the following code line:\n",
    "\n",
    "`self.conv1 = nn.Conv2d(1,6,5)`\n",
    "\n",
    "With a similar call, we create the second convolutional layer. Here, since we defined 6 output feature maps for the previous layer, we need 6 input channels for this layer.\n",
    "\n",
    "`self.conv1 = nn.Conv2d(6,16,5)`\n",
    "\n",
    "As we want to perform a maximum pooling, we add a pooling layer with the first parameter as the size of the pooling kernel, and the second parameter as the stride of the pooling window:\n",
    "\n",
    "`self.pool = nn.MaxPool2d(2,2)`\n",
    "\n",
    "The last convolutional layer projects onto a fully connected layer where the explicit number of inputs is relevant again. The size of an image after a convolutional layer is:\n",
    "`W-K+2P)/S+1` with input dimensions W, kernel size K, padding P and stride S. With max pooling here, it is halfed again. \n",
    "\n",
    "After the first convolution, the dimension is $(28-5+1)/2=12$ and after the second $(12-5+1)/2=4$. The last convolutional layer has 16 feature maps. Therefore, the number of outputs equals $16 \\times 4 \\times 4$ and represent the number of inputs of the fully-connected layer. \n",
    "\n",
    "`self.fc1 = nn.Linear(16*4*4,120)`\n",
    "\n",
    "Then we add a second fully-connected layer:\n",
    "\n",
    "`self.fc2 = nn.Linear(120,84)`\n",
    "\n",
    "And the classification head with 10 output neurons:\n",
    "\n",
    "`self.fc3 = nn.Linear(84,10)`\n",
    "\n",
    "Please note, that here we only define, which layers we want to use in our network. The order of defining them is not important at the moment.\n",
    "\n",
    "Afterwards, the feedforward pass is defined. A new concept here is the forward pass for the convolutional layer `x = self.pool(F.relu(self.conv1(x)))` where the input is passed through the convolutional layer. Then, the activation layer ReLU is used and, afterwards, max pooling is performed. Also, the output from the convolutional layer needs to be flattened for the fully-connected layer (as the images needed to be in the previous tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a21cd44-90d3-407a-9727-a057bdbdf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## define the network structure with the layers\n",
    "        self.conv1 = nn.Conv2d(1,6,5) # in_channels, out_channels, kernel_size \n",
    "        self.conv2 = nn.Conv2d(6,16,5) # in_channels,out_channels, kernel_size\n",
    "        self.pool  = nn.MaxPool2d(2,2) # kernel_size, stride\n",
    "        self.fc1   = nn.Linear(16*4*4, 120) # in_channels, out_channels\n",
    "        self.fc2   = nn.Linear(120,84) # in_channels, out_channels\n",
    "        self.fc3   = nn.Linear(84,10) # in_channels, out_channels\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## define the functionality of each layer/between the layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107db95-84d9-4245-9837-8fca1722036e",
   "metadata": {},
   "source": [
    "#### Creating the network\n",
    "Now we can simply create a new network object and assign it to the device identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019a9daf-5c5b-4416-a944-026e5dfcdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e5497-d883-484e-8c8d-e8fff01a7031",
   "metadata": {},
   "source": [
    "### Set up loss and optimizer\n",
    "We use the `CrossEntropyLoss`as loss-function on the error between the network output and the correct labels and using the simple\n",
    "Stochastic-Gradient-Desced (`SGD`) optimizer.\n",
    "Please note, that the optimizer needs the parameters of our network `net.parameters()` as input to define which need to be tuned.\n",
    "\n",
    "Further available loss functions and optimizers can be found on the [official PyTorch website](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099c6cef-8eeb-4cc5-8345-8d45b55e0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99440b4e-5ee5-4241-91d0-5421bb124947",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0d589fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlittle_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m timer\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining process\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# label smoothing to reduce overfit. Proper values are 0.01 to 0.2\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of epochs to wait before early stopping \u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# metric to monitor for early stopping. If this metric doesn't improve for the last `patience` epochs, early stopping is triggered\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# path to save the \"best\" model with best value of `monitor`\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/groem/PyTorch_MC/Utils/functions.py:58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, checkpoint_path, patience, min_delta, monitor)\u001b[0m\n\u001b[1;32m     55\u001b[0m total: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Use tqdm for progress bar\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m [Train]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_pbar:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/scratch/groem/miniforge3/envs/pytorch/lib/python3.9/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/groem/miniforge3/envs/pytorch/lib/python3.9/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "# Define your validation split\n",
    "val_split = 0.2  # 20% for validation\n",
    "val_size = int(len(trainset) * val_split)\n",
    "train_size = len(trainset) - val_size\n",
    "\n",
    "# Split into training and validation\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "from Utils.functions import train_model\n",
    "from Utils.little_helpers import timer\n",
    "\n",
    "with timer(\"Training process\"):\n",
    "    history = train_model(model=net,\n",
    "                          train_loader=train_loader,\n",
    "                          val_loader=val_loader,\n",
    "                          criterion=nn.CrossEntropyLoss(label_smoothing=0.1),  # label smoothing to reduce overfit. Proper values are 0.01 to 0.2\n",
    "                          optimizer=optimizer,\n",
    "                          scheduler=scheduler,\n",
    "                          patience=5,  # number of epochs to wait before early stopping \n",
    "                          monitor='val_loss',  # metric to monitor for early stopping. If this metric doesn't improve for the last `patience` epochs, early stopping is triggered\n",
    "                          device=device,\n",
    "                          num_epochs=2,\n",
    "                          checkpoint_path=\"./\",  # path to save the \"best\" model with best value of `monitor`\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9dcfca-b2db-4cc4-bb52-55c8f6de7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.068\n",
      "[1,  4000] loss: 0.179\n",
      "[1,  6000] loss: 0.141\n",
      "[1,  8000] loss: 0.112\n",
      "[1, 10000] loss: 0.102\n",
      "[1, 12000] loss: 0.095\n",
      "[1, 14000] loss: 0.084\n",
      "[2,  2000] loss: 0.051\n",
      "[2,  4000] loss: 0.074\n",
      "[2,  6000] loss: 0.063\n",
      "[2,  8000] loss: 0.053\n",
      "[2, 10000] loss: 0.054\n",
      "[2, 12000] loss: 0.057\n",
      "[2, 14000] loss: 0.048\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the input data and labels\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimze\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch +1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcff51-967f-4713-86f2-d4f4032b872d",
   "metadata": {},
   "source": [
    "After training, we save the parameters of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8601c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './mnist_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d12c015-9066-4b10-8b47-4c8b0d6d33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ede117-4dd8-42c3-a91e-b361e05c5b9b",
   "metadata": {},
   "source": [
    "## Test the network performance\n",
    "We use the testloader object to see, how well the network will recognice the numbers in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdcf93f-1fc2-4f43-88b3-03265626b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93454ce-b165-44e9-851e-5b8fadb91b3b",
   "metadata": {},
   "source": [
    "We create a new network object and load the parameters from the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410efac3-cd14-4eeb-b14a-8d1b9790f26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(path, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da716b-c13c-4dfd-94e6-3cea42dc3ddf",
   "metadata": {},
   "source": [
    "We test the first four images to see how well they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e652f700-819f-4139-a858-625227e76e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net(images)\n",
    "imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca3c57a-f392-4f59-9cea-d855df99c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  7     2     1     0    \n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(output, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27132b2a-fabe-4cfa-baaf-a4bbbc61a046",
   "metadata": {},
   "source": [
    "As this looks very promising, we write a loop to get the data from the testload batchwise and pass them through the network.\n",
    "\n",
    "To avoid further training and to reduce the computational costs, we deactivate the calculation of gradients with `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75da4833-f5e0-4ac0-9738-e8c508782330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 98%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "\n",
    "# use no_grad as we do not want further training\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "\n",
    "        outputs = net(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the test set: {100*correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39ea6d",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a convolutional neural network for the FashionMNIST dataset\n",
    "\n",
    "Similar to the previous tutorial and exercise, implement a CNN for the FashionMNIST dataset. Experiment with layer sizes, number of epochs, number of layers, batch size and kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# immport torch and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda36d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network layer and feedforward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network and define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa557bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781d6b2-bb8b-4a9a-ac1b-f97f9f62ead5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So in this tutorial we have covered:\n",
    "\n",
    "1. How to define a convolutional layer and its forward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
