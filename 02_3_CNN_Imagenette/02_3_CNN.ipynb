{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdc71d8-931d-41cf-bdc3-6adb1b015006",
   "metadata": {},
   "source": [
    "# Tutorial 2.3. How to handle a more realistic dataset\n",
    "\n",
    "Author: [Maren Gr√∂ne](mailto:maren.groene@s2016.tu-chemnitz.de)\n",
    "\n",
    "In this tutorial, we introduce a new and more realistic dataset: [Imagenette](https://github.com/fastai/imagenette).\n",
    "\n",
    "The previous MNIST examples are great for an introduction but are generally easy to solve with an accuracy of up to 99% in classification tasks. Imagenette is a subset of the much larger dataset [ImageNet](https://image-net.org/index.php) which was used in an image classification competition until the AlexNet crushed it in 2012. Nowadays, it is often used to train image classification models from scratch due to its massive image volume and number of classes.\n",
    "\n",
    "For now, this dataset is unnecessarily big for our stage. That is why we use its smaller derivative Imagenette with only 10 destinct classes and 10,000 images. Nonetheless, they are bigger, in RGB colors and real-world images which make them way more difficult to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863c4e3",
   "metadata": {},
   "source": [
    "Before diving in, let us check and set the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f55a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Device is ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b230341",
   "metadata": {},
   "source": [
    "From this notebook onwards, we will use modules defined in the *Utils* folder. You can import them like a Python package by adding the root directory to system path first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add root directory to system path\n",
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "# load packages from Python files    \n",
    "from Utils.dataloaders import prepare_imagenette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c6802",
   "metadata": {},
   "source": [
    "### Image preprocessing \n",
    "\n",
    "Now we implement necessary transformations and load the data as a Dataloader object. For training, it is necessary to convert all images to the same size to pass it to aan always fixed input layer of a neural network. Since $224 \\times 224$ is a common size (for example for the Vision Transformer later) we used it here.\n",
    "\n",
    "We also use two data augmentation techniques: cropping and flipping. Data augmentation is generally used to enrich the dataset with more diverse images which in turn improves the machine learning model. For further transformations, read [transforming and augmenting images - PyTorch Website](https://pytorch.org/vision/main/transforms.html).\n",
    "\n",
    "The functions we used are:\n",
    "- `v2.RandomResizedCrop` chooses a random part of the image and resizes it to our wanted image dimensions\n",
    "- `v2.RandomHorizontalFlip` decides with a chance of `p=0.5`, so 50/50, whether the image is flipped or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "## prepare data\n",
    "transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainloader, testloader, classes = prepare_imagenette(train_compose=transform,test_compose=transform,save_path=\"../Dataset/\",batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11015f13-ed06-415c-bfcd-ed8ffe74de09",
   "metadata": {},
   "source": [
    "Have a look, what images we are working with, here only in grey-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2cdb6-f1f1-4219-b54f-f34d51fbf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(imges):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        img = imges[i,0]\n",
    "        img = img / 2 + 0.5 #unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(npimg, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images)\n",
    "\n",
    "print('Classes are: ')\n",
    "print('| '.join(f'{classes[labels[j]]:5s} ' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72614289-c286-49d7-a705-3f86701de016",
   "metadata": {},
   "source": [
    "## Create the convolutional neural network\n",
    "\n",
    "### The network class\n",
    "\n",
    "As before, we create a class object which inherits from the `torch.nn.Module`class to build our neural network. This includes the layer objects and the feedforward pass.\n",
    "\n",
    "#### Define the network structure\n",
    "\n",
    "Let's adjust the network model of the previous tutorial to our new dataset.\n",
    "\n",
    "Remember, that a Conv2D layer has the parameters `(input channels,feature maps,kernel size)`. Since the Imagenette images are in RGB, the input now has 3 color channels. Therefore, the number of input channels into the first convolutional layer is $3$. We, again, want $6$ feature maps and a kernel size of $5 \\times 5$. In contrast to defining fully-connected layers, the pixel dimensions of the image is not relevant here.\n",
    "\n",
    "The first concolutional layer is:\n",
    "\n",
    "`self.conv1 = nn.Conv2d(3,6,5)`\n",
    "\n",
    "The second convolutional layer and the max pooling operation stay unchanged.\n",
    "\n",
    "`self.conv1 = nn.Conv2d(6,16,5)`\n",
    "\n",
    "`self.pool = nn.MaxPool2d(2,2)`\n",
    "\n",
    "Now, with the transition to the fully connected layer, we have to do math again. Remember, the size of an image after a convolutional layer is:\n",
    "`(W-K+2P)/S+1` with input dimensions `W`, kernel size `K`, padding `P` and stride `S`. With max pooling here, it is halved again. \n",
    "\n",
    "After the first convolution, the dimension is $((224-5+2*0)/1+1)/2=110$ and after the second (here abbreviated) $(110-5+1)/2=53$. The last convolutional layer has $16$ feature maps. Therefore, the number of outputs equals $16 \\times 53 \\times 53$ and represent the number of inputs of the fully-connected layer. \n",
    "\n",
    "`self.fc1 = nn.Linear(16*53*53,120)`\n",
    "\n",
    "The rest and the forward pass stay the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21cd44-90d3-407a-9727-a057bdbdf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## define the network structure with the layers\n",
    "        self.conv1 = nn.Conv2d(3,6,5) # in_channels, out_channels, kernel_size \n",
    "        self.conv2 = nn.Conv2d(6,16,5) # in_channels,out_channels, kernel_size\n",
    "        self.pool  = nn.MaxPool2d(2,2) # kernel_size, stride\n",
    "        self.fc1   = nn.Linear(16*53*53, 120) # in_channels, out_channels\n",
    "        self.fc2   = nn.Linear(120,84) # in_channels, out_channels\n",
    "        self.fc3   = nn.Linear(84,10) # in_channels, out_channels\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## define the functionality of each layer/between the layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "import torch.optim as optim\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99440b4e-5ee5-4241-91d0-5421bb124947",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92362c",
   "metadata": {},
   "source": [
    "For training the network, we also use a function from the *Utils* folder. Therefore, we have a modified visualization of the training progress. \n",
    "\n",
    "We use the same batch size and number of epochs as before with a neural network being overall the same.\n",
    "\n",
    "What do you think how well will it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functions import train_model\n",
    "\n",
    "num_epochs = 2\n",
    "history = train_model(net, trainloader,testloader,criterion,optimizer,scheduler=None,device=device,num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c0247",
   "metadata": {},
   "source": [
    "The accuracy is not even reaching 50% but we can assume that it just underfits. It has not reached its full potential. Therefore, we increase the number of epochs, for example to 50. In the train_model-function, early stopping via patience is implemented. This stops the training process when the validation accuracy (accuracy on data not used during training) begins to decrease.\n",
    "\n",
    "After you have watched the training process for more epochs, you can see, that the accuracy still stops at around 60%.\n",
    "\n",
    "So how can we improve the model? Play around with number of feature maps in conv-layers, number of conv-layers, number of neurons in the fully connected layers and number of fully-connected layers. If you change something in the conv-layers, remember to also change the number of input channels of the first fully connected layer! You can also play around with parameters outside the model itself, e.g. batch size, optimizer or loss function. \n",
    "\n",
    "This process is called **hyperparameter tuning**.\n",
    "\n",
    "If you cannot find a good solution (above 70%), help yourself and google other CNN architectures and rebuild them. \n",
    "\n",
    "Little reminder: Reload all the necessary cells to refresh their content; otherwise, you will use the old network. If the performance always stays the same, reloading the kernel and clearing all outputs might help.\n",
    "\n",
    "Generally, it is good to automate the process of hyperparameter tuning. If you want to dabble with it, check out [Hyperparameter tuning with Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3584d3",
   "metadata": {},
   "source": [
    "## Exercise: Go deeper\n",
    "\n",
    "Specifically test how the accuracy changes if you add many conv layers and fully connected layers. You should notice a severe degradation in accuracy or at least no significant increase. That is due to the vanishing gradient problem. At some point, the error becomes so small in earlier layers during backpropagation, so that they cannot be trained anymore.\n",
    "\n",
    "The next tutorial introduces Residual Networks which are designed to circumvent this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
