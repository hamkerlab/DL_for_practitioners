{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d271eff0-9096-43c1-b6ce-9a285b6d7187",
   "metadata": {},
   "source": [
    "# Tutorial 4.2: Compact Convolutional Transformer\n",
    "\n",
    "Author: [Ren√© Larisch](mailto:rene.larisch@informatik.tu-chemnitz.de)\n",
    "\n",
    "[Hassani et al. (2021)](https://arxiv.org/abs/2104.05704) published the Compact Convolutional Transformer (CCT), which attempts to address two of the Vision Transformer's (ViT) main drawbacks: the large number of parameters required for processing patches and the substantial amount of data needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4fc35-7354-483f-a8de-f15ca1db40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n",
    "\n",
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "from Utils.dataloaders import prepare_imagenette\n",
    "from Utils.little_helpers import timer, set_seed, get_parameters\n",
    "from Utils.functions import train_model, evaluate_model, test_model\n",
    "from Utils.plotting import visualize_training_results, visualize_test_results\n",
    "\n",
    "set_seed(42)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586deecd-9824-43f1-896d-769281b79d4c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"figures/CCT_0.png\" width=\"750\"/>\n",
    "    <p><i>Figure 1: CCT feeds the output of convolutional layers into the Transformer blocks, instead of patches.</i></p>\n",
    "</div>\n",
    "\n",
    "To achieve this, the CCT employs a block of convolutional layers and max-pooling operations. The output of these layers is fed into the Transformer encoder (see Fig. 1).\n",
    "These representations can be understood as **convolutional tokenization**, which replaces the patch embedding from the ViT. Additionally, the convolutional block encapsulates local information and reduces the dimensionality of the input to the encoder. This results in fewer parameters and reduced computation time with similar performance.\n",
    "\n",
    "A similar notation as used for the ViT can be used to distinguish between different CCT network configurations, extended by the number of convolutional layers. For example, CCT-7/3x2 has seven Transformer encoder layers and two convolutional layers with a 3x3 kernel size.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/comparision.png\" style=\"width:90%\"/>\n",
    "    <p><i>Figure 2: Left: CIFAR-10 accuracy vs. model size (sizes < 12M(illion) parameters). CCT* was trained longer. Right: ImageNet Top-1 validation accuracy comparison (no extra data or pretraining). MACs are the number of <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation\">Multiply-Accumulate Operations</a>, where each MAC is one operation consisting of a multiplication and an addition. </i></p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b7c2e-b7c0-4861-b077-78ddc7522b40",
   "metadata": {},
   "source": [
    "## Convolutional Tokenizer\n",
    "\n",
    "Unlike ViT, which uses patch embedding, CCT uses a convolutional block and employs its output as tokens for the Transformer encoder.\n",
    "We implemented the convolutional tokenizer as a class that inherits from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). \n",
    "The block consists of one or more convolutional layers, with a max-pooling layer following each one.\n",
    "Therefore, we need input parameters to define the convolutional layers, such as kernel size, stride, and padding, as well as the parameters of the potential max-pooling layers and the total number of convolutional layers and feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56364342-a7ff-4dab-b832-d50ad99d6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, kernel_size, stride, padding, pooling_kernel_size=3,\n",
    "                 pooling_stride = 2, pooling_padding =1, n_conv_layers = 1,\n",
    "                 n_input_channels = 3, n_output_channels = 64, in_planes = 64,\n",
    "                 activation=None, max_pool=True, conv_biase = False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        n_filter_list = [n_input_channels] + \\\n",
    "                        [in_planes for _ in range(n_conv_layers -1)] + \\\n",
    "                        [n_output_channels]\n",
    "        \n",
    "        ## use nn.Sequential to create the convolutional blocks\n",
    "        ## each block consists of a 2D-convolutional layer, an activation function and a 2D-max-pooling operation\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(n_filter_list[i], n_filter_list[i+1],\n",
    "                          kernel_size=(kernel_size,kernel_size),\n",
    "                          stride = (stride,stride),\n",
    "                          padding= (padding,padding), \n",
    "                           bias = conv_biase ),\n",
    "                nn.Identy() if activation is None else activation(),\n",
    "                nn.MaxPool2d(kernel_size = pooling_kernel_size,\n",
    "                             stride = pooling_stride,\n",
    "                             padding = pooling_padding) if max_pool else nn.Identy()\n",
    "            )\n",
    "                 for i in range(n_conv_layers)\n",
    "             ])\n",
    "        self.flattener = nn.Flatten(2,3)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    ## function to calculate the final sequence length of the convolution tokens\n",
    "    def sequence_length(self, n_channels = 3, height=128, width=128):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv_layers(x)).transpose(-2,-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb5fce-81fc-497b-b722-c24157a39a09",
   "metadata": {},
   "source": [
    "## Transformer (Encoder-only)\n",
    "\n",
    "For the Transformer encoder, we first define the multi-head attention layer as done for the ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df5571-90a8-47f6-9055-6754aad264da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = nn.Dropout(attention_dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(projection_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        #print(x.shape, self.num_heads, C//self.num_heads)        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c749d-4673-412e-8b1a-3556b9c7b2d0",
   "metadata": {},
   "source": [
    "### Stochastic Depth\n",
    "\n",
    " The CCT uses [Stochastic Depth](https://arxiv.org/abs/1603.09382) as a regularization technique. During training, it randomly drops a subset of layers (unlike Dropout, which drops single neurons).\n",
    "Stochastic depth was originally developed for very deep neural networks, such as ResNet. See the ResNet notebook (Tutorial 2.4) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb962c8-ce5c-4f18-b47c-1f6d37347ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to rwightman's timm package\n",
    "# github.com:rwightman/pytorch-image-models\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "#Stochastic Depth\n",
    "class DropPath(nn.Module):\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfefab6-be3b-4993-958d-8986cfad7559",
   "metadata": {},
   "source": [
    "With that, we can implement the class for a Transformer block, consisting of a multi-attention head layer, a MLP and some operations for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40909e25-f020-4729-815f-2d9c1740ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.pre_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
    "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        src = src + self.drop_path(self.self_attn(self.pre_norm(src))) # residual connection 1\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
    "        src = src + self.drop_path(self.dropout2(src2)) # residual connection 1\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9d369-6d2f-4cca-822c-81722356967f",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "To perform classification, we build a classification block consisting of multiple attention layers and a classification head. This block receives the convolution tokens as input.\n",
    "\n",
    "### Positional embedding\n",
    "For the positional embedding, the authors of the CCT evaluated three different approaches.\n",
    "1. Sinusoidal embedding: Introduced by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) for Transformers in a NLP task.\n",
    "2. Learnable embedding: As used by [Dosovitskiy et al. (2020)](https://arxiv.org/abs/2010.11929) in the ViT model (see the ViT notebook (Tutorial 4.1) for more information)\n",
    "3. None embedding: As the convolutional layer already encapsulates some local information, the authors demonstrate that removing the positional embedding has only a weak influence on the accuracy (see Table 7 in [Hassani et al. (2022)](https://arxiv.org/abs/2104.05704)).\n",
    "\n",
    "### Sequence Pooling\n",
    "Unlike the ViT, which has an additional class token used to aggregate class-specific information, the CCT introduces sequence pooling (**SeqPool**).\n",
    "The main idea is to pool the output sequence of tokens and weight them to determine the most important one.\n",
    "\n",
    "\n",
    "Assume the output of an $L$ layer Transformer encoder \n",
    "$$\n",
    "x_{L} \\in \\mathbb{R}^{B\\times n \\times d}\n",
    "$$\n",
    "where $B$ is the batch size, $n$ the sequence length, and $d$ is the embedding dimension. \n",
    "The output is fed into a linear layer $g(x_L) \\in \\mathbb{R}^{d \\times 1}$ and a softmax activation function is applied on that. So the new output is\n",
    "$$\n",
    "x'_L = \\text{softmax}\\left( g(x_L)^T \\right) \\in \\mathbb{R}^{B \\times 1 \\times n}\n",
    "$$\n",
    "\n",
    "This output ($x'_L$) can be understood as an importance weighting for each token in our original output, giving the output of the sequence pooling:\n",
    "$$\n",
    " z = x'_L x_L = \\text{softmax}\\left( g(x_L)^T \\right) \\times x_{L} \\in \\mathbb{R}^{B \\times 1 \\times d}\n",
    "$$\n",
    "\n",
    "By reducing the $1$-dimension, the final output is $z \\in \\mathbb{R}^{B \\times d}$ and can be sent to the classification head.\n",
    "\n",
    "Instead of sequence pooling, the CCT can be used with the cls token. This will be considered in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc384cb-58f5-42ba-bc69-0efbd0252780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_pool=True,\n",
    "                 embedding_dim = 256,\n",
    "                 num_layers=12,\n",
    "                 num_heads = 6,\n",
    "                 mlp_ratio = 4.,\n",
    "                 num_classes = 10,\n",
    "                 dropout =.1,\n",
    "                 attention_dropout=.1,\n",
    "                 stochastic_depth = .1,\n",
    "                 positional_embedding = 'learnable',\n",
    "                 sequence_length = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## positional embedding can be realised in three ways:\n",
    "        ## 1. sinus embedding \n",
    "        ## 2. learnable\n",
    "        ## 3. No additional posistional embedding\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seq_pool = seq_pool\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        ## if no SeqPool is used, us a cls token like the ViT\n",
    "        if not seq_pool:\n",
    "            sequence_length +=1\n",
    "            self.class_emb = nn.Parameter(torch.zeros(1,1, self.embedding_dim),\n",
    "                                      requires_grad=True)\n",
    "            self.num_tokens = 1\n",
    "        else:\n",
    "            ## if SeqPool is used, intialize here the linear layer g()\n",
    "            self.attention_pool = nn.Linear(self.embedding_dim, 1)\n",
    "\n",
    "        ## initialize the positional embedding\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = nn.Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "\n",
    "        ## create a list of transformer encoder layer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        ## if no SeqPool is used, use the cls token\n",
    "        if not self.seq_pool:\n",
    "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        ## apply SeqPool\n",
    "        if self.seq_pool:\n",
    "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7d5bd-6425-4629-b769-c0e60aa30df3",
   "metadata": {},
   "source": [
    "## CCT Model\n",
    "Now we bring everything together to built the CCT network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468750b6-bea0-4ec6-a021-535a4a6a0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embed_dim = 256,\n",
    "                 n_input_channels = 3,\n",
    "                 n_conv_layers = 2,\n",
    "                 kernel_size=3,\n",
    "                 stride = 1,\n",
    "                 padding = 3,\n",
    "                 pooling_kernel_size = 2,\n",
    "                 pooling_stride = 2,\n",
    "                 pooling_padding = 1,\n",
    "                 dropout = 0.2,\n",
    "                 attention_dropout = .1,\n",
    "                 stochastic_depth =.1,\n",
    "                 num_layers=7,\n",
    "                 num_heads = 6,\n",
    "                 mlp_ratio=4.,\n",
    "                 num_classes = 10,\n",
    "                 pos_embeding = 'learnable'):\n",
    "        super(CCT, self).__init__()\n",
    "\n",
    "        ## Convolutional-Tokenizer\n",
    "        self.tokenizer = Tokenizer(kernel_size = kernel_size,\n",
    "                                   n_input_channels=n_input_channels,\n",
    "                                   n_output_channels = embed_dim,\n",
    "                                   stride = stride,\n",
    "                                   padding = padding,\n",
    "                                   pooling_kernel_size = pooling_kernel_size,\n",
    "                                   pooling_stride = pooling_stride,\n",
    "                                   pooling_padding = pooling_padding,\n",
    "                                   max_pool = True,\n",
    "                                   activation = nn.ReLU,\n",
    "                                   n_conv_layers = n_conv_layers,\n",
    "                                   conv_biase = False)\n",
    "\n",
    "        ## Transformer-based classifier\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length = self.tokenizer.sequence_length(\n",
    "                n_channels=n_input_channels,\n",
    "                height = img_size,\n",
    "                width = img_size),\n",
    "                embedding_dim = embed_dim,\n",
    "                seq_pool = True,\n",
    "                dropout = dropout,\n",
    "                attention_dropout = attention_dropout,\n",
    "                stochastic_depth = stochastic_depth,\n",
    "                num_layers = num_layers,\n",
    "                num_heads = num_heads,\n",
    "                mlp_ratio = mlp_ratio,\n",
    "                num_classes = num_classes,\n",
    "                positional_embedding = pos_embeding\n",
    "                )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c5cab-1741-46aa-a1ae-9a785601876f",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "While the CCT has a relative low number of parameters, we use different data augmentation methods, such as crop and resize, horizontal flipping, rotation, jitter and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53884bd7-4b7c-401b-98c2-3fc68d0e74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2, Compose\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_augm = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    # Core transformations\n",
    "    v2.RandomResizedCrop(size=224, scale=(0.75, 1.0), ratio=(0.9, 1.05)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),  # People can face either direction\n",
    "    v2.RandomRotation(degrees=(-10, 10)),  # Small rotations\n",
    "    \n",
    "    # Lighting and appearance variations\n",
    "    v2.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.05),\n",
    "    v2.RandomAutocontrast(p=0.2),\n",
    "    \n",
    "    # Occasional realistic variations - with proper probability handling\n",
    "    v2.RandomApply([v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),\n",
    "    v2.RandomPerspective(distortion_scale=0.15, p=0.3),\n",
    "    v2.RandomErasing(p=0.1, scale=(0.02, 0.08), ratio=(0.3, 3.3)),\n",
    "    \n",
    "    # Normalization\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_norm = Compose(\n",
    "[    v2.ToImage(),\n",
    "     v2.ToDtype(torch.float32, scale=True),\n",
    "     v2.Resize(size=(224,224)),\n",
    "     v2.Normalize(mean = [0.485, 0.456, 0.406], std=[0.229,0.224,0.225]) , \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c58fc-635c-4203-9c80-bfe183ffcaad",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To train the network we use the [Imagenette](https://github.com/fastai/imagenette) dataset, which is already provided by [torchvision](https://pytorch.org/vision/main/generated/torchvision.datasets.Imagenette.html).\n",
    "\n",
    "It is a 10 class subset of Imagenet (tench, english springer, cassette player, chain saw, church, french horn, garbage truck, gas pump, golf ball, parachute), easily to be classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7c9b4-5b7b-4983-9b94-9bde97feeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_loader, test_loader, classes = prepare_imagenette(transform_augm, transform_norm, \n",
    "                                                        save_path='../Dataset/', batch_size = batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a93b6-07f9-4827-bad7-769c244289a7",
   "metadata": {},
   "source": [
    "Show some samples with the augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba19ff6-7038-4288-bb6d-c8d7c3602e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(imges):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        img = imges[i]\n",
    "        img = img / 2 + 0.5 #unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.show\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edc5a3-6b0b-421c-99cb-a8baa84f23d5",
   "metadata": {},
   "source": [
    "## Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478efa2-4af2-44df-8f3e-64cfc57b9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim =32*7\n",
    "num_heads =embed_dim//32\n",
    "depth = 7\n",
    "n_convlayers = 3\n",
    "kernel_size = 3\n",
    "\n",
    "net_cct = CCT(num_classes = len(classes), \n",
    "              embed_dim = embed_dim, \n",
    "              num_heads=num_heads, \n",
    "              num_layers=depth,\n",
    "              n_conv_layers=n_convlayers,\n",
    "              kernel_size = kernel_size)\n",
    "\n",
    "print('Trainable Parameters in CCT: %.3fM' % get_parameters(net_cct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81593208-db59-4188-984c-3a3dff0312e0",
   "metadata": {},
   "source": [
    "## Loss and optimizer\n",
    "\n",
    "We use the AdamW optimizer, provided by [PyTorch](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html). Without going into too much detail, the normal Adam optimizer applies a weight decay term directly to the calculated gradient of a batch. Since the gradient is affected by learning rates, so is the weight decay term, too. In contrast, AdamW adds the weight decay term to the parameter updates. This leads to a stable influence of the weight decay term (see [Loshchilov and Hutter, 2019](https://arxiv.org/abs/1711.05101) for a deeper explanation). \n",
    "\n",
    "Similar to the ViT implementation, we will use a cosine annealing learning rate scheduler for training. Please note that no warmup scheduler is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d61e1f-f9e7-454b-b01e-f70e090eeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# define optimizer\n",
    "init_lr = 1e-4\n",
    "optimizer = optim.AdamW(net_cct.parameters(), lr=init_lr, weight_decay=5e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534c8df-862e-46a8-ac50-96e47eb038a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "# train model\n",
    "history = train_model(model=net_cct, \n",
    "                        train_loader=train_loader, \n",
    "                        val_loader=test_loader, \n",
    "                        criterion=nn.CrossEntropyLoss(),\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        device=device,\n",
    "                        num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "# save model\n",
    "results_folder = 'cct_model/'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "torch.save(net_cct.state_dict(), f'{results_folder}cct_net_state_dict.pt')\n",
    "\n",
    "# save history + predictions\n",
    "np.save(f'{results_folder}history.npy', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "with timer(\"Evaluating process\"):\n",
    "    aggregate_df, per_image_df, overall_accuracy = test_model(model=net_cct,\n",
    "                                                              test_loader=test_loader,\n",
    "                                                              device=device,\n",
    "                                                              class_names=classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820091c-23d3-4a3b-862e-37891f1feb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plotting import visualize_training_results, visualize_test_results\n",
    "\n",
    "visualize_training_results(train_losses=history['train_loss'],\n",
    "                           train_accs=history['train_acc'],\n",
    "                           test_losses=history['val_loss'],\n",
    "                           test_accs=history['val_acc'],\n",
    "                           output_dir=None)\n",
    "\n",
    "visualize_test_results(aggregate_df=aggregate_df, per_image_df=per_image_df, overall_accuracy=overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab1728-8ccf-42a3-b984-615cab73d7ab",
   "metadata": {},
   "source": [
    "## Pre-trained CCT\n",
    "It is also possible to use a pre-trained CCT and fine-tune it on the new dataset.\n",
    "We use the pre-trained model from [SHI-Labs](https://github.com/SHI-Labs/Compact-Transformers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3b370-1b2f-4b67-8e82-0a339daa2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('Compact_Transformers'):\n",
    "    os.popen('git clone https://github.com/SHI-Labs/Compact-Transformers.git Compact_Transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4526c2c-52d0-4859-aa81-b211aaef41d5",
   "metadata": {},
   "source": [
    "## Fine tune on Imagenette\n",
    "\n",
    "We load a CCT network, pre-trained on the imagenet dataset, consisting of 7 transformer encoder layers, a convolutional block with 2 convolutional layers and a kernel size of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470ce6f-d817-4c90-b8e9-20a69add845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Compact_Transformers.src import cct_7_7x2_224_sine\n",
    "cct_net = cct_7_7x2_224_sine(pretrained=True, progress=False, img_size=224, positional_embedding='sine')\n",
    "\n",
    "print('Trainable Parameters in CCT: %.3fM' % get_parameters(cct_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66067a8-63a2-4856-8272-7092821894b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)\n",
    "## get the input dimensionality of the old classifier\n",
    "in_features = cct_net.classifier.fc.in_features\n",
    "## put a new classifier in top\n",
    "cct_net.classifier.fc = nn.Sequential(nn.Linear(in_features, 256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.3),\n",
    "                                      nn.Linear(256, num_classes)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72518525-ac9e-4339-910d-a91370e04427",
   "metadata": {},
   "source": [
    "We use the same optimizer and scheduler as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0491470-631b-4b9f-828e-1a05780b7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# define optimizer\n",
    "init_lr = 1e-4\n",
    "optimizer = optim.AdamW(cct_net.parameters(), lr=init_lr, weight_decay=5e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e7c84-e526-4e48-a3b5-cb4cb9d4e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = train_model(model=cct_net, \n",
    "                        train_loader=train_loader, \n",
    "                        val_loader=test_loader, \n",
    "                        criterion=nn.CrossEntropyLoss(),\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        device=device,\n",
    "                        num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "# save model\n",
    "results_folder = 'cct_model/'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "torch.save(cct_net.state_dict(), f'{results_folder}model_state_dict.pt')\n",
    "\n",
    "# save history + predictions\n",
    "np.save(f'{results_folder}history.npy', history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb704b-b700-490b-a5f1-7c0b3b145cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "with timer(\"Evaluating process\"):\n",
    "    aggregate_df, per_image_df, overall_accuracy = test_model(model=cct_net,\n",
    "                                                              test_loader=test_loader,\n",
    "                                                              device=device,\n",
    "                                                              class_names=classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fb0ed-b687-4ea1-887e-7d3ccec1b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "from Utils.plotting import visualize_training_results, visualize_test_results\n",
    "\n",
    "visualize_training_results(train_losses=history['train_loss'],\n",
    "                           train_accs=history['train_acc'],\n",
    "                           test_losses=history['val_loss'],\n",
    "                           test_accs=history['val_acc'],\n",
    "                           output_dir=None)\n",
    "\n",
    "\n",
    "visualize_test_results(aggregate_df=aggregate_df, per_image_df=per_image_df, overall_accuracy=overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef113824-6b9c-45db-8164-25787f1cbf66",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## 1. Change the parameters\n",
    "\n",
    "The CCT is a good example of how different techniques and approaches (such as Transformers, convolutional networks, sequence pooling, and stochastic depth) can be combined. Experiment with the various components and adjust their parameters to observe their impact on performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a71331-0da2-42f2-a5d7-04ffc06ac114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a6ecd-14a4-40ee-9314-4ec169b7158c",
   "metadata": {},
   "source": [
    "## 2. Plot the attention maps\n",
    "\n",
    "In the ViT notebook, you visualized the attention maps for the ViT model. You can use the same algorithm for the CCT with some adaptations, such as adapting the code for the max pooling layers in the convolution tokenizer.\n",
    "How does the algorithm change depending on whether **SeqPool** or the cls token is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d820ce3-3a19-4d2f-9618-4f492df1aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
