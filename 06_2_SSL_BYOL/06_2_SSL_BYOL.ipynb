{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8eeb30c-2673-448a-9b09-c6a53ee3bf03",
   "metadata": {},
   "source": [
    "# Tutorial 6.2: Bootstrap Your Own Latent (BYOL)\n",
    "\n",
    "Author: [Ren√© Larisch](mailto:rene.larisch@informatik.tu-chemnitz.de)\n",
    "\n",
    "The self-supervised approach **B**ootstrap **Y**our **O**wn **L**atent (BYOL) has been introduced by [Grill et al. 2020](https://arxiv.org/abs/2006.07733)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a3d32-1fc8-4699-adc4-e20086e588f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_image\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n",
    "\n",
    "\n",
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "from Utils.dataloaders import prepare_UTKFace_age_task\n",
    "from Utils.little_helpers import timer, set_seed, get_parameters\n",
    "from Utils.functions import train_model, evaluate_model, test_model\n",
    "from Utils.optimizers import LARS\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027aa16-b9e4-44cf-b273-485b7b08252e",
   "metadata": {},
   "source": [
    "## How BYOL works\n",
    "\n",
    "Self-supervised approaches like SimCLR are contrastive learning methods that aim to increase the similarity between representations of two similar input samples (a positive pair) and decrease the similarity between two different input samples (a negative pair). For these approaches, selecting negative pairs is important for achieving good representation. Due to this, the performance of the network depends on the right batch size, the right augmentation pipeline and other things.\n",
    "\n",
    "Grill et al. (2020) tackle this problem by presenting a self-supervised learning approach, which does not need negative pairs.\n",
    "\n",
    "To do this, BYOL uses two networks, an online (with the parameter set $\\theta$) and a target network (with the parameter set $\\xi$) (see Fig.1). Similar to SimCLR, both networks consist of an encoder model (for example a ResNet-like network), a representation layer (here called embedding layer), and a non-linear projection head. While both networks are identical in there architecture until the non-linear projection head, the online network is extended by a prediction head. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/BYOL_net.png\" width=\"650\"/>\n",
    "    <p><i>Figure 1: BYOL uses two networks, an online and a target network. While the weights in the online network are updated with backpropagation, the weights in the target network are updated with an exponential moving average from the weights of the online network.</i></p>\n",
    "</div>\n",
    "\n",
    "BYOL creates for an image ($x$) two differently augmented versions ($v$ and $v'$) and uses  $v$ to calculate the output of the online network ($q_{\\theta}(z_{\\theta})$) and $v'$ to calculate the output of the target network ($z'_{\\xi}$).\n",
    "Here, $z_{\\theta}$ and $z_{\\xi}$ are the outputs of the projection head of the online and target network, respectively, and $q_{\\theta}()$ the prediction function. The loss between the outputs of the online and target network is defined as:\n",
    "\n",
    "$$\n",
    " \\mathcal{L_{\\theta,\\xi}} = \\frac{\\langle q_{\\theta}(z_{\\theta}), z'_{\\xi} \\rangle}{\\|q_{\\theta}(z_{\\theta})\\|_{2} \\cdot \\| z'_{\\xi} \\|_2}\n",
    "$$\n",
    "\n",
    "While only the weight updates of the online network are based on the loss, the target network is updated as a moving average of the online network:\n",
    "\n",
    "$$\n",
    " \\xi = \\tau \\xi + (1- \\tau)\\theta\n",
    "$$\n",
    "\n",
    "The learning approach can therefore be understood in such a way, that the online network tries to predict the output of the target network, without knowing the used augmentation and the parameters of the network. Changing the parameters of the target network and adding asymmetry between the model architectures add additional variability to the output of the target network. The authors suggest, that this variability avoids the collapsing to a simple solution and forces the online network to learn robust representations.\n",
    "\n",
    "After training, the output of the encoding network, or the embedding layer, from the online network will be used for the downstream task. \n",
    "\n",
    "Without the need for negative samples, the authors demonstrate that the performance on the downstream task is less sensitive for the batch size and more robust against changes in the augmentation pipeline than SimCLR (Fig.2).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/results.png\" width=\"650\"/>\n",
    "    <p><i>Figure 2: The decrease of accuracy on a downstream task (ImageNet) compared between BYOL (red) and SimCLR (blue) for different batch sizes and different combinations of augmentations.</i></p>\n",
    "</div>\n",
    "\n",
    "While BYOL has the advantage of a simpler loss function and less sensitivity to batch sizes and augmentations over SimCLR, it requires two networks, which leads to longer computation times. Additionally, BYOL requires more than twice the memory storage due to the two networks and the additional prediction head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ee2ac-dc87-426b-b816-92944785fe3f",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Instead of building one big object, we will implement the different parts (encoder model, projection head, prediction head) as different objects, as they enable us to be more flexible in changing the structure of the different parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd972b5d-9647-4a0e-b3cb-fdc5ebf65761",
   "metadata": {},
   "source": [
    "### Prediction head\n",
    "\n",
    "The prediction head (which will only be used on the online network) will be implemented as a simple multi-layer perceptron (MLP) network. \n",
    "Please note, that while we here implement a very shallow prediction head, a more deeper network can improve the performance of BYOL, but also slows down the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a328f1c-3f60-4e89-a33a-5598385bf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PredHead(nn.Module):\n",
    "    def __init__(self, dim, embedding_size=256, hidden_size=2048, batch_norm_mlp=False):\n",
    "        super().__init__()\n",
    "        norm = nn.BatchNorm1d(hidden_size) if batch_norm_mlp else nn.Identity()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            norm,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f04743-80e9-4bac-b375-2b4f5de5f517",
   "metadata": {},
   "source": [
    "### Projection head\n",
    "\n",
    "The projection head is just one layer with an activation function, which gets the flatten output from the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e2bea-79c8-42d4-baff-1b61cf6ea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 use_bias = True,\n",
    "                 use_bn = False,\n",
    "                 **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, \n",
    "                                self.out_features, \n",
    "                                bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "             self.bn = nn.BatchNorm1d(self.out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class AddProjHead(nn.Module):\n",
    "    def __init__(self, model, in_features, layer_name, hidden_size=4096,\n",
    "                embedding_size = 256, batch_norm_mlp=True):\n",
    "        super(AddProjHead, self).__init__()\n",
    "        self.base_model = model\n",
    "\n",
    "        if isinstance(self.base_model, str):\n",
    "            #PRETRAINED MODEL\n",
    "            if self.base_model == 'resnet50':\n",
    "                self.encoder_model = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            self.encoder_model = self.base_model\n",
    "\n",
    "        if self.encoder_model == None:\n",
    "            print('No valid encoder model chosen')\n",
    "            \n",
    "        ## if base_model == resnet50, switch the first convolutional and some later layers for fine tuning\n",
    "        if self.base_model == 'resnet50':\n",
    "            self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "            self.pretrained.maxpool = nn.Identity()\n",
    "            self.pretrained.fc = nn.Identity()\n",
    "            ## set the pretrained weights fix for the resnet model\n",
    "            for p in self.pretrained.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        ## embedding layer from where we later want the embeddings for the downstream task\n",
    "        self.embedding_layer = LinearLayer(in_features,embedding_size, True, True)\n",
    "        self.projection =   nn.Sequential(\n",
    "                LinearLayer(embedding_size,hidden_size,True, True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(hidden_size,hidden_size,False,True),\n",
    "                nn.ReLU(),)\n",
    "        \n",
    "    def forward(self, x, return_embedding = False):\n",
    "        out = self.encoder_model(x)\n",
    "        embedding = self.embedding_layer(out)\n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        return self.projection(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da83f5-2241-41da-95a2-ffb04af9b89d",
   "metadata": {},
   "source": [
    "### Encoder network\n",
    "We will implement here a little convolutional neural network (as used in the other notebooks). A more common encoder network is ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f45485-abf5-41f9-9e91-662664b0213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, lat_dim):\n",
    "        super().__init__()\n",
    "        ## define the network structure with the layers\n",
    "        self.conv1 = nn.Conv2d(3,256,3) # in_channels, out_channels, kernel_size \n",
    "        self.pool  = nn.MaxPool2d(2,2) # kernel_size, stride\n",
    "        self.dropout = nn.Dropout(0.2) # dropout factor\n",
    "        self.conv2 = nn.Conv2d(256,128,3) # in_channels,out_channels, kernel_size\n",
    "        self.conv3 = nn.Conv2d(128,64,3) # in_channels,out_channels, kernel_size\n",
    "        self.fc1   = nn.Linear(64*14*14, 120) # in_channels, out_channels\n",
    "        self.fc2   = nn.Linear(120,1024)    \n",
    "        self.fc3   = nn.Linear(1024,lat_dim) # in_channels, out_channels\n",
    "        #self.fc3   = nn.Linear(84,4) # in_channels, out_channels\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## define the functionality of each layer/between the layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e69455-e233-42ae-b70f-c95589b5fd40",
   "metadata": {},
   "source": [
    "### Exponential moving average (EMA)\n",
    "Before we stitch everything together, we have to implement an object to calculate the exponential moving average to update the target network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e771ea-9e45-4efd-8e9b-9ffb04741864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.alpha + (1 - self.alpha) * new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f242b-dfa3-489c-b69f-3ff3f3e295e3",
   "metadata": {},
   "source": [
    "### The complete network\n",
    "Finally, we must decide:\n",
    "* whether to use batch normalization in the projection and prediction heads\n",
    "* how many features should be sent to the projection head\n",
    "* the size of the projection network\n",
    "* the decay for the moving average\n",
    "* and whether to use momentum for the moving average.\n",
    "\n",
    "To symmetrize the loss ($\\mathcal{L}_{\\theta,\\xi}$), we feed $v'$ to the online network and $v$ to the target network to calculate the loss between the predictions of the different augmentation variants. The final loss is described as follows:\n",
    "\n",
    "$$\n",
    " \\mathcal{L}_{\\theta,\\xi} = 2 - 2 \\cdot \\frac{\\langle q_{\\theta}(z_{\\theta}), z'_{\\xi} \\rangle}{\\|q_{\\theta}(z_{\\theta})\\|_{2} \\cdot \\| z'_{\\xi} \\|_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5889b5-09b1-46ec-9a78-9852066ab21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "    def __init__( self, net,\n",
    "        batch_norm_mlp=True,\n",
    "        layer_name = 'fc',\n",
    "        in_features = 512,\n",
    "        projection_size = 256,\n",
    "        projection_hidden_size = 2048,\n",
    "        moving_average_decay = 0.99,\n",
    "        use_momentum = True):\n",
    "        super().__init__()\n",
    "        ## init online model with projection head\n",
    "        self.online_model = AddProjHead(model = net, in_features = in_features,\n",
    "                                         layer_name = layer_name,\n",
    "                                         embedding_size = projection_size,\n",
    "                                         hidden_size = projection_size,\n",
    "                                         batch_norm_mlp = batch_norm_mlp)\n",
    "        self.use_momentum = use_momentum\n",
    "        ## extra function that we use to get the target (see below)\n",
    "        self.target_model = self._get_target()\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "        ## additional prediction head for the online network\n",
    "        self.online_predictor = PredHead(projection_size,projection_size, projection_hidden_size)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_target(self):\n",
    "        return copy.deepcopy(self.online_model)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum ' \\\n",
    "                                  'for the target encoder '\n",
    "        assert self.target_model is not None, 'target encoder has not been created yet'\n",
    "\n",
    "        for online_params, target_params in zip(self.online_model.parameters(), self.target_model.parameters()):\n",
    "            old_weight, up_weight = target_params.data, online_params.data\n",
    "            target_params.data = self.target_ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "    ## Loss function\n",
    "    def loss_fn(self, x,y):\n",
    "        #L2 normalization\n",
    "        x = F.normalize(x, dim=-1, p=2)\n",
    "        y = F.normalize(y, dim=-1, p=2)\n",
    "    \n",
    "        return 2 - 2 * (x * y).sum(dim=-1) \n",
    "    \n",
    "    def forward(self,image_one, image_two = None, return_embedding = False):\n",
    "        if return_embedding or (image_two is None):\n",
    "            return self.online_model(image_one, return_embedding=True)\n",
    "\n",
    "        # online projections: backbone + projection\n",
    "        online_proj_one = self.online_model(image_one)\n",
    "        online_proj_two = self.online_model(image_two)\n",
    "\n",
    "        # additional predictor\n",
    "        online_pred_one = self.online_predictor(online_proj_one)\n",
    "        online_pred_two = self.online_predictor(online_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ## target network processes the images and predicts: backbone + PredHead\n",
    "            target_proj_one = self.target_model(image_one).detach_()\n",
    "            target_proj_two = self.target_model(image_two).detach_()\n",
    "\n",
    "        loss_one = self.loss_fn(online_pred_one, target_proj_one)\n",
    "        loss_two = self.loss_fn(online_pred_two, target_proj_two)\n",
    "\n",
    "        return((loss_one + loss_two).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421a35b-e722-463a-ac2f-9e46a0aff68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 512#128\n",
    "byol_net = BYOL(Net(num_features), in_features=num_features, batch_norm_mlp=True).to(device)\n",
    "\n",
    "print('Trainable Parameters in CCT: %.3fM' % get_parameters(byol_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594d1d2-3fe8-4d65-bf02-685d0bfa9d1d",
   "metadata": {},
   "source": [
    "## AgeDB dataset\n",
    "The AgeDB, published by [Moschoglou et al.2017](10.1109/CVPRW.2017.250), contains 16,488 images of famous people at different ages.\n",
    "Each sample image contains three pieces of information: the person's name, gender, and age.\n",
    "\n",
    "### Prepare the dataset\n",
    "\n",
    "To run the following lines of code, make sure you have access to the dataset (e.g. download it beforehand from https://www.kaggle.com/datasets/nitingandhi/agedb-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441c6c0-4069-4cc0-8909-dfd27f14e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_file(path_data, split):\n",
    "    ## function to iterate through the path and create a annotation csv\n",
    "    list_data    = []\n",
    "    label_name   = []\n",
    "    label_age    = []\n",
    "    label_gender = []\n",
    "    \n",
    "    for idx,file in enumerate(os.listdir(path_data)):\n",
    "        _, name, age, gender = file.split('_')\n",
    "        gender = gender.split('.')[0]\n",
    "        \n",
    "        list_data.append((path_data+'/'+file, name, age, gender))\n",
    "\n",
    "    ## shuffle and then split the dataset into train, test, validation\n",
    "    n_samples = len(list_data)\n",
    "    idx = np.linspace(0,n_samples-1, n_samples, dtype='int32')\n",
    "    np.random.shuffle(idx)    \n",
    "    list_data   = np.asarray(list_data)[idx]\n",
    "\n",
    "    n_train= int(n_samples*split[0])\n",
    "    n_test = int(n_samples*split[1])\n",
    "\n",
    "    ##train data\n",
    "    train_data = list_data[:n_train]\n",
    "\n",
    "    ## test data\n",
    "    test_data  = list_data[n_train:n_train+n_test]\n",
    "    \n",
    "    ## create two csv-files for the annotations\n",
    "    csv_train = pd.DataFrame(train_data, columns = ['Images','Name','Age','Gender'])\n",
    "    csv_test = pd.DataFrame(test_data, columns = ['Images','Name','Age','Gender'])\n",
    "\n",
    "    ## save them for later \n",
    "    csv_train.to_csv('annot_train.csv', sep=',', index=False)\n",
    "    csv_test.to_csv('annot_test.csv', sep=',', index=False)\n",
    "\n",
    "    labels_name = csv_test['Name']\n",
    "    labels_age = csv_test['Age']\n",
    "    labels_gender = csv_test['Gender']\n",
    "\n",
    "    return(labels_name, labels_age, labels_gender)\n",
    "\n",
    "path_data = '../Dataset/AgeDB/'\n",
    "split = [0.8,0.2]\n",
    "\n",
    "labels_name, labels_age, labels_gender = make_annotation_file(path_data, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab25d9-dc37-4282-8840-4d715fd1f3c2",
   "metadata": {},
   "source": [
    "### Augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5b99b-d0c2-4176-9f91-ed688217dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "class DataSetAugment(Dataset):\n",
    "    def __init__(self,phase, annotations_file, s = 0.5):\n",
    "        self.phase = phase\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.s = s\n",
    "        self.resize = v2.Resize((128,128))\n",
    "        self.transforms = transforms.Compose([v2.RandomResizedCrop(size=(128,128), scale=(0.75, 1.0), ratio=(0.9, 1.05)),\n",
    "                                            transforms.Compose([v2.RandomApply([\n",
    "                                                                v2.ColorJitter(0.8*self.s, \n",
    "                                                                                       0.8*self.s, \n",
    "                                                                                       0.8*self.s, \n",
    "                                                                                       0.2*self.s)], p = 0.3),\n",
    "                                                                  v2.RandomGrayscale(p=0.2)]),\n",
    "                                             v2.RandomHorizontalFlip(),\n",
    "                                             v2.RandomApply([v2.GaussianBlur((3,3), (1.0,2.0))],p=0.2),\n",
    "                                             v2.RandomResizedCrop(size=(128,128), scale=(0.08,1.0)),])            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_labels.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join(self.img_labels.iloc[idx,0])\n",
    "        y =  self.img_labels.iloc[idx,3] # we select here the age\n",
    "        x = read_image(img_path)\n",
    "        x = x.numpy()\n",
    "        x = x.astype(np.float32)/255.0\n",
    "        \n",
    "        \n",
    "        x1 = self.augment(torch.from_numpy(x))\n",
    "        x2 = self.augment(torch.from_numpy(x))\n",
    "        \n",
    "        x1 = self.preprocess(x1)\n",
    "        x2 = self.preprocess(x2)\n",
    "        \n",
    "        return x1, x2, y\n",
    "\n",
    "    # shuffles the dataset at the end of each epoch\n",
    "    def on_epoch_end(self):\n",
    "         self.img_labels = self.img_labels.sample(frac=1).reset_index(drop=True)\n",
    "    #    self.imgarr = self.imgarr[random.sample(population = list(range(self.__len__())),k = self.__len__())]\n",
    "\n",
    "    def preprocess(self,frame):\n",
    "        MEAN = torch.tensor([[[0.485]], [[0.456]], [[0.406]]])\n",
    "        STD = torch.tensor([[[0.229]],[[0.224]],[[0.225]]])\n",
    "        frame = (frame-MEAN)/STD\n",
    "        return frame\n",
    "    \n",
    "    # applies randomly selected augmentations to each clip (same for each frame in the clip)\n",
    "    def augment(self, frame, transformations = None):\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            frame = self.transforms(frame)\n",
    "        else:\n",
    "            frame = self.resize(frame)\n",
    "            \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f9800",
   "metadata": {},
   "source": [
    "Now create train and test set and plot some examples.\n",
    "\n",
    "*Note: If you're working on a Windows system use 0 workers to avoid multiprocessing issues (Windows uses \"spawn\" for creating processes instead of \"fork\", which can cause issues with multiprocessing). Thus, `num_workers = 0`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92979b-b6ee-4e64-8773-334e4736c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = DataSetAugment('train', 'annot_train.csv')\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = DataSetAugment('valid', 'annot_test.csv')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212dfbc-c29e-4dd6-b0eb-6613112f1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(imges):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        img = imges[i]\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "        plt.axis('off')\n",
    "    plt.show\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images_1, image_2, _= next(dataiter)\n",
    "imshow(image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a14d8e7",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before we start with training the model, we define a saving routine, so that we can use the trained model afterwards, and the optimizer as well as a scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ebbae-1f5c-430c-9dea-ef29d81f9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
    "    os.makedirs('./content/saved_models/', exist_ok=True) \n",
    "    out = os.path.join('./content/saved_models/',name.format(current_epoch))\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict()}, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916e3c2-2806-4e6c-aadd-cca2ae5ca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZER\n",
    "optimizer = LARS(\n",
    "    [params for params in byol_net.parameters() if params.requires_grad],\n",
    "    lr= 0.2 * batch_size/256,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "#SCHEDULER OR LINEAR WARMUP\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
    "\n",
    "#SCHEDULER FOR COSINE DECAY\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96cb92",
   "metadata": {},
   "source": [
    "Let's start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b69e4e-15b4-43d9-90e4-24057bc2bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "nr = 0\n",
    "current_epoch = 0\n",
    "num_epochs = 20\n",
    "tr_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    stime = time.time()\n",
    "\n",
    "    byol_net.train()\n",
    "    tr_loss_epoch = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch +1}/{num_epochs} [Train]')\n",
    "    \n",
    "    for x_i, x_j, _ in train_pbar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.to(device).float()\n",
    "        x_j = x_j.to(device).float()\n",
    "        \n",
    "        loss =  byol_net(x_i, x_j)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # EMA update\n",
    "        byol_net.update_moving_average()\n",
    "\n",
    "        tr_loss_epoch += loss.item()\n",
    "        \n",
    "    if epoch < 10:\n",
    "        warmupscheduler.step()\n",
    "    if epoch >= 10:\n",
    "        mainscheduler.step()\n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        save_model(byol_net, optimizer, mainscheduler, current_epoch,\"BYOL_net_checkpoint.pt\")\n",
    "\n",
    "    byol_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_epoch = 0\n",
    "        val_bar = tqdm(test_loader, desc=f'Epoch {epoch +1}/{num_epochs} [Val]')\n",
    "        for x_i, x_j, _ in val_bar:\n",
    "            \n",
    "            x_i = x_i.to(device).float()\n",
    "            x_j = x_j.to(device).float()\n",
    "\n",
    "            loss =  byol_net(x_i, x_j)\n",
    "\n",
    "            val_loss_epoch += loss.item()\n",
    "\n",
    "\n",
    "    tr_loss.append(tr_loss_epoch / len(train_loader))\n",
    "    val_loss.append(val_loss_epoch / len(test_loader))\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]\\t Training Loss: {tr_loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]\\t Validation Loss: {val_loss_epoch / len(test_loader)}\\t lr: {round(lr, 5)}\")\n",
    "    current_epoch += 1\n",
    "\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]\\t Time Taken: {time_taken} minutes\")\n",
    "\n",
    "save_model(byol_net, optimizer, mainscheduler, current_epoch, \"BYOL_net_final.pt\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_loss, label='Trainings Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e278374-3150-44e7-90a9-9d49ad89fb26",
   "metadata": {},
   "source": [
    "## Fine Tune\n",
    "\n",
    "As for the SimCLR notebook, we will now use the pre-trained network and fine-tune it to predict the gender of the persons in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0448e-e0e1-4bf9-a2fd-00eee968e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_byol_net = BYOL(Net(num_features), in_features=num_features, batch_norm_mlp=True).to(device)\n",
    "new_byol_net.load_state_dict(torch.load('./content/saved_models/BYOL_net_final.pt', weights_only=True)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7534ab-cd80-4809-8242-246e5a76ff7f",
   "metadata": {},
   "source": [
    "Before the fine-tuning, a brief reminder that we wrote the forward() function in the BOYL class so that, when only one image is fed to the network, the output of the projection head is obtained from the online network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba67901-1d32-4e41-8947-d0da7a0766d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self,base_model, num_classes, freeze_base = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        ## freeze the pre-trained model if necessary\n",
    "        if freeze_base:\n",
    "            self.base_model.requires_grad_(False)\n",
    "        else:\n",
    "            self.base_model.requires_grad_(True)\n",
    "\n",
    "        \n",
    "        in_features = 512\n",
    "\n",
    "        ## new head for the classification\n",
    "        self.new_head = nn.Sequential(nn.Linear(in_features, 256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.3),\n",
    "                                      nn.Linear(256, 256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.3),\n",
    "                                      nn.Linear(256, num_classes))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.base_model(x)\n",
    "        out = self.new_head(out)\n",
    "        return (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee4ef2",
   "metadata": {},
   "source": [
    "Now, define the Optimizer and Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cb76c-6497-4b07-9627-1acde3bf5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "num_classes = 2\n",
    "\n",
    "gender_model = FineTuneModel(new_byol_net, num_classes, freeze_base=False)\n",
    "\n",
    "print('Trainable Parameters in network: %.3fM' % get_parameters(gender_model))\n",
    "\n",
    "num_epochs=5\n",
    "init_lr= 1e-4\n",
    "optimizer = optim.AdamW(gender_model.parameters(), lr=init_lr, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487e042",
   "metadata": {},
   "source": [
    "Let's start the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c3e69-012d-49f5-91f4-1356028589b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    gender_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]')\n",
    "\n",
    "    for x_i,_, labels in train_pbar:\n",
    "        x_i = x_i.to(device)\n",
    "        y = np.zeros(len(labels))\n",
    "        y[np.asarray(labels)=='m'] = 0\n",
    "        y[np.asarray(labels)=='f'] = 1\n",
    "        y = torch.tensor(y)\n",
    "        y = y.to(torch.int64)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = gender_model(x_i)\n",
    "        out = out.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * x_i.size(0)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_train_loss: float = running_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc: float = 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce6dfd",
   "metadata": {},
   "source": [
    "Let's see, how well it performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f631b9d-9f29-4e4f-b43f-5cdecab375d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.eval()\n",
    "gender_model.to(device)\n",
    "\n",
    "predictions = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_pbar = tqdm(test_loader, desc='Evaluation')\n",
    "\n",
    "    for x_i,_, labels in eval_pbar:\n",
    "        if x_i.size()[0] < batch_size:\n",
    "            ##ignore the last batch, if it did not fit \n",
    "            break\n",
    "        x_i = x_i.to(device)\n",
    "        out = gender_model(x_i)\n",
    "        if isinstance(out, tuple):\n",
    "            out = out[0]\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        predictions.append(predicted.cpu())\n",
    "        label_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e4e6c-88f9-4eb3-8774-2e0416da3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.asarray(predictions).flatten()\n",
    "label_list = np.asarray(label_list).flatten()\n",
    "labels = np.zeros(len(label_list))\n",
    "labels[label_list=='f'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15acc52a-8d86-4ce3-aa9a-faceebbe32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3be33c-654a-408e-99c6-fb4fc7dc7950",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 1. Change the encoder network to ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504606e-68f3-4ae6-8e89-54bb27ee38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08be5b-744f-4ed2-a330-5b22f781d51a",
   "metadata": {},
   "source": [
    "### 2. Use t-SNE to see how gender is represented in the latent representation from the online network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85b7cb-ff47-44a3-98e8-caa7bf04e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04a324-1c92-462c-a0f6-fc2c9d07e1c1",
   "metadata": {},
   "source": [
    "### 3. Change the augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb6b07-e3ce-49e6-88a0-6812f18630e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
