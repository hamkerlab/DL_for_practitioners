{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec62a6d0233b47c",
   "metadata": {},
   "source": [
    "# Tutorial 3.2: Custom Datasets\n",
    "\n",
    "Author: [Maren Gr√∂ne](mailto:maren.groene@s2016.tu-chemnitz.de)\n",
    "\n",
    "Our tutorial focuses on supervised learning with PyTorch. This is why it is crucial to understand how to implement a data pipeline for training and testing your models. PyTorch provides two fundamental components for this task: \n",
    "- **Dataset**:\n",
    "    - A Dataset is an abstract class that represents a map from indices to data samples\n",
    "    - Each sample can be any Python object: tensors, numbers, dictionaries, lists, custom classes and in our case mostly images with shape (shape, height, width).\n",
    "    - You can import it via:\n",
    "        \n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "```\n",
    "\n",
    "- **DataLoader**: \n",
    "    - Takes a Dataset and creates an iterable that returns batches\n",
    "    - Can transform individual samples into batches using collate_fn\n",
    "    - Supports multi-process data loading to parallelize CPU operations\n",
    "    - Can transfer data asynchronously to the GPU with pinned memory\n",
    "    - You can import it via:\n",
    "    \n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd5141",
   "metadata": {},
   "source": [
    "## Annotation Files\n",
    "\n",
    "It is best practice to extract the relevant metadata from your dataset and save it as a separate file. These are called annotation files. The most commonly used formats are CSV, JSON, XML or simple plain text but there exist many more. In this example, we use CSV for good readability.\n",
    "\n",
    "This process highly depends on your specific data structure and task, e.g. classification, detection etc.\n",
    "\n",
    "We assume here that the task is image classification and that the data is structured as follows:\n",
    "```bash\n",
    "dataset/\n",
    "    test/\n",
    "        class1/\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "        class2/\n",
    "            img3.jpg\n",
    "            img4.jpg\n",
    "    train/\n",
    "        class1/\n",
    "            ...\n",
    "        class2/\n",
    "            ...\n",
    "    val/\n",
    "        class1/\n",
    "            ...\n",
    "        class2/\n",
    "            ...\n",
    "```\n",
    "If you are using your own data instead of a pre-existing dataset, randomly sort the contents of each class into the train/test/validation folders roughly using an 80/10/10 split. \n",
    "\n",
    "We define a function to generate three annotation files, one each for the training, validation, and test sets. Each file contains two columns: the image filename and its corresponding label. Instead of using class names (strings), we use the index of a label in a list of all labels. This numerical format aligns with the output classes of the neural network, where each label corresponds to a specific output neuron.\n",
    "\n",
    "We use the `pandas` function `pd.DataFrame(list_train, columns=['', ''])` to specify the data structure as a table with two columns and save it as a CSV file with `csv_train.to_csv(path_train + 'annot_train.csv', sep=',', index=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e268743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd # package to create CSV\n",
    "def make_annotation_file(path_train: str, path_test: str, path_val: str, labels: list[str]):\n",
    "    ## function to iterate through the path and create an annotation csv\n",
    "    list_train = []\n",
    "    list_test = []\n",
    "    list_val = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        for file in os.listdir(path_train + label):\n",
    "            list_train.append((label + '/' + file, idx))\n",
    "\n",
    "        for file in os.listdir(path_test + label):\n",
    "            list_test.append((label + '/' + file, idx))\n",
    "\n",
    "        for file in os.listdir(path_val + label):\n",
    "            list_val.append((label + '/' + file, idx))\n",
    "\n",
    "    ## create csv-files for the annotations\n",
    "    csv_train = pd.DataFrame(list_train, columns=['', ''])\n",
    "    csv_test = pd.DataFrame(list_test, columns=['', ''])\n",
    "    csv_val = pd.DataFrame(list_val, columns=['', ''])\n",
    "\n",
    "    csv_train.to_csv(path_train + 'annot_train.csv', sep=',', index=False)\n",
    "    csv_test.to_csv(path_test + 'annot_test.csv', sep=',', index=False)\n",
    "    csv_val.to_csv(path_val + 'annot_val.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95828ef6",
   "metadata": {},
   "source": [
    "## Datasets in PyTorch\n",
    "\n",
    "The PyTorch `Dataset` controls how individual data samples are loaded and preprocessed.\n",
    "\n",
    "To make this work, we need to define three essential magic methods:\n",
    "1. `__init__`\n",
    "\n",
    "    This method runs when you create an instance of the dataset. It sets up everything needed to load data later. Typically, it includes:\n",
    "    - Reading the annotation file (e.g., a CSV) to get filenames and labels\n",
    "    - Storing the path to the dataset directory\n",
    "    - Setting up any image or label transformations (e.g., resizing, normalization)\n",
    "\n",
    "2. `__len__`\n",
    "\n",
    "    This method lets Python know how many samples are in the dataset. It's what makes len(dataset) work.\n",
    "3. `__getitem__`\n",
    "\n",
    "    This method retrieves a single sample by index (so you can do `dataset[0]`) and get an image and its label. It handles:\n",
    "    - Loading the image file from disk\n",
    "    - Fetching the correct label\n",
    "    - Applying any transformations\n",
    "\n",
    "```Python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, ...):\n",
    "        # Initialization logic (e.g., load file paths, labels)\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of samples: len(dataset)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return one sample at index `idx`: dataset[0]\n",
    "        return image, label\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f72826",
   "metadata": {},
   "source": [
    "The following code section shows you an example for a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        # Initialization logic (e.g., load file paths, labels)\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of samples: len(dataset)  \n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13789147",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "Where `Dataset` in PyTorch accesses a single data entry, the `DataLoader` uses it to reorganize the data in minibatches and randomizes them for each training epoch. Additionally, it speeds up information retrieval. We already used this in the previous tutorials for exactly this purpose.\n",
    "\n",
    "The DataLoader function needs a `Dataset` as input. Also, the batch size needs to be defined and with `shuffle=True` randomization is activated. We also recommend to set the number of workers which splits the processing load.\n",
    "\n",
    "```Python\n",
    "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "```\n",
    "\n",
    "For more details, look into the [PyTorch documentation on DataLoader](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f02fb",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Transformations are necessary to prepare the data for the neural network model, e.g. resizing, normalization and tensor conversion. It also handles data augmentation.\n",
    "\n",
    "The following code block is an example for image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd311014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4beea91",
   "metadata": {},
   "source": [
    "## Common Dataset Workflow: Putting Everything Together\n",
    "\n",
    "1. Organize your data (with annotation files or directory structure).\n",
    "2. Create a custom Dataset class to load and return (input, label) pairs.\n",
    "3. Use DataLoader to batch and shuffle samples during training.\n",
    "\n",
    "The following is an example for this workflow. The variables in the beginning are to be set according to the data structure.\n",
    "\n",
    "```Python\n",
    "train_data_path = '...'\n",
    "test_data_path = '...'\n",
    "val_data_path = '...'\n",
    "classes = ['...',...]\n",
    "batchsize = ...\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "make_annotation_file(train_data_path, test_data_path, val_data_path, classes)\n",
    "\n",
    "training_data = CustomDataset(train_data_path + 'annot_train.csv', train_data_path, transform=train_compose)\n",
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_data = CustomDataset(test_data_path + 'annot_test.csv', test_data_path, transform=test_compose)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
