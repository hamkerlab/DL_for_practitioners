{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8323bb15d60962af",
   "metadata": {},
   "source": [
    "# Tutorial 3.1: Evaluating Neural Network Performance: Metrics and Visualizations\n",
    "\n",
    "Author: [Erik Syniawa](mailto:erik.syniawa@informatik.tu-chemnitz.de)\n",
    "\n",
    "This notebook introduces essential quantitative measures to evaluate neural network learning performance and how they are implemented in some of the Utils functions. We'll further explore key metrics and visualization techniques that help us understand:\n",
    "\n",
    "1. How efficiently a model learns\n",
    "2. Whether it's overfitting or underfitting\n",
    "3. How its internal representations capture the structure of the data\n",
    "4. How it performs on individual classes and examples\n",
    "\n",
    "These insights are crucial for diagnosing problems, tuning hyperparameters, and ultimately building better models.\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and the custom functions we'll use throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326c1f02c6a3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# We append the parent directory into the sys.path here. Normally in python you create modules with __init__ but since we are working with jupyter notebook we simply do this:\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "    \n",
    "\n",
    "# Import our custom functions\n",
    "from Utils.functions import train_model, evaluate_model, test_model, get_model_predictions\n",
    "from Utils.plotting import (visualize_training_results, \n",
    "                            visualize_test_results, \n",
    "                            visualize_misclassified, )\n",
    "\n",
    "# some little useful helper functions                            \n",
    "from Utils.little_helpers import set_seed, timer\n",
    "\n",
    "# set random seed for reproducibility\n",
    "set_seed(1729)\n",
    "\n",
    "# Importing PyTorch and show version/ device info\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__} running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef6400a676c76a",
   "metadata": {},
   "source": [
    "### 1.1 The Importance of Setting Random Seeds in Deep Learning\n",
    "\n",
    "Random seeds control the initialization of random number generators, which affect many aspects of neural network training:\n",
    "\n",
    "1. **Weight initialization**: Random initial weights can lead to different local minima\n",
    "2. **Data shuffling**: Different batch orderings affect gradient updates\n",
    "3. **Dropout patterns**: Randomized dropout masks change which neurons are active\n",
    "4. **Data augmentation**: Random transformations create different training examples\n",
    "\n",
    "#### Benefits of fixed seeds:\n",
    "\n",
    "1. **Reproducibility**: Ensures experiments can be reproduced exactly\n",
    "2. **Debugging**: Helps distinguish between bugs and random variation\n",
    "3. **Fair Comparisons**: Allows different models/approaches to be compared under identical conditions\n",
    "4. **Scientific Rigor**: Makes results replicable by other researchers\n",
    "\n",
    "#### Why CUDA-specific seed setting is necessary:\n",
    "\n",
    "Neural network training often uses GPU acceleration through CUDA. Setting CUDA-specific seeds is critical because:\n",
    "\n",
    "1. **GPU Parallelism**: GPUs use parallel processing with their own random number generators\n",
    "2. **CUDA-specific operations**: Some operations have GPU-specific implementations with different random behaviors\n",
    "3. **Multi-GPU setups**: Each GPU needs its own seed for consistent behavior\n",
    "\n",
    "Our `set_seed(1729)` function addresses these issues by:\n",
    "\n",
    "- Setting Python's `random` seed for general randomization -> `random.seed(1729)`\n",
    "- Setting NumPy's seed for array operations -> `np.random.seed(1729)`\n",
    "- Setting PyTorch's CPU seed with `torch.manual_seed(1729)`\n",
    "- Setting CUDA seeds with `torch.cuda.manual_seed(1729)` for the current device\n",
    "- Setting all CUDA device seeds with `torch.cuda.manual_seed_all(1729)` for using multiple GPUs\n",
    "- Making cuDNN deterministic with `torch.backends.cudnn.deterministic = True`\n",
    "- Disabling cuDNN benchmarking with `torch.backends.cudnn.benchmark = False`\n",
    "\n",
    "The last two settings are particularly important but come with performance costs. Setting ``cudnn.deterministic = True`` forces cuDNN to use deterministic algorithms instead of selecting the fastest but potentially non-deterministic implementation. Setting ``cudnn.benchmark = False`` prevents cuDNN from benchmarking multiple convolution algorithms and selecting the fastest one, which can introduce non-determinism. For detailed information look [here](https://pytorch.org/docs/stable/notes/randomness.html).\n",
    "\n",
    "> **Note**: Setting `torch.backends.cudnn.deterministic = True` and `torch.backends.cudnn.benchmark = False` can significantly slow down training, especially for convolutional networks. This trade-off between reproducibility and performance is an important consideration in research vs. production environments.\n",
    "\n",
    "## 2. The Main Objective of Deep Learning: Learning Representations\n",
    "\n",
    "The core idea behind deep learning is to automatically learn meaningful representations of data. As highlighted in [LeCun, Bengio & Hinton (2015)](https://www.nature.com/articles/nature14539), deep learning methods are representation-learning methods with multiple levels of representation. Each layer transforms the representation from the previous level into a more abstract representation, allowing the model to learn complex patterns from raw data.\n",
    "Consider this example: Imagine a model trained to distinguish between cats and cows based on images. When we look at the final accuracy, we might be satisfied with around 90% correct classifications. However, what the model actually \"learned\" remains hidden.\n",
    "\n",
    "It's possible the model has learned legitimate discriminative features (cats have pointed ears and whiskers, cows have horns and spots). But it might instead have learned from statistical biases in the training data - perhaps cows are more frequently photographed outdoors while cats are photographed indoors. In this case, the model might be associating blue skies and green grass with cows, and indoor lighting and furniture with cats.\n",
    "\n",
    "This creates a problem: these associations are inherently biased by the training dataset and can lead to false classifications when cats are photographed outdoors or cows are photographed in stables. By visualizing the [embeddings](#41-why-embeddings-matter), we might discover that images are clustered primarily by background environment rather than by the animal's intrinsic features - revealing a potential reliability issue that accuracy alone wouldn't show.\n",
    "The power of deep learning comes from its ability to progressively learn more abstract and useful representations:\n",
    "\n",
    "- Early layers might detect simple patterns like edges and colors\n",
    "- Middle layers combine these into more complex patterns like textures and shapes\n",
    "- Later layers assemble these into object parts and abstract object representations\n",
    "\n",
    "This hierarchical representation learning is what enables deep networks to solve complex tasks that were previously intractable with traditional machine learning approaches.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/features.png\" width=\"750\"/>\n",
    "    <p><i>Figure 1: This image from <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">Zeiler & Fergus (2014)</a> visualizes what different layers in their convolutional neural network learn by showing reconstructions of features that most strongly activate specific feature maps. The progression clearly demonstrates the hierarchical nature of learned representations: lower layers (1-2) detect simple edges and textures, middle layers (3-4) capture more complex patterns and object parts (like dog faces or bird legs), while deeper layers (5) represent entire objects with significant pose variation.</i></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c80c385463a439",
   "metadata": {},
   "source": [
    "## 3. Data Splitting and Cross-Validation\n",
    "\n",
    "Before we dive into loss functions and other evaluation metrics, we need to establish proper data splitting strategies to ensure reliable model evaluation. How we divide our data directly impacts our ability to assess model generalization.\n",
    "\n",
    "### 3.1 The Three-Way Split: Train, Validation, and Test\n",
    "\n",
    "- Training Set (60-80%): Used to optimize model parameters through backpropagation\n",
    "- Validation Set (10-20%): For tracking accuracy and early stopping decisions. Can also be used for hyperparameter tuning (for example `ReduceLROnPlateau` Scheduler).\n",
    "- Test Set (10-20%): Used only for final model evaluation, never for making modeling decisions. Note that in some datasets like in ImageNet the true labels are not even available for the test set for calculating the accuracy.\n",
    "\n",
    "> What can be the advantages of using a validation set that the model has never seen before?\n",
    "\n",
    "### 3.2 Creating an Example Dataset\n",
    "\n",
    "Let's create a dataset for an exemplary animal classification task that we'll use throughout this notebook. We'll intentionally make it imbalanced to simulate real-world challenges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e9eda59f963a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic animal classification dataset with class imbalance\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our class names\n",
    "class_names = ['cat', 'dog', 'bird', 'cow', 'lynx']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Create a synthetic classification dataset with imbalanced classes\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,  # 20 features per sample\n",
    "    n_informative=10,  # 10 informative features\n",
    "    n_redundant=5,    # 5 redundant features\n",
    "    n_classes=num_classes,      # 5 classes (matching our class_names)\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.4, 0.3, 0.15, 0.1, 0.05],  # Making 'cat' most common, 'lynx' least common\n",
    "    class_sep=1.5,    # Class separation factor\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Let's check class distribution\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(unique_classes)), class_counts, color='skyblue')\n",
    "plt.xticks(range(len(unique_classes)), [class_names[c] for c in unique_classes])\n",
    "plt.xlabel('Animal Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Animal Classification Dataset')\n",
    "\n",
    "# Add count labels above bars\n",
    "for i, (count, bar) in enumerate(zip(class_counts, bars)):\n",
    "    plt.text(i, count + 10, str(count), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c97132e4f847cc",
   "metadata": {},
   "source": [
    "This creates an imbalanced dataset where cats are most common and lynxes are rare - similar to what we might encounter in real-world datasets. This imbalance will help us demonstrate the importance of proper evaluation techniques.\n",
    "\n",
    "### 3.3 Basic Data Splitting Approaches\n",
    "\n",
    "#### 3.3.1 Random Splitting\n",
    "\n",
    "The simplest approach is random splitting, which works well when data is independently and identically distributed (i.i.d.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08626ee28eaa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: separate test set (20%)\n",
    "train_val_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_val_size\n",
    "train_val_dataset, test_dataset = random_split(dataset, [train_val_size, test_size])\n",
    "\n",
    "# Second split: separate validation set from training (25% of train_val, which is 20% of total)\n",
    "train_size = int(0.75 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Dataset sizes: Total={len(dataset)}, Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa7a0d2d2978e9",
   "metadata": {},
   "source": [
    "#### 3.3.2 The Problem with Random Splitting for Imbalanced Data\n",
    "With random splitting on our imbalanced animal dataset, we might end up with splits that don't represent the true distribution. Let's examine the class distribution across our random splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72836bf101d2b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get class distribution from subset indices\n",
    "def get_class_distribution(dataset, indices):\n",
    "    # Extract labels for the specified indices\n",
    "    labels = [dataset.tensors[1][i].item() for i in indices]\n",
    "    class_counts = np.bincount(labels, minlength=num_classes)\n",
    "    return class_counts\n",
    "\n",
    "# Get indices from each split\n",
    "train_indices = train_dataset.indices\n",
    "val_indices = val_dataset.indices\n",
    "test_indices = test_dataset.indices\n",
    "\n",
    "# Calculate class distributions\n",
    "train_dist = get_class_distribution(dataset, train_indices)\n",
    "val_dist = get_class_distribution(dataset, val_indices)\n",
    "test_dist = get_class_distribution(dataset, test_indices)\n",
    "\n",
    "# Visualize distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, train_dist / train_dist.sum(), width, label='Train', color='skyblue')\n",
    "plt.bar(x, val_dist / val_dist.sum(), width, label='Validation', color='lightgreen')\n",
    "plt.bar(x + width, test_dist / test_dist.sum(), width, label='Test', color='salmon')\n",
    "\n",
    "plt.xlabel('Animal Class')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Class Distribution Across Random Splits')\n",
    "plt.xticks(x, class_names)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12458868f823ffed",
   "metadata": {},
   "source": [
    "Notice how the proportions of each class can vary between splits. This could be problematic - especially for rare classes like 'lynx', which might be underrepresented or even in the worst case absent in some splits.\n",
    "\n",
    "#### 3.3.3 Stratified Splitting\n",
    "For imbalanced datasets like ours, stratified splitting maintains the class distribution across all partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bdb21257df770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split using scikit-learn\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1729, stratify=y  # stratify parameter ensures class balance\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=1729, stratify=y_train_val\n",
    ")\n",
    "\n",
    "# Verify class distributions in each split\n",
    "def plot_class_distribution(y_train, y_val, y_test):\n",
    "    train_dist = np.bincount(y_train, minlength=num_classes) / len(y_train)\n",
    "    val_dist = np.bincount(y_val, minlength=num_classes) / len(y_val)\n",
    "    test_dist = np.bincount(y_test, minlength=num_classes) / len(y_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, train_dist, width, label='Train', color='skyblue')\n",
    "    plt.bar(x, val_dist, width, label='Validation', color='lightgreen')\n",
    "    plt.bar(x + width, test_dist, width, label='Test', color='salmon')\n",
    "    \n",
    "    plt.xlabel('Animal Class')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.title('Class Distribution Across Stratified Splits')\n",
    "    plt.xticks(x, class_names)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(y_train, y_val, y_test)\n",
    "\n",
    "# Convert back to PyTorch datasets for model training\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93516e7cd9f078",
   "metadata": {},
   "source": [
    "With stratified sampling, each split maintains the same proportion of each animal class. This is crucial for rare classes like 'lynx', ensuring that our model sees examples from all classes during training and is evaluated fairly during validation and testing.\n",
    "\n",
    "### 3.4 Cross-Validation Techniques\n",
    "\n",
    "For smaller datasets or when we need more robust evaluation, cross-validation provides a way to use our data more efficiently.\n",
    "\n",
    "#### 3.4.1 K-Fold Cross-Validation\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/cross_validation.png\" width=\"750\"/>\n",
    "    <p><i>Figure 2: k-fold cross validation. Source: https://en.wikipedia.org/wiki/Cross-validation_(statistics)</i></p>\n",
    "</div>\n",
    "\n",
    "K-fold cross-validation divides the data into $k$ equally sized folds and trains $k$ models, each using $k-1$ folds for training and the remaining fold for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e2951efd0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# K-fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Visualize the folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "sample_range = np.arange(len(X))\n",
    "\n",
    "# Create a colored array for visualization\n",
    "fold_visualization = np.zeros(len(X))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "    print(f\"  Training: {len(train_idx)} samples\")\n",
    "    print(f\"  Validation: {len(val_idx)} samples\")\n",
    "    \n",
    "    # Mark this fold's validation indices in our visualization array\n",
    "    fold_visualization[val_idx] = fold + 1\n",
    "    \n",
    "    # Check class distribution in this fold\n",
    "    train_class_dist = np.bincount(y[train_idx], minlength=num_classes)\n",
    "    val_class_dist = np.bincount(y[val_idx], minlength=num_classes)\n",
    "    \n",
    "    print(f\"  Training class distribution: {train_class_dist}\")\n",
    "    print(f\"  Validation class distribution: {val_class_dist}\")\n",
    "    print()\n",
    "\n",
    "# Plot the fold assignment for a subset of data points for clearer visualization\n",
    "subset_size = 500  # Show first 500 samples for clarity\n",
    "plt.scatter(sample_range[:subset_size], fold_visualization[:subset_size], \n",
    "            c=fold_visualization[:subset_size], cmap='viridis', \n",
    "            marker='|', s=100, alpha=0.8)\n",
    "\n",
    "plt.yticks(np.arange(0, k_folds+1))\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Fold Assignment (0 = not in validation set)')\n",
    "plt.title('K-Fold Cross-Validation Data Splitting')\n",
    "plt.colorbar(label='Validation Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Let's also look at how class distribution varies across folds\n",
    "fold_class_dist = np.zeros((k_folds, num_classes))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Count class occurrences in each validation fold\n",
    "    fold_class_dist[fold] = np.bincount(y[val_idx], minlength=num_classes)\n",
    "\n",
    "# Plot class distribution across folds\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.15\n",
    "index = np.arange(num_classes)\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    plt.bar(index + fold * bar_width, fold_class_dist[fold], bar_width,\n",
    "            label=f'Fold {fold+1}')\n",
    "\n",
    "plt.xlabel('Animal Class')\n",
    "plt.ylabel('Number of Samples in Validation Set')\n",
    "plt.title('Class Distribution Across Validation Folds')\n",
    "plt.xticks(index + bar_width * (k_folds-1)/2, class_names)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d02d91fdd8a13",
   "metadata": {},
   "source": [
    "#### 3.4.2 Stratified K-Fold for Imbalanced Data\n",
    "\n",
    "For our imbalanced animal dataset, standard K-Fold might result in some folds having few or no examples of the rare 'lynx' class just like in random data splitting (see section [3.3.2](#332-the-problem-with-random-splitting-for-imbalanced-data)). Stratified K-Fold can as well maintain same class distribution for each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4fbaa48a2ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Stratified K-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Visualize fold distributions\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    # Calculate class distribution for this fold\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    train_dist = np.bincount(y_train_fold, minlength=num_classes) / len(y_train_fold)\n",
    "    val_dist = np.bincount(y_val_fold, minlength=num_classes) / len(y_val_fold)\n",
    "    \n",
    "    # Plot distributions\n",
    "    plt.subplot(2, 3, fold+1)\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_dist, width, label='Train', color='skyblue')\n",
    "    plt.bar(x + width/2, val_dist, width, label='Validation', color='salmon')\n",
    "    \n",
    "    plt.title(f'Fold {fold+1} Class Distribution')\n",
    "    plt.xlabel('Animal Class')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(x, class_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527635c5994deb2",
   "metadata": {},
   "source": [
    "Notice how Stratified K-Fold maintains similar class proportions across all folds. This is particularly important for our animal classification task where 'lynx' examples are rare.\n",
    "\n",
    "### 3.5 Handling Class Imbalance Beyond Stratification\n",
    "#### 3.5.1 Weighted Loss Functions\n",
    "\n",
    "Stratified sampling is just one approach to handle imbalanced data. But even with stratified sampling, we might still face challenges during training. For instance, if the model sees many more 'cat' examples than 'lynx' examples, it might learn to favor the former. So we need to ensure that the model learns to treat all classes equally. One way to do this is by using weighted loss functions so we can penalize errors on minority classes (like 'lynx') more heavily:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a6bd1ca06f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights inversely proportional to class frequencies\n",
    "class_counts = np.bincount(y, minlength=num_classes)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = class_weights / np.sum(class_weights) * num_classes  # Normalize\n",
    "\n",
    "# Display weights\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(class_names)), class_weights, color='skyblue')\n",
    "plt.xticks(range(len(class_names)), class_names)\n",
    "plt.xlabel('Animal Class')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Class Weights for Imbalanced Data')\n",
    "\n",
    "# Add weight values above bars\n",
    "for i, (weight, bar) in enumerate(zip(class_weights, bars)):\n",
    "    plt.text(i, weight + 0.1, f'{weight:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert to PyTorch tensor for the loss function\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Use weights in CrossEntropyLoss\n",
    "weighted_criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08ec64b0160499",
   "metadata": {},
   "source": [
    "#### 3.5.2 Resampling Techniques\n",
    "\n",
    "Alternatively, we can modify the data distribution through:\n",
    "\n",
    "- **Oversampling**: Duplicate samples from minority classes like 'lynx'\n",
    "- **Undersampling**: Remove samples from majority classes like 'cat'\n",
    "- **Combined approach**: Balance the dataset using both oversampling and undersampling\n",
    "\n",
    "But please beware that these techniques can lead to other problems as well. For example, oversampling can lead to overfitting, as the model may learn to memorize duplicated samples rather than generalize from them. Undersampling can lead to loss of valuable information, especially if the removed samples are informative. Also undersampling may not be possible for smaller datasets. Note that there are no one-size-fits-all solutions. The best approach depends on the specific dataset and task at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908d89cc9f8e47",
   "metadata": {},
   "source": [
    "# 4. Understanding Training and Validation Metrics\n",
    "Now that we have properly split our data, we need to understand which metrics to use to evaluate our models and how these metrics are affected by our splitting strategies. The way we split our data directly impacts the reliability of our evaluation metrics - appropriate splitting ensures that our metrics actually reflect how well our model will perform on **unseen** data.\n",
    "\n",
    "### 4.1 Loss Functions: The Optimization Target\n",
    "\n",
    "The loss function is the core metric that guides neural network optimization. It quantifies how far the model's predictions are from the ground truth, providing a signal for how to adjust the model's parameters.\n",
    "Loss functions are mathematical formulas that measure the error between predictions and actual values. The specific choice of loss function depends on the task.\n",
    "\n",
    "#### 4.1.1 Classification Loss: Cross-Entropy\n",
    "\n",
    "For classification tasks, the most common loss function is *Cross-Entropy Loss*. It measures the difference between the predicted probability distribution and the true distribution.\n",
    "\n",
    "**Binary Cross-Entropy**\n",
    "\n",
    "For binary classification (two classes), we use Binary Cross-Entropy:\n",
    "\n",
    "$\\text{BCE}(y, \\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the true label (0 or 1)\n",
    "- $\\hat{y}_i$ is the predicted class label\n",
    "- $N$ is the number of samples\n",
    "\n",
    "Binary cross-entropy requires input to be probabilities. You initialize this with ``nn.BCELoss()`` in PyTorch.\n",
    "- If you have raw logits (not probabilities), you can use ``nn.BCEWithLogitsLoss()`` which combines a sigmoid layer and the binary cross-entropy loss in one single class. This is numerically more stable than using a plain Sigmoid followed by a BCELoss.\n",
    "\n",
    "**Categorical Cross-Entropy**\n",
    "\n",
    "For multi-class classification, we use Categorical Cross-Entropy:\n",
    "\n",
    "$\\text{CE}(y, \\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_{i,c}$ is 1 if sample $i$ belongs to class $c$ and 0 otherwise (one-hot encoding)\n",
    "- $\\hat{y}_{i,c}$ is the predicted probability that sample $i$ belongs to class $c$\n",
    "- $C$ is the number of classes\n",
    "- $N$ is the number of samples\n",
    "\n",
    "Categorical cross-entropy combines LogSoftmax and NLLLoss (negative log likelihood loss) in one single class. Importantly, this expects you to give raw logits (**not** softmax outputs) and class indices (**not** one-hot encoded labels). You initialize this with ``nn.CrossEntropyLoss()`` in PyTorch.\n",
    "\n",
    "Let's see a practical example of how the model makes \"decisions\" based on logits and how to calculate the loss based on these logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e8ffa5c853777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class_names = ['cat', 'dog', 'bird', 'cow', 'lynx']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Create dummy model outputs (logits) for 3 examples\n",
    "# These are the raw scores before softmax\n",
    "logits = torch.tensor([\n",
    "    [2.0, 0.5, 0.2, 0.1, 0.1],  # Example 1: Model predicts cat with high \"confidence\"\n",
    "    [0.2, 0.5, 1.8, 0.3, 0.2],  # Example 2: Model predicts bird with medium \"confidence\"\n",
    "    [0.4, 0.4, 0.4, 0.4, 0.5]   # Example 3: Model predicts lynx with low \"confidence\"\n",
    "])\n",
    "\n",
    "# Convert logits to probabilities with softmax\n",
    "probs = F.softmax(logits, dim=1)\n",
    "for i, prob in enumerate(probs):\n",
    "    pred_class = class_names[torch.argmax(prob).item()]\n",
    "\n",
    "# Plot the predicted probabilities\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Predicted Probabilities from Logits', fontsize=16)\n",
    "for i, (prob, ax) in enumerate(zip(probs, axes)):\n",
    "    prob_np = prob.detach().numpy()\n",
    "    bars = ax.bar(np.arange(len(class_names)), prob_np, color='skyblue')\n",
    "    \n",
    "    # Highlight the highest probability\n",
    "    max_idx = np.argmax(prob_np)\n",
    "    bars[max_idx].set_color('navy')\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'Example {i+1}: Predicts {class_names[max_idx]}')\n",
    "    \n",
    "    # Add values above bars\n",
    "    for j, p in enumerate(prob_np):\n",
    "        ax.annotate(f'{p:.2f}', (j, p), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c7c1c374a9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the loss, we need to know the true labels for these examples.\n",
    "labels = torch.tensor([0, 2, 3])  # One label per logits\n",
    "n_picks = len(labels)\n",
    "\n",
    "# 1. Convert logits to log probabilities (log-softmax)\n",
    "log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "# 2. Pick the log probability corresponding to the true class for each example\n",
    "picked_log_probs = log_probs[range(n_picks), labels]\n",
    "\n",
    "# 3. Take the negative mean\n",
    "manual_loss = -picked_log_probs.mean()\n",
    "print(f\"Loss calculated manually: {manual_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82983523bbdea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Or we can use the built-in CrossEntropyLoss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, labels)\n",
    "print(f\"Loss using CrossEntropyLoss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7b763d6310d06",
   "metadata": {},
   "source": [
    "#### 4.1.2 Confidence and Loss\n",
    "\n",
    "Despite the vast difference in confidence between Example 1 and 2 to 3, all are treated as the same definitive predictions during inference. In training however uncertain correct predictions get more penalized than confident correct predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b361a7719b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the loss behaves for different scenarios:\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "# High confidence, correct prediction | Confidence is another term for the model's certainty about its prediction eg the \"probability\" of the predicted class\n",
    "high_conf_correct = torch.tensor([[10.0, 0.1, 0.1, 0.1, 0.1]])  # Very confident it's a cat\n",
    "loss_high_conf_correct = loss_fn(high_conf_correct, torch.tensor([0]))\n",
    "print(f\"High confidence, correct prediction: {loss_high_conf_correct.item():.4f}\")\n",
    "\n",
    "# Low confidence, correct prediction\n",
    "low_conf_correct = torch.tensor([[1.0, 0.8, 0.7, 0.6, 0.5]])  # Somewhat confident it's a cat\n",
    "loss_low_conf_correct = loss_fn(low_conf_correct, torch.tensor([0]))\n",
    "print(f\"Low confidence, correct prediction: {loss_low_conf_correct.item():.4f}\")\n",
    "\n",
    "# High confidence, wrong prediction\n",
    "high_conf_wrong = torch.tensor([[0.1, 0.1, 0.1, 0.1, 10.0]])  # Very confident it's a lynx (but it's a cat)\n",
    "loss_high_conf_wrong = loss_fn(high_conf_wrong, torch.tensor([0]))\n",
    "print(f\"High confidence, wrong prediction: {loss_high_conf_wrong.item():.4f}\")\n",
    "\n",
    "# Low confidence, wrong prediction\n",
    "low_conf_wrong = torch.tensor([[0.5, 0.6, 0.7, 0.8, 1.0]])  # Somewhat confident it's a lynx (but it's a cat)\n",
    "loss_low_conf_wrong = loss_fn(low_conf_wrong, torch.tensor([0]))\n",
    "print(f\"Low confidence, wrong prediction: {loss_low_conf_wrong.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e69efc0e33b117",
   "metadata": {},
   "source": [
    "> **Note**: You can modulate the losses based on confidence by using label smoothing. Label smoothing is a technique used to prevent the model from becoming too confident in its predictions. Instead of assigning a probability of 1 to the true class and 0 to all others, we assign a small probability (e.g., 0.9) to the true class and distribute the remaining probability (e.g., 0.1) evenly among the other classes. This encourages the model to be less certain about its predictions and can help improve generalization. Try it out by inserting `nn.CrossEntropyLoss(label_smoothing=0.1)` above and see how the losses change!\n",
    "\n",
    "#### 4.1.3 Regression Losses\n",
    "\n",
    "For regression tasks, where we predict continuous values, the most common loss function is Mean Squared Error (MSE or L2 loss). It measures the average squared difference between predicted and true values:\n",
    "\n",
    "$\\text{MSE}(y, \\hat{y}) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the true value\n",
    "- $\\hat{y}_i$ is the predicted value\n",
    "- $N$ is the number of samples\n",
    "\n",
    "In PyTorch, this is implemented as ``nn.MSELoss()``.\n",
    "\n",
    "The Mean Absolute Error (MAE) is another common regression loss function, which measures the average absolute difference between predicted and true values:\n",
    "\n",
    "$\\text{MAE}(y, \\hat{y}) = \\frac{1}{N}\\sum_{i=1}^{N} |y_i - \\hat{y}_i|$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the true value\n",
    "- $\\hat{y}_i$ is the predicted value\n",
    "- $N$ is the number of samples\n",
    "\n",
    "In PyTorch, this is implemented as ``nn.L1Loss()``.\n",
    "\n",
    "This tutorial will focus on classification tasks, so we won't dwell on this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92d760fa618030",
   "metadata": {},
   "source": [
    "#### 4.1.4 Loss Monitoring\n",
    "\n",
    "During training, the model parameters are updated to minimize the loss, effectively improving the model's predictions. Monitoring the loss is crucial for understanding how well the model is learning. We typically track both training and validation loss. The former indicates how well the model fits the training data, while the latter shows how well it generalizes to **unseen** data.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/losses.png\" width=\"1000\"/>\n",
    "    <p><i>Figure 3: Different loss behaviors</i></p>\n",
    "</div>\n",
    "\n",
    "Let's examine why monitoring loss is crucial:\n",
    "\n",
    "1. **Good Convergence**: Both training and validation loss decrease and converge to similar values. The gap between them remains small, indicating the model generalizes well.\n",
    "\n",
    "2. **Overfitting**: Training loss continues to decrease, but validation loss starts increasing after a certain point. This indicates the model is memorizing the training data rather than learning generalizable patterns.\n",
    "\n",
    "3. **Underfitting**: Both losses plateau at relatively high values. This suggests the model lacks capacity to capture the underlying patterns in the data.\n",
    "\n",
    "### 4.2 Accuracy: The Interpretable Metric\n",
    "\n",
    "While loss functions drive optimization, accuracy is often more interpretable. It measures the percentage of correct predictions out of total predictions. To be complete:\n",
    "\n",
    "$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/accuracy.png\" width=\"1000\"/>\n",
    "    <p><i>Figure 4: Different accuracy behaviors</i></p>\n",
    "</div>\n",
    "\n",
    "### 4.3 Using the `visualize_training_results` Function\n",
    "\n",
    "Our `visualize_training_results` function makes it easy to plot both loss and accuracy during training:\n",
    "\n",
    "```python\n",
    "# The history will be returned from train_model\n",
    "\n",
    "history = {\n",
    "    'train_loss': train_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'train_acc': train_acc,\n",
    "    'val_acc': val_acc\n",
    "}\n",
    "\n",
    "# Visualize the results\n",
    "visualize_training_results(\n",
    "    train_losses=history['train_loss'],\n",
    "    train_accs=history['train_acc'],\n",
    "    test_losses=history['val_loss'],\n",
    "    test_accs=history['val_acc'],\n",
    "    output_dir=None  # Set to a path to save the plot\n",
    ")\n",
    "```\n",
    "### 4.4 Implementing Early Stopping\n",
    "\n",
    "To prevent overfitting, we often use early stopping. Our `train_model` function has built-in early stopping logic:\n",
    "\n",
    "```python\n",
    "# Not run here, but this is how you would use it\n",
    "history = train_model(\n",
    "    model=model,  # Your model\n",
    "    train_loader=train_loader,  # Your training data\n",
    "    val_loader=val_loader,  # Your validation data\n",
    "    criterion=nn.CrossEntropyLoss(),  # Your loss function\n",
    "    optimizer=optimizer,  # Your optimizer\n",
    "    scheduler=scheduler,  # Your learning rate scheduler (optional can be None)\n",
    "    device=device,  # Your device to train on\n",
    "    num_epochs=100,  # Number of epochs\n",
    "    checkpoint_path='./model_checkpoints/',  # Path to save the model with the best performance in `monitor`\n",
    "    patience=10,  # Stop if no improvement for 10 epochs\n",
    "    min_delta=0.001,  # Minimum change to qualify as improvement\n",
    "    monitor='val_loss',  # Monitor validation loss for early stopping and checkpointing\n",
    ")\n",
    "```\n",
    "\n",
    "Early stopping can help us find the sweet spot between underfitting and overfitting. In the example above we monitor the validation loss and stop training if it doesn't improve for 10 epochs. `min_delta` is the minimum change to qualify as an improvement. This prevents early stopping from being triggered by small fluctuations in the validation loss.\n",
    "\n",
    "## 5. Understanding Test-Time Evaluation\n",
    "\n",
    "Once a model is trained, we need to evaluate it thoroughly on a test set.\n",
    "\n",
    "### 5.1 Beyond Simple Accuracy\n",
    "\n",
    "Overall accuracy can be misleading, especially for imbalanced datasets. Our `test_model` function provides detailed metrics. Let's look at a simulated example of test results, why this might be a good idea:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccc934cb09845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the existing dataset from Section 2 instead of creating a new one\n",
    "num_samples = len(y)\n",
    "\n",
    "# Simulate model predictions and confidences\n",
    "per_image_data = {\n",
    "    'image_idx': list(range(num_samples)),\n",
    "    'true_label': y,\n",
    "    'true_class': [class_names[label] for label in y],\n",
    "    'predicted_label': [],\n",
    "    'predicted_class': [],\n",
    "    'confidence': np.random.uniform(0.7, 1.0, num_samples),\n",
    "    'correct': [],\n",
    "}\n",
    "\n",
    "# Define class-specific accuracy rates\n",
    "class_accuracy_rates = {\n",
    "    'cat': 0.95,   # High accuracy for common class\n",
    "    'dog': 0.85,\n",
    "    'bird': 0.70,\n",
    "    'cow': 0.65,\n",
    "    'lynx': 0.50   # Low accuracy for rare class\n",
    "}\n",
    "\n",
    "# Simulate predictions\n",
    "for i in range(num_samples):\n",
    "    true_label = per_image_data['true_label'][i]\n",
    "    true_class = class_names[true_label]\n",
    "    \n",
    "    # Determine if prediction is correct based on class-specific accuracy\n",
    "    correct_prob = class_accuracy_rates[true_class]\n",
    "    is_correct = np.random.random() < correct_prob\n",
    "    \n",
    "    # Get predicted label\n",
    "    if is_correct:\n",
    "        pred_label = true_label\n",
    "    else:\n",
    "        # If wrong, randomly choose another class\n",
    "        pred_label = np.random.choice([c for c in range(num_classes) if c != true_label])\n",
    "    \n",
    "    # Store results\n",
    "    per_image_data['predicted_label'].append(pred_label)\n",
    "    per_image_data['predicted_class'].append(class_names[pred_label])\n",
    "    per_image_data['correct'].append(is_correct)\n",
    "\n",
    "# Create per-image DataFrame\n",
    "per_image_df = pd.DataFrame(per_image_data)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = 100 * per_image_df['correct'].mean()\n",
    "\n",
    "# Create aggregated metrics\n",
    "aggregate_data = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Filter for this class\n",
    "    class_samples = per_image_df[per_image_df['true_label'] == i]\n",
    "    \n",
    "    if len(class_samples) > 0:\n",
    "        class_accuracy = 100 * class_samples['correct'].mean()\n",
    "        # Average confidence for correct predictions only\n",
    "        correct_samples = class_samples[class_samples['correct']]\n",
    "        avg_confidence = correct_samples['confidence'].mean() if len(correct_samples) > 0 else 0\n",
    "        \n",
    "        aggregate_data.append({\n",
    "            'class_name': class_name,\n",
    "            'accuracy': class_accuracy,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'support': len(class_samples)\n",
    "        })\n",
    "\n",
    "# Create aggregate DataFrame\n",
    "aggregate_df = pd.DataFrame(aggregate_data)\n",
    "\n",
    "# Print summary\n",
    "print(f'Overall Test Accuracy: {overall_accuracy:.2f}%')\n",
    "print(\"\\nPer-class Performance:\")\n",
    "print(aggregate_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567fd64904816ba",
   "metadata": {},
   "source": [
    "> Do you think that the model has learned to generalize well to categorize these classes?\n",
    "\n",
    "### 5.2 Visualizing Test Results\n",
    "\n",
    "After training and evaluating the model, it's important to visualize the results to gain deeper insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f1d1a5c24adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our visualization function on the simulated test results\n",
    "visualize_test_results(\n",
    "    aggregate_df=aggregate_df,\n",
    "    per_image_df=per_image_df,\n",
    "    overall_accuracy=overall_accuracy,\n",
    "    output_dir=None  # Set to a path to save the plot. If None the plot gets displayed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020da4270c5a389",
   "metadata": {},
   "source": [
    "### Key insights from test visualizations:\n",
    "\n",
    "1. **Per-class accuracy**: Reveals which classes the model struggles with, highlighting potential issues with the data or model architecture.\n",
    "\n",
    "2. **Confidence vs. accuracy**: Shows if the model is well-calibrated. Ideally, confidence should align with accuracy. \n",
    "\n",
    "3. **Confusion matrix**: Identifies specific pairs of classes that get confused with each other.\n",
    "\n",
    "4. **Confidence distribution**: Helps understand if the model is making confident predictions and if those confident predictions are actually correct.\n",
    "\n",
    "## 6. Understanding Neural Network Embeddings\n",
    "\n",
    "Deep learning models create internal representations (embeddings) of the input data. These embeddings capture the essential features that the model uses for classification, typically encoded as vectors in a high-dimensional space where similar concepts are positioned close together.\n",
    "\n",
    "### 6.1 Why Embeddings Matter\n",
    "\n",
    "Deep learning models create internal representations (embeddings) of the input data (see [introduction](#12-the-main-objective-of-deep-learning-learning-representations)). These embeddings capture the essential features that the model uses for classification, typically encoded as vectors in a high-dimensional space where similar concepts are positioned close together.\n",
    "\n",
    "In summary, examining embeddings helps us:\n",
    "\n",
    "- **Reveal feature learning**: Show what information the model extracts from raw data\n",
    "- **Expose clustering structure**: Similar inputs should have similar embeddings\n",
    "- **Help identify failure modes**: Misclassified examples may have unusual embedding patterns\n",
    "- **Provide interpretability**: Show the model's \"understanding\" of the data\n",
    "\n",
    "In our example we might assume that cats and lynx's are more similar in appearance than cats and cows, because they share the same feature (whiskers and ears). These similarity can be reflected in the embeddings.\n",
    "\n",
    "### 6.2 Visualizing Embeddings with Dimension Reduction\n",
    "\n",
    "High-dimensional embeddings are hard to visualize directly. Dimension reduction techniques like t-SNE and PCA help us visualize them in 2D or 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee0b188df615f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from Utils.plotting import visualize_embeddings_tsne, visualize_embeddings_pca\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "samples_per_class = 500\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create clustered embeddings for each class\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    # Create a cluster center for this class\n",
    "    center = np.random.normal(0, 1, embedding_dim)\n",
    "    \n",
    "    # Generate samples around this center\n",
    "    class_samples = center + np.random.normal(0, 1.5, (samples_per_class, embedding_dim))\n",
    "    \n",
    "    embeddings.append(class_samples)\n",
    "    labels.append(np.ones(samples_per_class) * class_idx)\n",
    "\n",
    "# Combine all classes\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "# Visualize using t-SNE\n",
    "visualize_embeddings_tsne(\n",
    "    embeddings=embeddings,\n",
    "    labels=labels,\n",
    "    output_dir=None,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Visualize using PCA\n",
    "visualize_embeddings_pca(\n",
    "    embeddings=embeddings,\n",
    "    labels=labels,\n",
    "    output_dir=None,\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aad3efe28134ce",
   "metadata": {},
   "source": [
    "\n",
    "### 6.3 Understanding t-SNE vs PCA\n",
    "\n",
    "Why is the clustering on the same embeddings so different between t-SNE and PCA?\n",
    "\n",
    "1. **PCA visualization** shows the direction of maximum variance in the data. It tries to preserve the global structure, emphasizing the overall distribution of points. Large distances in the original space remain large in the PCA projection, and the relative positions of distant clusters are generally maintained.\n",
    "\n",
    "2. **t-SNE visualization** focuses on preserving neighborhood relationships. It excels at revealing local clusters and patterns, making it excellent for visualizing how data points group together. However, the sizes and distances between clusters may not reflect their true relationships in the original space!\n",
    "\n",
    "**Key Limitations to Be Aware Of**:\n",
    "\n",
    "- **PCA can miss non-linear structures**: Since PCA is a linear technique, it may fail to capture complex, non-linear relationships in your embeddings. Two classes that are separable by a non-linear boundary might appear mixed in PCA.\n",
    "\n",
    "- **t-SNE can distort global structure**: While excellent at showing clusters, t-SNE may not preserve the distances between them. Clusters that are far apart in the original space might appear close in t-SNE visualization, or vice versa.\n",
    "\n",
    "- **t-SNE is sensitive to hyperparameters**: The perplexity parameter in t-SNE (which influences the effective number of neighbors) can significantly affect the visualization. Different perplexity values can produce dramatically different visualizations.\n",
    "\n",
    "- **Random initialization in t-SNE**: Due to its non-deterministic nature, t-SNE can produce different visualizations in different runs unless you fix the random seed. In `visualize_embeddings_tsne` and `visualize_embeddings_pca` functions, we set the random seed to 42.\n",
    "\n",
    "**When Interpreting Model Embeddings**:\n",
    "\n",
    "When examining neural network embeddings, it's important to use both techniques:\n",
    "\n",
    "- Use **PCA** to understand the global structure and the principal directions of variation\n",
    "- Use **t-SNE** to identify clusters and local patterns\n",
    "- Don't draw strong conclusions from a single visualization technique\n",
    "- Remember that both techniques are simplifications of a complex, high-dimensional space\n",
    "\n",
    "By understanding the strengths and limitations of each visualization technique, you can better interpret what your neural network has actually learned from the data. A detailed overview about the limitation of each technique can be found [\"Understanding Dimensionality Reduction: PCA vs t-SNE vs UMAP vs FIt-SNE vs LargeVis vs Laplacian Eigenmaps\"](https://carnotresearch.medium.com/understanding-dimensionality-reduction-pca-vs-t-sne-vs-umap-vs-fit-sne-vs-largevis-vs-laplacian-13d0be9ef7f4) and [\"Why you should not rely on t-SNE, UMAP or TriMAP\"](https://towardsdatascience.com/why-you-should-not-rely-on-t-sne-umap-or-trimap-f8f5dc333e59/).\n",
    "\n",
    "In these articles there is also a discussion about other dimension reduction techniques like UMAP (Uniform Manifold Approximation and Projection), TriMAP (Large-scale Dimensionality Reduction Using Triplets), etc. that can be useful for embeddings visualization but won't be covered here.\n",
    "\n",
    "### 6.4 Using Embeddings in Practice\n",
    "\n",
    "Besides visualization, embeddings have several practical uses:\n",
    "\n",
    "1. **Transfer learning**: Use embeddings from pre-trained models as features for new tasks\n",
    "2. **Similarity search**: Find similar examples by comparing embedding distances\n",
    "3. **Anomaly detection**: Identify unusual inputs that don't cluster well with known classes\n",
    "4. **Interpretability**: Analyze which input features influence specific embedding dimensions\n",
    "\n",
    "### 6.5 Visualizing Patch and Position Embeddings for Vision Transformers\n",
    "\n",
    "We discuss the concepts of the patch and position embeddings for Vision Transformers in the corresponding notebook (section 04_1_ViT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c24a8fe018d342",
   "metadata": {},
   "source": [
    "## 7. One more thing: Finding the right model capacity\n",
    "\n",
    "Recent research has revealed that the relationship between model capacity and generalization is more complex than the classical U-shaped curve we see in overfitting. When models are scaled significantly beyond the traditional \"sweet spot\", we observe what's called the \"double descent\" phenomenon.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/double_descent.PNG\" width=\"1000\"/>\n",
    "    <p><i>Figure 5: Double descent risk curve showing improved performance when scaling beyond the interpolation threshold</i></p>\n",
    "</div>\n",
    "\n",
    "> What is Double Descent?\n",
    "\n",
    "The double descent phenomenon ([Belkin et al., 2019](https://www.pnas.org/doi/abs/10.1073/pnas.1903070116)) shows that:\n",
    "\n",
    "- Classical Regime: Initially follows the traditional U-shaped curve with a sweet spot balancing underfitting and overfitting.\n",
    "- Interpolation Threshold: Test error peaks when the model has just enough capacity to perfectly fit the training data.\n",
    "- Modern Interpolating Regime: As model capacity increases further, test error surprisingly decreases again, often below the error at the classical sweet spot.\n",
    "\n",
    "This explains why extremely large neural networks that perfectly fit training data can still generalize remarkably well in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
