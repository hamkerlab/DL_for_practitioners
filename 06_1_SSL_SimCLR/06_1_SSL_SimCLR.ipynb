{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b05023-7812-4e9d-abdb-bf2f955cc3bb",
   "metadata": {},
   "source": [
    "# Tutorial 6.1: Simple framework for Contrastive Learning of visual Representations (SimCLR)\n",
    "\n",
    "Author: [Ren√© Larisch](mailto:rene.larisch@informatik.tu-chemnitz.de)\n",
    "\n",
    "The **Sim**ple framework for **C**ontrastive **L**earning of visual **R**epresentations (SimCLR), a contrastive learning approach that does not require knowledge of labels, was published by [Chen et al. in 2020](https://arxiv.org/abs/2002.05709).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963968c-936b-4ec9-8bdc-beea29d84650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "\n",
    "from Utils.little_helpers import timer, set_seed, get_parameters\n",
    "set_seed(42)\n",
    "\n",
    "from Utils.optimizers import LARS\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94219b8-b628-4958-9fa5-27ba7db8c68c",
   "metadata": {},
   "source": [
    "\n",
    "## Short reminder of contrastive learning\n",
    "\n",
    "Before diving deeper into SimCLR, let's briefly review the main idea behind contrastive learning.\n",
    "\n",
    "Suppose we have multiple images of cats and birds and feed them into a deep neural network (as depicted in Fig. 1). Each input sample would lead to a representation within the network. If there is a (final) layer with only two neurons, we can display these different representations on a two-dimensional map.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/Contrastive_1.png\" width=\"450\"/>\n",
    "    <p><i>Figure 1: Different input samples fed into a deep neural network (DNN) create different latent representations within the network.</i></p>\n",
    "</div>\n",
    "\n",
    "The goal of contrastive learning is to have the representations of two input images showing a similar concept (such as two cats) be close to each other (having a minimum distance). Meanwhile, the representations of two input images showing different concepts (e.g., a bird and a cat) should be far from each other (i.e., have a large distance). Instead of distance, you can also think of the similarity between the representations.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/Contrastive.gif\" width=\"450\"/>\n",
    "    <p><i>Figure 2: Representations of the same class should become closer (blue arrows), representations of different classes should become more distant (red arrows).</i></p>\n",
    "</div>\n",
    "\n",
    "Most contrastive learning approaches use positive pairs (samples showing the same concept) and negative pairs (samples showing different concepts) to maximize similarity between positive pairs and minimize similarity between negative pairs.\n",
    "This allows the network to learn which input features are similar in positive pairs and which are different in negative pairs.\n",
    "\n",
    "Early approaches, like [Chopra et al. (2005)](https://doi.org/10.1109/CVPR.2005.202), created positive and negative pairs through pre-selection. However, SimCLR did not require this prior knowledge.\n",
    "\n",
    "## How SimCLR works\n",
    "\n",
    "### The overall idea\n",
    "Similar to previous approaches, SimCLR aims to maximize representational similarity between images of the same concept and minimize it between images of different concepts. Here, representational similarity refers to a representation within the neural network, also called a latent representation.\n",
    "\n",
    "The cosine similarity is used to measure the similarity between representations $z_i$ and $z_j$:\n",
    "\n",
    "$$\n",
    " \\cos(z_i,z_j) = \\frac{z_i^T z_j}{||z_i|| \\; ||z_j||}\n",
    "$$\n",
    "\n",
    "As a semi-supervised approach, SimCLR is designed primarily for pre-training on unlabeled data. These representations can then be used or fine-tuned for the final task, called the downstream task. To accomplish this, a deep neural network (e.g., a ResNet) is augmented with a representation layer and a nonlinear projection head (Fig. 3).\n",
    "Although only the network up to the representation layer (i.e. $h_i$ and $h_j$) is used for the downstream task, Chen et al. (2020) demonstrated that adding a nonlinear projection head and optimizing its output ($z_i$ and $z_j$) improves encoding in the representation layer.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/simclr-general-architecture.png\" width=\"750\"/>\n",
    "    <p><i>Figure 3: General structure of the SimCLR learning approach. A deep neural network, called the base encoder, is extended by a representation layer and a nonlinear projection head. Optimizing the similarity between the representations from the projection head also tunes the representations for the downstream tasks.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61c333-fc07-4ebc-a3d2-d6c966b46e18",
   "metadata": {},
   "source": [
    "### Projection Head\n",
    "\n",
    "Since the representation layer can easily be implemented with a standard linear layer, we will first implement the projection head. Although we could use the standard linear layer from PyTorch, we will define our own linear layer class that can perform batch normalization and determine whether the layer should have a bias value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28195f56-c099-4942-ae0c-83ccc9f6673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 use_bias = True,\n",
    "                 use_bn = False,\n",
    "                 **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, \n",
    "                                self.out_features, \n",
    "                                bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "             self.bn = nn.BatchNorm1d(self.out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6e814-35f9-4421-b1a4-692ad2eb05fd",
   "metadata": {},
   "source": [
    "Now, we can write the ProjectionHead class, which receives input features from the encoder model and outputs the activity of some neurons.\n",
    "The projection head can consist of one linear layer or multiple nonlinear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eff3cb-cfbd-43ff-8435-b2bdd5386ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 head_type = 'nonlinear',\n",
    "                 **kwargs):\n",
    "        super(ProjectionHead,self).__init__(**kwargs)\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features,self.out_features,False, True)\n",
    "        elif self.head_type == 'nonlinear':\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features,self.hidden_features,True, True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features,self.out_features,False,True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d293e4",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Now we can connect the ProjectionHead to the preceding encoder. Here in this case, the encoder can be a pre-trained ResNet50 from PyTorch or a self-defined network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e0ad1-d2a8-4dbb-933b-52c16d7cd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self,base_model, num_features=64):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_features = num_features\n",
    "\n",
    "        if isinstance(base_model, str):\n",
    "            #PRETRAINED MODEL\n",
    "            if base_model == 'resnet50':\n",
    "                self.pretrained = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            self.pretrained = self.base_model\n",
    "\n",
    "        if self.pretrained == None:\n",
    "            print('No valid pretrained model chosen')\n",
    "        \n",
    "\n",
    "        #self.pretrained[0] = nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "        ## if base_model == resnet50, switch the first convolutional and some later layers for fine tuning\n",
    "        if base_model == 'resnet50':\n",
    "            self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "            self.pretrained.maxpool = nn.Identity()\n",
    "            self.pretrained.fc = nn.Identity()\n",
    "            ## set the pretrained weights fix for the resnet model\n",
    "            for p in self.pretrained.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.latent_layer = LinearLayer(128,self.num_features, True, True)\n",
    "        self.projector = ProjectionHead(self.num_features, 2048, 128)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.pretrained(x)\n",
    "        #print(out.size())\n",
    "        latent = F.relu(self.latent_layer(out))\n",
    "        xp = self.projector(torch.squeeze(latent))\n",
    "        # we return the output from projection head and \n",
    "        # the latent layer as we need it later for the real task \n",
    "        return (xp,latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c97bd-348b-41f9-8784-4d5e1dc6a185",
   "metadata": {},
   "source": [
    "Let's define a small encoder network consisting of three convolutional layers and several fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f2380-a72a-46d9-95a6-1b61e1927391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## define the network structure with the layers\n",
    "        self.conv1 = nn.Conv2d(3,256,3) # in_channels, out_channels, kernel_size \n",
    "        self.pool  = nn.MaxPool2d(2,2) # kernel_size, stride\n",
    "        self.dropout = nn.Dropout(0.2) # dropout factor\n",
    "        self.conv2 = nn.Conv2d(256,128,3) # in_channels,out_channels, kernel_size\n",
    "        self.conv3 = nn.Conv2d(128,32,3) # in_channels,out_channels, kernel_size\n",
    "        self.fc1   = nn.Linear(32*14*14, 120) # in_channels, out_channels\n",
    "        self.fc2   = nn.Linear(120,1024)    \n",
    "        self.fc3   = nn.Linear(1024,128) # in_channels, out_channels\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## define the functionality of each layer/between the layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #print(x.size())\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204aa3b-0eac-4eab-ba1e-e9266f28d6ed",
   "metadata": {},
   "source": [
    "### Full Network\n",
    "\n",
    "Create the complete SimCLRModel, using our custom encoder network. You can also use a pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd8e74-a864-4fbc-b98b-86810ef2d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 128\n",
    "#model = SimCLRModel('resnet50', num_features).to(device)\n",
    "model = SimCLRModel(EncoderNet(), num_features).to(device)\n",
    "\n",
    "print('Trainable Parameters in CCT: %.3fM' % get_parameters(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2b69f",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "<table style>\n",
    "<td style=\"border: none!important; width:55%\">\n",
    "\n",
    "Since deep learning typically uses batch learning to update weights, SimCLR uses batches to identify positive and negative pairs.\n",
    "\n",
    "After creating two variants of each image in a batch using data augmentation, all variants are paired with each other.\n",
    "Variants of the same original image are considered positive pairs, and pairs of variants from different images are considered negative pairs.\n",
    "\n",
    "With $k$ samples per batch, there are $2k$ data points after augmentation. Each data point has one positive pair partner, and the remaining $2(k-1)$ augmented samples are considered negative samples (even if they accidentally depict an object from the same class or concept).\n",
    "\n",
    "The loss is calculated for each positive pair in the batch as follows:</p> <p></p>\n",
    "$$\n",
    " \\mathcal{l}(i,j) = -\\log{} \\frac{\\exp(sim(z_i,z_j)/\\tau )}{ \\sum^{2N}_{k=1} \\mathbb{1}_{[k \\neq i]}\\exp(sim(z_i,z_k)/\\tau)}\n",
    "$$\n",
    "with $sim(z_i,z_j) = \\cos(z_i,z_j)$ is the cosine similarity between the representations $z_i$ and $z_j$.\n",
    "\n",
    "The numerator depends only on the similarity of the single positive pair, while the denominator depends on the summed similarity of all negative pairs plus the positive one. The parameter $\\tau$ is a temperature parameter that controls the influence of the numerator and denominator.\n",
    "\n",
    "Thus, loss decreases as the similarity between positive pairs increases and increases as the similarity between negative pairs increases.\n",
    "\n",
    "Note: Switching the order in the positive pair, i.e. calculate the loss between $z_j$ and $z_i$, results in a different loss $\\mathcal{l}(j,i)$ as it is not symmetric.\n",
    "\n",
    "The loss for one image is the sum of the two sub-losses $\\mathcal{l}(i,j)$ and $\\mathcal{l}(j,i)$.\n",
    "\n",
    "At the end of an epoch, the total loss is calculated as follow:\n",
    "$$\n",
    "    \\mathcal{L} = \\frac{1}{2N} \\sum^{N}_{k=1}[\\mathcal{l}(2k-1,2k) + \\mathcal{l}(2k,2k-1)]\n",
    "$$   \n",
    "\n",
    "</td>\n",
    "<td style=\"border: none!important; width:45%\">\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/SimCLR_schema.gif\" width=\"400\"/>\n",
    "</div>   \n",
    "</td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b090e-6116-4ac0-a532-f6a6fddaae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        \n",
    "        sim_i_j = torch.diag(sim, diagonal=self.batch_size) #torch.diag(input = sim, diagonal = self.batch_size)\n",
    "        sim_j_i = torch.diag(input = sim, diagonal =-self.batch_size)\n",
    "        \n",
    "        # We have 2N samples\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c894c0a-4755-4496-9348-576e9a384ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZER\n",
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "#SCHEDULER OR LINEAR WARMUP\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
    "\n",
    "#SCHEDULER FOR COSINE DECAY\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)\n",
    "batch_size = 32\n",
    "#LOSS FUNCTION\n",
    "criterion = SimCLR_Loss(batch_size = batch_size, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc2158-c50a-42e5-aa2e-e95169e5b871",
   "metadata": {},
   "source": [
    "## AgeDB dataset\n",
    "The AgeDB, published by [Moschoglou et al.2017](10.1109/CVPRW.2017.250), contains 16,488 images of famous people at different ages.\n",
    "Each sample image contains three pieces of information: the person's name, gender, and age.\n",
    "\n",
    "### Prepare the dataset\n",
    "\n",
    "To run the following lines of code, make sure you have access to the dataset (e.g. download it beforehand from https://www.kaggle.com/datasets/nitingandhi/agedb-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6a809-8d35-4d00-8703-6858b092f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_file(path_data, split):\n",
    "    ## function to iterate through the path and create a annotation csv\n",
    "    list_data    = []\n",
    "    label_name   = []\n",
    "    label_age    = []\n",
    "    label_gender = []\n",
    "    \n",
    "    for idx,file in enumerate(os.listdir(path_data)):\n",
    "        _, name, age, gender = file.split('_')\n",
    "        gender = gender.split('.')[0]\n",
    "        \n",
    "        list_data.append((path_data+'/'+file, name, age, gender))\n",
    "\n",
    "    ## shuffle and then split the dataset into train, test, validation\n",
    "    n_samples = len(list_data)\n",
    "    idx = np.linspace(0,n_samples-1, n_samples, dtype='int32')\n",
    "    np.random.shuffle(idx)    \n",
    "    list_data   = np.asarray(list_data)[idx]\n",
    "\n",
    "    n_train= int(n_samples*split[0])\n",
    "    n_test = int(n_samples*split[1])\n",
    "\n",
    "    ##train data\n",
    "    train_data = list_data[:n_train]\n",
    "\n",
    "    ## test data\n",
    "    test_data  = list_data[n_train:n_train+n_test]\n",
    "    \n",
    "    ## create two csv-files for the annotations\n",
    "    csv_train = pd.DataFrame(train_data, columns = ['Images','Name','Age','Gender'])\n",
    "    csv_test = pd.DataFrame(test_data, columns = ['Images','Name','Age','Gender'])\n",
    "\n",
    "    ## save them for later \n",
    "    csv_train.to_csv('annot_train.csv', sep=',', index=False)\n",
    "    csv_test.to_csv('annot_test.csv', sep=',', index=False)\n",
    "\n",
    "    labels_name = csv_test['Name']\n",
    "    labels_age = csv_test['Age']\n",
    "    labels_gender = csv_test['Gender']\n",
    "\n",
    "    return(labels_name, labels_age, labels_gender)\n",
    "\n",
    "path_data = '../Dataset/AgeDB/'\n",
    "split = [0.8,0.2]\n",
    "\n",
    "labels_name, labels_age, labels_gender = make_annotation_file(path_data, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b40f7e-942c-4c56-98dc-4e365d109d1a",
   "metadata": {},
   "source": [
    "### Augmentation pipeline\n",
    "As shown in the original publication, the types and order of augmentations play a crucial role in the quality of learned representations.\n",
    "\n",
    "For this reason, we will stick to the original pipeline mentioned in [Chen et al. (2020)](https://arxiv.org/abs/2002.05709), though we should mention that other self-supervised approaches propose augmentation pipelines that may lead to better results.\n",
    "\n",
    "Please note that the dataset we create will return two augmented versions of each image in a batch, along with the corresponding age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f538fbd-1900-4029-9cf1-0eb52e6f662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.io import ImageReadMode\n",
    "\n",
    "class DataSetAugment(Dataset):\n",
    "    def __init__(self,phase, annotations_file, s = 0.5):\n",
    "        self.phase = phase\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.s = s\n",
    "        self.resize = v2.Resize((128,128))\n",
    "        self.transforms = transforms.Compose([v2.RandomResizedCrop(size=(128,128), scale=(0.75, 1.0), ratio=(0.9, 1.05)),\n",
    "                                            transforms.Compose([v2.RandomApply([\n",
    "                                                                v2.ColorJitter(0.8*self.s, \n",
    "                                                                                       0.8*self.s, \n",
    "                                                                                       0.8*self.s, \n",
    "                                                                                       0.2*self.s)], p = 0.3),\n",
    "                                                                  v2.RandomGrayscale(p=0.2)]),\n",
    "                                             v2.RandomHorizontalFlip(),\n",
    "                                             v2.RandomApply([v2.GaussianBlur((3,3), (1.0,2.0))],p=0.2),\n",
    "                                             v2.RandomResizedCrop(128,(0.08,1.0)),])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_labels.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join( self.img_labels.iloc[idx,0])\n",
    "        y =  self.img_labels.iloc[idx,3] # we select here the age\n",
    "        \n",
    "         # Some images are in RGB and some in gray-scale, so we set the ImageReadMode to convert all to RGB\n",
    "        x = read_image(img_path, mode = ImageReadMode.RGB)\n",
    "        x = x.numpy()\n",
    "        x = x.astype(np.float32)/255.0\n",
    "        \n",
    "        x1 = self.augment(torch.from_numpy(x))\n",
    "        x2 = self.augment(torch.from_numpy(x))\n",
    "\n",
    "        return x1, x2, y\n",
    "\n",
    "    def preprocess(self,frame):\n",
    "        MEAN = torch.tensor([[[0.485]], [[0.456]], [[0.406]]])\n",
    "        STD = torch.tensor([[[0.229]],[[0.224]],[[0.225]]])\n",
    "        frame = (frame-MEAN)/STD\n",
    "        return frame\n",
    "    \n",
    "    #applies randomly selected augmentations to each clip (same for each frame in the clip)\n",
    "    def augment(self, frame, transformations = None):\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            frame = self.transforms(frame)\n",
    "        else:\n",
    "            frame = self.resize(frame)\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a61de-5de9-4fd7-b8ec-e79721a06598",
   "metadata": {},
   "source": [
    "Now create train and test set.\n",
    "\n",
    "*Note: If you're working on a Windows system use 0 workers to avoid multiprocessing issues (Windows uses \"spawn\" for creating processes instead of \"fork\", which can cause issues with multiprocessing). Thus, `num_workers = 0`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caa90f-8e2a-48f5-b88e-b110a6885ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataSetAugment('train','annot_train.csv')\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = DataSetAugment('valid','annot_test.csv')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e488e0-a03a-46af-9c7b-3f91df3bf459",
   "metadata": {},
   "source": [
    "Let's plot some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25b7aa-98da-44ee-97ed-b90067169664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(imges):\n",
    "    plt.figure(figsize=(8,2))\n",
    "    for i in range(4):\n",
    "        img = imges[i]\n",
    "        #img = img / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "        plt.axis('off')\n",
    "    plt.show\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images_1,_, y= next(dataiter)\n",
    "imshow(images_1)\n",
    "print(y[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe119de-474a-4e51-8bd0-014e4078431a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7f211-f2ac-44ce-9f24-e9e0317e0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
    "    os.makedirs('./content/saved_models/', exist_ok=True) \n",
    "    out = os.path.join('./content/saved_models/',name.format(current_epoch))\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict()}, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285ec52-cb24-4d43-b877-efc691c9e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_epoch = 0\n",
    "epochs = 50#100\n",
    "tr_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "    stime = time.time()\n",
    "\n",
    "    model.train()\n",
    "    tr_loss_epoch = 0\n",
    "    pbar = tqdm(total = len(train_loader)-1 )\n",
    "    \n",
    "    for step, (x_i, x_j, _) in enumerate(train_loader):\n",
    "        if x_i.size()[0] < batch_size:\n",
    "            ##ignore the last batch, if it did not fit\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.squeeze().to(device).float()\n",
    "        x_j = x_j.squeeze().to(device).float()\n",
    "        \n",
    "        # positive pair, with encoding\n",
    "        z_i,_ = model(x_i)\n",
    "        z_j,_ = model(x_j)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        tr_loss_epoch += loss.item()\n",
    "\n",
    "        pbar.update(1)\n",
    "    if epoch < 10:\n",
    "        warmupscheduler.step()\n",
    "    if epoch >= 10:\n",
    "        mainscheduler.step()\n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        save_model(model, optimizer, mainscheduler, current_epoch,\"SimCLR_net_checkpoint.pt\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_epoch = 0\n",
    "        for step, (x_i, x_j, _) in enumerate(test_loader):\n",
    "          if x_i.size()[0] < batch_size:\n",
    "              ##ignore the last batch, if it did not fit \n",
    "              break\n",
    "          x_i = x_i.squeeze().to(device).float()\n",
    "          x_j = x_j.squeeze().to(device).float()\n",
    "\n",
    "          # positive pair, with encoding\n",
    "          z_i,_ = model(x_i)\n",
    "          z_j,_ = model(x_j)\n",
    "\n",
    "          loss = criterion(z_i, z_j)\n",
    "\n",
    "          if step % 50 == 0:\n",
    "              print(f\"Step [{step}/{len(test_loader)}]\\t Loss: {round(loss.item(),5)}\")\n",
    "\n",
    "          val_loss_epoch += loss.item()\n",
    "\n",
    "\n",
    "    tr_loss.append(tr_loss_epoch / len(train_loader))\n",
    "    val_loss.append(val_loss_epoch / len(test_loader))\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t Training Loss: {tr_loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\")\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t Validation Loss: {val_loss_epoch / len(test_loader)}\\t lr: {round(lr, 5)}\")\n",
    "    current_epoch += 1\n",
    "\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t Time Taken: {time_taken} minutes\")\n",
    "\n",
    "save_model(model, optimizer, mainscheduler, current_epoch, \"SimCLR_net_final.pt\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_loss, label='Trainings Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f54da-c4ab-4f19-8ce7-f399e0b4ef5d",
   "metadata": {},
   "source": [
    "## Fine tune\n",
    "We will now use the pre-trained network and fine-tune it to predict the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a21c9f-9faf-4086-887b-53b4947d77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = SimCLRModel(EncoderNet(), num_features).to(device)\n",
    "new_model.load_state_dict(torch.load('./content/saved_models/SimCLR_net_final.pt', weights_only=True)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e1022-1311-468a-9b15-b46c859858f0",
   "metadata": {},
   "source": [
    "First, we will create a new model that we want to fine-tune.\n",
    "Here, we try to keep the FineTuneModel class as simple as possible by building in only one if-statement to determine whether the weights from the contrastive-learning network should be frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b438ba-12a1-440e-a000-38766bd7db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self,base_model, num_classes, freeze_base = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        ## freeze the pre-trained model if necessary\n",
    "        if freeze_base:\n",
    "            self.base_model.requires_grad_(False)\n",
    "        else:\n",
    "            self.base_model.requires_grad_(True)\n",
    "\n",
    "        \n",
    "        in_features = self.base_model.latent_layer.out_features\n",
    "\n",
    "        ## new head for the classification\n",
    "        self.new_head = nn.Sequential(nn.Linear(in_features, 256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.3),\n",
    "                                      nn.Linear(256, num_classes))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## NOTE: we use here the output of the latent_layer!\n",
    "        _, out = self.base_model(x)\n",
    "        out = self.new_head(out)\n",
    "        return (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8f376-7c01-4528-8448-a99ca0dd2529",
   "metadata": {},
   "source": [
    "Init the model for predicting the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d1e03-d7b1-4235-b420-7db2db9f8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "num_classes = 2\n",
    "\n",
    "gender_model = FineTuneModel(new_model, num_classes, freeze_base=True)\n",
    "\n",
    "print('Trainable Parameters in network: %.3fM' % get_parameters(gender_model))\n",
    "\n",
    "num_epochs=5\n",
    "init_lr= 1e-4\n",
    "optimizer = optim.AdamW(gender_model.parameters(), lr=init_lr, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4ffc7-0ee6-48ec-a198-8a60e47d6f1f",
   "metadata": {},
   "source": [
    "We will build the labels inside the training loop as we get strings (\"m\" for male and \"f\" for female) from the train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1c000-cc0d-42da-b528-f91cfd07c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    gender_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]')\n",
    "\n",
    "    for x_i,_, labels in train_pbar:\n",
    "        x_i = x_i.to(device)\n",
    "        y = np.zeros(len(labels))\n",
    "        y[np.asarray(labels)=='m'] = 0\n",
    "        y[np.asarray(labels)=='f'] = 1\n",
    "        y = torch.tensor(y)\n",
    "        y = y.to(torch.int64)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = gender_model(x_i)\n",
    "        out = out.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * x_i.size(0)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_train_loss: float = running_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc: float = 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214be36b",
   "metadata": {},
   "source": [
    "Now we evaluate the performance of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356d82-8efb-43c5-bd62-77a6fc855834",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.eval()\n",
    "gender_model.to(device)\n",
    "\n",
    "predictions = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_pbar = tqdm(test_loader, desc='Evaluation')\n",
    "\n",
    "    for x_i,_, labels in eval_pbar:\n",
    "        if x_i.size()[0] < batch_size:\n",
    "            ##ignore the last batch, if it did not fit \n",
    "            break\n",
    "        x_i = x_i.to(device)\n",
    "        out = gender_model(x_i)\n",
    "        if isinstance(out, tuple):\n",
    "            out = out[0]\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        predictions.append(predicted.cpu())\n",
    "        label_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa143504-ee37-4bf4-bc40-7605300184c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.asarray(predictions).flatten()\n",
    "label_list = np.asarray(label_list).flatten()\n",
    "labels = np.zeros(len(label_list))\n",
    "labels[label_list=='f'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d48c8d-fcee-4887-bc0b-e56a14e54f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11abfdfe-0b00-475d-9efa-d7b2334a1c9d",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f889cef-2099-476d-b183-109d3cc3acc8",
   "metadata": {},
   "source": [
    "### 1. Evaluate parameters and network structure\n",
    "\n",
    "a) Play with the `batch_size`. What happens if you make it bigger?\n",
    "\n",
    "b) Change the encoder network to ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1a14a-a8be-497e-ace7-d2cf37ae1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a235d24-bc7e-4eb1-a8ba-5b75c5f6fb19",
   "metadata": {},
   "source": [
    "### 2. Change the code to predict the age as a regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802d29e-0c02-41ba-bb21-07ab87a45c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19e5bb-ddd5-475c-83a8-82c9881d4ce9",
   "metadata": {},
   "source": [
    "### 3. Use t-SNE to see how age is represented in the latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4768a-7156-4f17-86b5-bb4b31b75fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
