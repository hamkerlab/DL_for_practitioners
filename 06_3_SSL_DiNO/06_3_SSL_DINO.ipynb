{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b92aa3b0e31bcad",
   "metadata": {},
   "source": [
    "# Tutorial 6.3: Self-distillation with no labels (DINO)\n",
    "\n",
    "Author: [Erik Syniawa](mailto:erik.syniawa@informatik.tu-chemnitz.de)\n",
    "\n",
    "The self-supervised approach Self-**di**stillation with **no** labels (DINO) has been introduced by Caron et al., 2021 [[1](#caron2021self)].\n",
    "\n",
    "The Vision Transformer (ViT; [[3](#dosovit)]) demonstrated competitive performance with CNNs, but did not show dramatic benefits (see our notebook).\n",
    "A key factor of the success of Transformers in NLP comes from the use of self-supervised training. See:\n",
    "\n",
    "- [BERT (Devlin et al., 2019)](https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email): \n",
    "    - **Learning**: Learns by predicting masked words using context from both directions\n",
    " \n",
    "    - **Prediction**: Predicts missing words in text, enabling understanding of language relationships\n",
    "        \n",
    "- [GPT (Radford et al., 2019)](https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf):\n",
    "    - **Learning**: Learns by predicting the next word given all previous words in a sequence\n",
    "\n",
    "    - **Prediction**: Predicts and generates coherent text continuations by anticipating what comes next \n",
    "\n",
    "But our ViT is trained in a supervised manner with a lot of labeled data. So would self-supervised training help training ViTs?\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/example.gif\" width=\"700\"/>\n",
    "    <p><i>Figure 1: Self-attention extracted from DINO (animation from [2])</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3a6396ae4fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n",
    "\n",
    "# Add utils path\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "from Utils.dataloaders import prepare_UTKFace_age_task\n",
    "from Utils.little_helpers import timer, set_seed, get_parameters\n",
    "from Utils.functions import train_model, evaluate_model, test_model\n",
    "from Utils.optimizers import LARS\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ec0196cf3553a",
   "metadata": {},
   "source": [
    "## 1. Self-Distillation Architecture\n",
    "\n",
    "DINO employs a novel approach to self-supervised learning, building on the concept of knowledge distillation but without any labels. The key insight is to use a teacher-student architecture similar to [[4](#grill2020bootstrap)] where both networks share the same architecture but have different sets of parameters (see Tutorial 8.2 (BYOL)).\n",
    "\n",
    "### 1.1 Teacher-Student Framework\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/dino.gif\" width=\"700\"/>\n",
    "    <p><i>Figure 2: DINO main architecture (animation from [2])</i></p>\n",
    "</div>\n",
    "\n",
    "The DINO framework consists of:\n",
    "\n",
    "1. **Student Network**: Updated through direct gradient backpropagation\n",
    "2. **Teacher Network**: Updated through exponential moving average (EMA) of the student's weights\n",
    "3. **Different Views**: Multiple augmented views of the same image are created, with:\n",
    "   - Global views (covering most of the image)\n",
    "   - Local views (smaller crops of the image)\n",
    "\n",
    "The teacher-student framework in DINO addresses a fundamental challenge in self-supervised learning: **how to avoid collapse** (all images mapping to the same representation).\n",
    "The asymmetric architecture helps with that: Student sees all views (global + local), teacher only sees global views. That:\n",
    "\n",
    "- forces student to predict global context from local patches\n",
    "- creates a natural pretext task: \"local-to-global correspondence\"\n",
    "\n",
    "Updating the teacher network - the **Momentum Teacher** - where teacher parameters are EMA of student parameters. This leads to:\n",
    "\n",
    "- **Stability**: Teacher provides consistent, slowly-changing targets\n",
    "- **Quality**: Teacher represents a \"better\" version of the student (ensemble effect)\n",
    "- **Preventing Collapse**: Student can't immediately change teacher's outputs\n",
    "\n",
    "#### Mathematical Intuition Behind EMA Update\n",
    "\n",
    "The exponential moving average update mechanism forms the mathematical foundation of DINO's stability. The teacher parameters are updated according to the following equation:\n",
    "\n",
    "$$\\theta_t \\leftarrow \\lambda \\theta_t + (1 - \\lambda) \\theta_s$$\n",
    "\n",
    "where $\\theta_t$ represents the teacher parameters, $\\theta_s$ represents the student parameters, and $\\lambda$ is the momentum coefficient.\n",
    "\n",
    "**Mathematical Properties and Implications:** The momentum coefficient $\\lambda \\approx 0.996$ ensures that teacher parameters change slowly, providing stable targets for the student network. This high momentum value means that each update incorporates only 0.4% of the current student parameters, creating a smoothed version of the student's learning trajectory.\n",
    "\n",
    "**Ensemble Effect Through Temporal Averaging:** The teacher network represents accumulated knowledge over many training steps. Mathematically, the teacher parameters at step $t$ can be expressed as a weighted sum of all previous student parameters:\n",
    "\n",
    "$$\\theta_t^{teacher} = (1-\\lambda) \\sum_{i=0}^{t} \\lambda^i \\theta_{t-i}^{student}$$\n",
    "\n",
    "This formulation demonstrates that the teacher maintains a memory of the student's evolution, with exponentially decaying weights for older parameters.\n",
    "\n",
    "**Quality Improvement Mechanism:** In practice, the EMA teacher often outperforms the student throughout training due to the variance reduction effect of parameter averaging. The mathematical intuition follows from the bias-variance decomposition: while individual student parameters may have high variance due to stochastic gradient updates, the EMA teacher reduces this variance while maintaining low bias.\n",
    "\n",
    "The center vector follows a similar EMA update rule with momentum parameter $m$:\n",
    "\n",
    "$$c \\leftarrow m \\cdot c + (1-m) \\cdot \\frac{1}{B} \\sum_{i=1}^{B} g_{\\theta_t}(x_i)$$\n",
    "\n",
    "where $B$ represents the batch size. This centering operation ensures that the teacher outputs maintain zero mean across the feature dimensions, preventing dimensional collapse while preserving the relative relationships between features.\n",
    "\n",
    "\n",
    "```python\n",
    "# Pseudo-code for DINO training process\n",
    "# gs, gt: student and teacher networks\n",
    "# C: center (dimensionality K)\n",
    "# tps, tpt: student and teacher temperatures\n",
    "# l, m: network and center momentum rates\n",
    "\n",
    "gt.params = gs.params  # Initialize teacher with student weights\n",
    "\n",
    "for x in loader:  # Load a minibatch with n samples\n",
    "    x1, x2 = augment(x), augment(x)  # Create random views\n",
    "    local_views = [augment(x) for _ in range(local_crops_number)]\n",
    "    \n",
    "    s1, s2 = gs(x1), gs(x2)  # Student output for global views\n",
    "    s_local = [gs(v) for v in local_views]  # Student output for local views\n",
    "    \n",
    "    t1, t2 = gt(x1), gt(x2)  # Teacher output for global views (only)\n",
    "    \n",
    "    # Calculate loss: cross-entropy between student and teacher\n",
    "    loss = H(t1, s2)/2 + H(t2, s1)/2\n",
    "    for s_l in s_local:\n",
    "        loss += (H(t1, s_l) + H(t2, s_l)) / (2 * len(local_views))\n",
    "    \n",
    "    loss.backward()  # Back-propagate\n",
    "    \n",
    "    # Update student with SGD\n",
    "    update(gs)\n",
    "    \n",
    "    # Update teacher with momentum\n",
    "    gt.params = l * gt.params + (1-l) * gs.params\n",
    "    \n",
    "    # Update center with momentum\n",
    "    C = m * C + (1-m) * cat([t1, t2]).mean(dim=0)\n",
    "```\n",
    "\n",
    "\n",
    "### 1.2 Loss Function\n",
    "\n",
    "DINO uses a cross-entropy loss between the student and teacher outputs:\n",
    "\n",
    "```python\n",
    "def H(t, s):\n",
    "    t = t.detach()  # Stop gradient on teacher\n",
    "    s = softmax(s / tps, dim=1)  # Student output with temperature\n",
    "    t = softmax((t - C) / tpt, dim=1)  # Teacher output with centering and sharpening\n",
    "    return - (t * log(s)).sum(dim=1).mean()\n",
    "```\n",
    "\n",
    "The teacher output is processed with:\n",
    "1. **Centering**: Subtract a moving average of the teacher output\n",
    "2. **Sharpening**: Apply a lower temperature to make the distribution more peaked\n",
    "\n",
    "These two simple techniques effectively prevent the model from collapsing to trivial solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d01c09bf3a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_dim: int, \n",
    "                 ncrops: int, \n",
    "                 warmup_teacher_temp: float, \n",
    "                 teacher_temp: float,\n",
    "                 warmup_teacher_temp_epochs: float, \n",
    "                 nepochs: int, \n",
    "                 student_temp: float = 0.1,\n",
    "                 center_momentum: float = 0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.ncrops = ncrops\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "\n",
    "        # Teacher temperature schedule\n",
    "        self.teacher_temp_schedule = np.concatenate((\n",
    "            np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n",
    "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
    "        ))\n",
    "\n",
    "    def forward(self, student_output, teacher_output, epoch):\n",
    "        student_out = student_output / self.student_temp\n",
    "        student_out = student_out.chunk(self.ncrops)\n",
    "\n",
    "        # Teacher centering and sharpening\n",
    "        temp = self.teacher_temp_schedule[epoch]\n",
    "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
    "        teacher_out = teacher_out.detach().chunk(2)\n",
    "\n",
    "        total_loss = 0\n",
    "        n_loss_terms = 0\n",
    "        for iq, q in enumerate(teacher_out):\n",
    "            for v in range(len(student_out)):\n",
    "                if v == iq:\n",
    "                    continue\n",
    "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
    "                total_loss += loss.mean()\n",
    "                n_loss_terms += 1\n",
    "\n",
    "        total_loss /= n_loss_terms\n",
    "        self.update_center(teacher_output)\n",
    "        return total_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
    "        batch_center = batch_center / len(teacher_output)\n",
    "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6915ad011bcf829",
   "metadata": {},
   "source": [
    "\n",
    "The complete DINO loss function incorporates multiple views of the same image. Given a set of views $\\mathcal{V} = \\{x_1^g, x_2^g, x_1^l, ..., x_N^l\\}$ where $g$ denotes global views and $l$ denotes local views, the loss is computed as:\n",
    "\n",
    "$$\\mathcal{L}_{DINO} = \\sum_{x \\in \\{x_1^g, x_2^g\\}} \\sum_{\\substack{x' \\in \\mathcal{V} \\\\ x' \\neq x}} H(P_t(x), P_s(x'))$$ \n",
    "\n",
    "This asymmetric formulation ensures that:\n",
    "\n",
    "1. **Global-to-Global**: Student global views learn from teacher global views\n",
    "2. **Local-to-Global**: Student local views learn from teacher global views\n",
    "3. **Teacher Consistency**: Teacher only processes global views, providing stable targets\n",
    "\n",
    "The mathematical intuition is that local views must predict the global context, forcing the model to understand part-to-whole relationships without supervision.\n",
    "\n",
    "\n",
    "## 2. Image Augmentations\n",
    "\n",
    "### 2.1 Multi-Crop Strategy\n",
    "\n",
    "The multi-crop strategy creates two types of views:\n",
    "- **Global Views**: Higher resolution crops (224×224) covering >50% of the image\n",
    "- **Local Views**: Lower resolution crops (96×96) covering <50% of the image\n",
    "\n",
    "Let's examine key augmentations used in DINO (mostly they are the same as in BYOL [[4](#grill2020bootstrap)]):\n",
    "\n",
    "### 2.2 Specialized Augmentations\n",
    "\n",
    "Next to simple augmentations like crop, flip, or gray scaling, there are more complex augmentations like the following ones:\n",
    "\n",
    "#### 2.2.1 Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbe867b0789c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur:\n",
    "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.0):\n",
    "        self.prob = p\n",
    "        self.radius_min = radius_min\n",
    "        self.radius_max = radius_max\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = v2.functional.to_pil_image(img)\n",
    "\n",
    "        if torch.rand(1).item() <= self.prob:\n",
    "            radius = torch.empty(1).uniform_(self.radius_min, self.radius_max).item()\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4699d23801b1e",
   "metadata": {},
   "source": [
    "#### 2.2.2 Solarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5778d86630de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solarization:\n",
    "    def __init__(self, p=0.2):\n",
    "        self.prob = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = v2.functional.to_pil_image(img)\n",
    "\n",
    "        if torch.rand(1).item() <= self.prob:\n",
    "            img = ImageOps.solarize(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6821bce1adc4d1f",
   "metadata": {},
   "source": [
    "### 2.3 Data Augmentation Pipeline\n",
    "\n",
    "Now we can stich the different augmentations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3e99b242e5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataAugmentationDINO:\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number, input_size=128):\n",
    "        self.input_size = input_size\n",
    "        self.local_crops_number = local_crops_number\n",
    "\n",
    "        color_jitter = v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)\n",
    "\n",
    "        # Global crop 1 (always blur)\n",
    "        self.global_transform1 = v2.Compose([\n",
    "            v2.RandomResizedCrop(self.input_size, scale=global_crops_scale),\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomApply([color_jitter], p=0.8),\n",
    "            v2.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=1.0),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # Global crop 2 (sometimes blur + solarization)\n",
    "        self.global_transform2 = v2.Compose([\n",
    "            v2.RandomResizedCrop(self.input_size, scale=global_crops_scale),\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomApply([color_jitter], p=0.8),\n",
    "            v2.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.1),\n",
    "            Solarization(p=0.2),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # Local crops\n",
    "        self.local_transform = v2.Compose([\n",
    "            v2.RandomResizedCrop(self.input_size, scale=local_crops_scale),\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomApply([color_jitter], p=0.8),\n",
    "            v2.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.5),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        crops = []\n",
    "        crops.append(self.global_transform1(img))\n",
    "        crops.append(self.global_transform2(img))\n",
    "\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transform(img))\n",
    "\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d4aaa82ed0bd4",
   "metadata": {},
   "source": [
    "### 2.4 Applying the Augmentation Pipeline\n",
    "\n",
    "Let's have a look on the Augmentation Pipeline for an example image.\n",
    "\n",
    "First see, how the original image looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc355d2bbd7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "# Load a sample image \n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbc1a0",
   "metadata": {},
   "source": [
    "Now, have a look on the different augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First resize the image to maintain consistent size\n",
    "image_resized = v2.Resize(224)(image)\n",
    "\n",
    "# Create a figure with subplots to show all augmentations together\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(\"Individual DINO Augmentations\", fontsize=16)\n",
    "\n",
    "# Original Image\n",
    "axes[0, 0].imshow(image_resized)\n",
    "axes[0, 0].set_title(\"Original\")\n",
    "\n",
    "# Random Resized Crop (Global)\n",
    "crop_transform = v2.RandomResizedCrop(224, scale=(0.4, 1.0))\n",
    "crop_img = crop_transform(image_resized)\n",
    "axes[0, 1].imshow(crop_img)\n",
    "axes[0, 1].set_title(\"Random Resized Crop (Global)\")\n",
    "\n",
    "# Color Jitter\n",
    "color_jitter = v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)\n",
    "jitter_img = color_jitter(image_resized)\n",
    "axes[0, 2].imshow(jitter_img)\n",
    "axes[0, 2].set_title(\"Color Jitter\")\n",
    "\n",
    "# Grayscale\n",
    "grayscale = v2.RandomGrayscale(p=1.0)  # p=1.0 to force grayscale\n",
    "gray_img = grayscale(image_resized)\n",
    "axes[1, 0].imshow(gray_img)\n",
    "axes[1, 0].set_title(\"Grayscale\")\n",
    "\n",
    "# Gaussian Blur\n",
    "blur = GaussianBlur(p=1.0, radius_min=1.0, radius_max=1.0)  # p=1.0 to force blur\n",
    "blur_img = blur(image_resized)\n",
    "axes[1, 1].imshow(blur_img)\n",
    "axes[1, 1].set_title(\"Gaussian Blur\")\n",
    "\n",
    "# Solarization\n",
    "solarize = Solarization(p=1.0)  # p=1.0 to force solarization\n",
    "solar_img = solarize(image_resized)\n",
    "axes[1, 2].imshow(solar_img)\n",
    "axes[1, 2].set_title(\"Solarization\")\n",
    "\n",
    "# Remove axis ticks\n",
    "for ax in axes.flatten():\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Also demonstrate a local crop comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_resized)\n",
    "plt.title(\"Original (Resized)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "local_crop_transform = v2.RandomResizedCrop(96, scale=(0.05, 0.4))\n",
    "local_crop = local_crop_transform(image)\n",
    "plt.imshow(local_crop)\n",
    "plt.title(\"Random Resized Crop (Local View)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cf452",
   "metadata": {},
   "source": [
    "Finally, the full pipeline produces such images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff04c47763ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DINO augmentations with explicit parameters\n",
    "transform = DataAugmentationDINO(\n",
    "    global_crops_scale=(0.4, 1.0),\n",
    "    local_crops_scale=(0.05, 0.4),\n",
    "    local_crops_number=8,\n",
    "    input_size=224  # Set input_size to match TinyImageNet dimensions\n",
    ")\n",
    "\n",
    "# Generate different crops of the image\n",
    "crops = transform(image)\n",
    "\n",
    "# Prepare a resized version of the original for consistent display\n",
    "original_resized = v2.Resize(224)(image)\n",
    "original_tensor = v2.ToImage()(original_resized)\n",
    "original_tensor = v2.ToDtype(torch.float32, scale=True)(original_tensor)\n",
    "\n",
    "# Display the first few augmented views\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle(\"DINO Multi-Crop Augmentations\", fontsize=16)\n",
    "\n",
    "# First row: Original + 2 global views + 2 local views\n",
    "axes[0, 0].imshow(original_tensor.permute(1, 2, 0).numpy())\n",
    "axes[0, 0].set_title(\"Original (Resized)\")\n",
    "\n",
    "axes[0, 1].imshow(crops[0].permute(1, 2, 0).numpy())\n",
    "axes[0, 1].set_title(\"Global View 1\")\n",
    "\n",
    "axes[0, 2].imshow(crops[1].permute(1, 2, 0).numpy())\n",
    "axes[0, 2].set_title(\"Global View 2\")\n",
    "\n",
    "axes[0, 3].imshow(crops[2].permute(1, 2, 0).numpy())\n",
    "axes[0, 3].set_title(\"Local View 1\")\n",
    "\n",
    "axes[0, 4].imshow(crops[3].permute(1, 2, 0).numpy())\n",
    "axes[0, 4].set_title(\"Local View 2\")\n",
    "\n",
    "# Second row: 5 more local views\n",
    "for i in range(5):\n",
    "    if i+4 < len(crops):\n",
    "        axes[1, i].imshow(crops[i+4].permute(1, 2, 0).numpy())\n",
    "        axes[1, i].set_title(f\"Local View {i+3}\")\n",
    "\n",
    "# Remove axis ticks\n",
    "for ax in axes.flatten():\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e885aa823315a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Backbone & DINO Head Architecture\n",
    "\n",
    "DINO supports both traditional convolutional architectures (ResNet) and Vision Transformers (ViT). First we will implement a ViT via [`timm`](https://github.com/huggingface/pytorch-image-models) and in a later exercise you will implement a ResNet.\n",
    "\n",
    "### 3.1 Vision Transformer (ViT)\n",
    "\n",
    "The Vision Transformer divides an image into fixed-size patches, linearly embeds them, adds position embeddings, and processes the sequence with transformer blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d06aee9f0252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ViT from timm\n",
    "import timm\n",
    "def create_vit_model(model_name='vit_small_patch16_224', img_size=128, num_classes=0):\n",
    "    \"\"\"Create ViT model from timm\"\"\"\n",
    "    model = timm.create_model(\n",
    "        model_name, \n",
    "        pretrained=False,\n",
    "        img_size=img_size,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbc40fd0c57f48",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3.2 DINO Head\n",
    "\n",
    "The DINO Head is a projection head that maps the backbone features to the space where the self-distillation is performed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afacb55c31d5ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=False):\n",
    "        super().__init__()\n",
    "        hidden_dim = in_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(hidden_dim) if use_bn else nn.Identity(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(hidden_dim) if use_bn else nn.Identity(),\n",
    "        )\n",
    "\n",
    "        self.last_layer = nn.Linear(hidden_dim, out_dim, bias=False)\n",
    "        self.norm_last_layer = norm_last_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        if self.norm_last_layer:\n",
    "            w = F.normalize(self.last_layer.weight, dim=1, p=2)\n",
    "            x = F.linear(x, w)\n",
    "        else:\n",
    "            x = self.last_layer(x)\n",
    "            \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be6ce810a312e5",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 MultiCropWrapper\n",
    "\n",
    "A special wrapper class helps handling multiple crops of different resolutions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0973f5448e0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiCropWrapper(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        backbone.head = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "\n",
    "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
    "            torch.tensor([inp.shape[-1] for inp in x]),\n",
    "            return_counts=True,\n",
    "        )[1], 0)\n",
    "\n",
    "        start_idx, output = 0, torch.empty(0).to(x[0].device)\n",
    "        for end_idx in idx_crops:\n",
    "            _out = self.backbone(torch.cat(x[start_idx: end_idx]))\n",
    "            \n",
    "            if isinstance(_out, tuple):\n",
    "                _out = _out[0]\n",
    "            \n",
    "            output = torch.cat((output, _out))\n",
    "            start_idx = end_idx\n",
    "\n",
    "        return self.head(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed74d0d3b766d1",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Evaluation Methods\n",
    "\n",
    "DINO produces rich visual representations that can be evaluated in various ways:\n",
    "\n",
    "### 4.1 k-NN Classification\n",
    "\n",
    "One of the simplest evaluation methods is k-nearest neighbors classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df69ba6333a4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_knn(model, data_loader, k=20):\n",
    "    \"\"\"\n",
    "    Evaluate the model using k-NN classification.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    targets = []\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in data_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            \n",
    "            # Normalize features\n",
    "            feats = F.normalize(feats, dim=1, p=2)\n",
    "            \n",
    "            features.append(feats.cpu())\n",
    "            targets.append(labels)\n",
    "    \n",
    "    # Concatenate all features and targets\n",
    "    features = torch.cat(features, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarity = torch.mm(features, features.t())\n",
    "    \n",
    "    # Get top-k neighbors\n",
    "    _, indices = similarity.topk(k + 1, dim=1, largest=True)\n",
    "    indices = indices[:, 1:]  # Exclude self\n",
    "    \n",
    "    # Get labels of neighbors\n",
    "    neighbor_labels = torch.gather(targets.unsqueeze(1).expand(-1, k), 1, indices)\n",
    "    \n",
    "    # Predict by majority voting\n",
    "    predictions = torch.mode(neighbor_labels, dim=1)[0]\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = (predictions == targets).sum().item()\n",
    "    accuracy = 100 * correct / len(targets)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3bb0e2f5975d7",
   "metadata": {},
   "source": [
    "### 4.2 Linear Probing\n",
    "\n",
    "Another common evaluation method is training a linear classifier on top of frozen features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad26c08753fa393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_eval(model, train_loader, val_loader, epochs=100):\n",
    "    \"\"\"\n",
    "    Evaluate model by training a linear classifier on frozen features.\n",
    "    \"\"\"\n",
    "    # Freeze backbone\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    # Create linear classifier\n",
    "    classifier = nn.Linear(model.embed_dim, 200).to(device)  # 200 classes in TinyImageNet\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        classifier.train()\n",
    "        for images, targets in train_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = model(images)\n",
    "            \n",
    "            # Forward pass through classifier\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        classifier.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                \n",
    "                # Extract features\n",
    "                features = model(images)\n",
    "                \n",
    "                # Forward pass through classifier\n",
    "                outputs = classifier(features)\n",
    "                _, predicted = outputs.max(1)\n",
    "                \n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dafff6637471fc",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "1. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. *International Conference on Computer Vision (ICCV)*.  <a id=\"caron2021self\"></a>\n",
    "2. <https://github.com/facebookresearch/dino>   <a id=\"caron2021github\"></a>\n",
    "3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *International Conference on Learning Representations (ICLR)*. <a id=\"dosovit\"></a>\n",
    "4. Grill, J. B., Strub, F., Altché, F., Tallec, C., Richemond, P. H., Buchatskaya, E., ... & Valko, M. (2020). Bootstrap your own latent: A new approach to self-supervised learning. *Advances in Neural Information Processing Systems (NeurIPS)*. <a id=\"grill2020bootstrap\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b47121bbd72d3",
   "metadata": {},
   "source": [
    "## DINO in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d861c48e3aedc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'model_name': 'vit_small_patch16_224',\n",
    "    'img_size': 128,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'lr': 0.0005,\n",
    "    'weight_decay': 0.04,\n",
    "    'momentum_teacher': 0.996,\n",
    "    'out_dim': 2048,\n",
    "    'local_crops_number': 8,\n",
    "    'warmup_teacher_temp': 0.04,\n",
    "    'teacher_temp': 0.07,\n",
    "    'warmup_teacher_temp_epochs': 30,\n",
    "    'student_temp': 0.1,\n",
    "    'center_momentum': 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a65e97c5cb6ec2",
   "metadata": {},
   "source": [
    "\n",
    "### Training Process\n",
    "\n",
    "The training process follows these steps:\n",
    "\n",
    "1. Initialize student and teacher networks with the same architecture\n",
    "2. Setup loss function, optimizer, and learning rate scheduler\n",
    "3. For each epoch:\n",
    "   - Generate multiple views of each image\n",
    "   - Compute student and teacher outputs\n",
    "   - Calculate DINO loss\n",
    "   - Update student via backpropagation\n",
    "   - Update teacher with EMA\n",
    "   - Update center with EMA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47904c81",
   "metadata": {},
   "source": [
    "First, prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f090f523a76fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_file(path_data, split):\n",
    "    list_data = []\n",
    "    \n",
    "    for idx, file in enumerate(os.listdir(path_data)):\n",
    "        try:\n",
    "            _, name, age, gender = file.split('_')\n",
    "            gender = gender.split('.')[0]\n",
    "            list_data.append((path_data+'/'+file, name, age, gender))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    n_samples = len(list_data)\n",
    "    idx = np.linspace(0, n_samples-1, n_samples, dtype='int32')\n",
    "    np.random.shuffle(idx)    \n",
    "    list_data = np.asarray(list_data)[idx]\n",
    "\n",
    "    n_train = int(n_samples * split[0])\n",
    "    n_test = int(n_samples * split[1])\n",
    "\n",
    "    train_data = list_data[:n_train]\n",
    "    test_data = list_data[n_train:n_train+n_test]\n",
    "    \n",
    "    csv_train = pd.DataFrame(train_data, columns=['Images','Name','Age','Gender'])\n",
    "    csv_test = pd.DataFrame(test_data, columns=['Images','Name','Age','Gender'])\n",
    "\n",
    "    csv_train.to_csv('annot_train.csv', sep=',', index=False)\n",
    "    csv_test.to_csv('annot_test.csv', sep=',', index=False)\n",
    "\n",
    "    return csv_train, csv_test\n",
    "\n",
    "# Create dataset annotations\n",
    "path_data = '../Dataset/AgeDB/'\n",
    "split = [0.8, 0.2]\n",
    "\n",
    "labels_train, labels_test = make_annotation_file(path_data, split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e3889",
   "metadata": {},
   "source": [
    "*Remember: If you're working on a Windows system use 0 workers to avoid multiprocessing issues (Windows uses \"spawn\" for creating processes instead of \"fork\", which can cause issues with multiprocessing). Thus, `num_workers = 0`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef710cfed64a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetAugmentDINO(Dataset):\n",
    "    def __init__(self, phase, annotations_file, local_crops_number=8):\n",
    "        self.phase = phase\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.local_crops_number = local_crops_number\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.transform = DataAugmentationDINO(\n",
    "                global_crops_scale=(0.4, 1.0),\n",
    "                local_crops_scale=(0.05, 0.4),\n",
    "                local_crops_number=local_crops_number,\n",
    "                input_size=128\n",
    "            )\n",
    "        else:\n",
    "            self.transform = v2.Compose([\n",
    "                v2.Resize(128),\n",
    "                v2.CenterCrop(128),\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_labels.iloc[idx, 0]\n",
    "        label = self.img_labels.iloc[idx, 3]\n",
    "        \n",
    "        try:\n",
    "            # Load as PIL Image directly\n",
    "            from PIL import Image\n",
    "            x = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.phase == 'train':\n",
    "                crops = self.transform(x)\n",
    "                return crops, label\n",
    "            else:\n",
    "                x = self.transform(x)\n",
    "                return x, label\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            if self.phase == 'train':\n",
    "                dummy_crops = [torch.zeros(3, 128, 128) for _ in range(2 + self.local_crops_number)]\n",
    "                return dummy_crops, 'unknown'\n",
    "            else:\n",
    "                return torch.zeros(3, 128, 128), 'unknown'\n",
    "\n",
    "# Create datasets\n",
    "batch_size = 32\n",
    "local_crops_number = 8\n",
    "\n",
    "train_set = DataSetAugmentDINO('train', 'annot_train.csv', local_crops_number)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_set = DataSetAugmentDINO('valid', 'annot_test.csv', local_crops_number)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63b91c",
   "metadata": {},
   "source": [
    "Second, configure our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc597c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f\"Creating model: {config['model_name']}\")\n",
    "student = create_vit_model(config['model_name'], config['img_size'], num_classes=0)\n",
    "embed_dim = student.embed_dim\n",
    "\n",
    "# Create DINO head\n",
    "head = DINOHead(\n",
    "    in_dim=embed_dim,\n",
    "    out_dim=config['out_dim'],\n",
    "    use_bn=False,\n",
    "    norm_last_layer=True,\n",
    ")\n",
    "\n",
    "# Wrap with MultiCropWrapper\n",
    "student = MultiCropWrapper(student, head)\n",
    "student = student.to(device)\n",
    "\n",
    "# Create teacher (EMA copy)\n",
    "teacher = copy.deepcopy(student)\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f'Trainable Parameters: {get_parameters(student):.3f}M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e44ad4",
   "metadata": {},
   "source": [
    "Finally, define loss function, optimizer, and learning rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c13f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_loss = DINOLoss(\n",
    "    out_dim=config['out_dim'],\n",
    "    ncrops=2 + config['local_crops_number'],\n",
    "    warmup_teacher_temp=config['warmup_teacher_temp'],\n",
    "    teacher_temp=config['teacher_temp'],\n",
    "    warmup_teacher_temp_epochs=config['warmup_teacher_temp_epochs'],\n",
    "    nepochs=config['epochs'],\n",
    "    student_temp=config['student_temp'],\n",
    "    center_momentum=config['center_momentum'],\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    student.parameters(),\n",
    "    lr=config['lr'] * config['batch_size'] / 256,\n",
    "    weight_decay=config['weight_decay'],\n",
    ")\n",
    "# Learning rate scheduler\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0):\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    total_iters = epochs * niter_per_ep\n",
    "    \n",
    "    schedule = []\n",
    "    if warmup_iters > 0:\n",
    "        warmup_schedule = np.linspace(0, base_value, warmup_iters)\n",
    "        schedule.extend(warmup_schedule)\n",
    "    \n",
    "    iters = np.arange(total_iters - len(schedule))\n",
    "    cosine_schedule = final_value + 0.5 * (base_value - final_value) * \\\n",
    "                      (1 + np.cos(np.pi * iters / len(iters)))\n",
    "    \n",
    "    schedule.extend(cosine_schedule)\n",
    "    return schedule\n",
    "\n",
    "niter_per_ep = len(train_loader)\n",
    "lr_schedule = cosine_scheduler(\n",
    "    config['lr'] * config['batch_size'] / 256,\n",
    "    0,\n",
    "    config['epochs'], \n",
    "    niter_per_ep,\n",
    "    warmup_epochs=10\n",
    ")\n",
    "\n",
    "momentum_schedule = cosine_scheduler(config['momentum_teacher'], 1, config['epochs'], niter_per_ep)\n",
    "\n",
    "wd_schedule = cosine_scheduler(\n",
    "    config['weight_decay'],\n",
    "    0.4,  # weight_decay_end\n",
    "    config['epochs'], \n",
    "    len(train_loader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380bcc02450fadd",
   "metadata": {},
   "source": [
    "Putting everything together and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949f50e371a8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting DINO training for {config['epochs']} epochs...\")\n",
    "\n",
    "train_losses = []\n",
    "lr_values = []\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    student.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]}')\n",
    "    \n",
    "    for i, (images, _) in enumerate(train_pbar):\n",
    "        # Update learning rate\n",
    "        it = len(train_loader) * epoch + i\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr_schedule[it]\n",
    "            if i == 0:\n",
    "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
    "        \n",
    "        # Move images to device\n",
    "        images = [im.to(device, non_blocking=True) for im in images]\n",
    "\n",
    "        # Forward passes\n",
    "        teacher_output = teacher(images[:2])  # Only global views\n",
    "        student_output = student(images)     # All views\n",
    "        \n",
    "        loss = dino_loss(student_output, teacher_output, epoch)\n",
    "\n",
    "        if not math.isfinite(loss.item()):\n",
    "            print(f\"Loss is {loss.item()}, stopping training\")\n",
    "            break\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(student.parameters(), 3.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA update for teacher\n",
    "        with torch.no_grad():\n",
    "            m = momentum_schedule[it]\n",
    "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
    "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        train_pbar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'lr': optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    lr_values.append(optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']}: Loss = {avg_loss:.4f}, LR = {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save checkpoint every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0 or epoch == config['epochs'] - 1:\n",
    "        torch.save({\n",
    "            'student': student.state_dict(),\n",
    "            'teacher': teacher.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'config': config,\n",
    "            'loss': avg_loss\n",
    "        }, f'dino_checkpoint_{epoch+1:04d}.pth')\n",
    "\n",
    "print(\"DINO pretraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1087b4",
   "metadata": {},
   "source": [
    "Plot some evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f07be901ba1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lr_values)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeebd9d40adac41",
   "metadata": {},
   "source": [
    "### Fine-Tuning to predict gender\n",
    "\n",
    "As in the previous notebooks, we want to fine-tune the model to predict the two gender classes.\n",
    "\n",
    "First, create a class for the fine-tuning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc03af44b11659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes, freeze_base=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        if freeze_base:\n",
    "            self.base_model.requires_grad_(False)\n",
    "        else:\n",
    "            self.base_model.requires_grad_(True)\n",
    "\n",
    "        embed_dim = self.base_model.backbone.embed_dim\n",
    "        \n",
    "        self.new_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        features = self.base_model.backbone(x)\n",
    "        if isinstance(features, tuple):\n",
    "            features = features[0]\n",
    "        \n",
    "        out = self.new_head(features)\n",
    "        return out\n",
    "\n",
    "# Load pretrained DINO model\n",
    "checkpoint_path = 'dino_checkpoint_0100.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading pretrained DINO model...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Recreate model with same config\n",
    "    config_loaded = checkpoint['config']\n",
    "    student_loaded = create_vit_model(config_loaded['model_name'], config_loaded['img_size'])\n",
    "    embed_dim = student_loaded.embed_dim\n",
    "    \n",
    "    head_loaded = DINOHead(embed_dim, config_loaded['out_dim'], use_bn=False, norm_last_layer=True)\n",
    "    dino_model = MultiCropWrapper(student_loaded, head_loaded)\n",
    "    dino_model.load_state_dict(checkpoint['student'])\n",
    "    dino_model = dino_model.to(device)\n",
    "    \n",
    "    # Create fine-tuning model\n",
    "    gender_model = FineTuneModel(dino_model, num_classes=2, freeze_base=False)\n",
    "    gender_model = gender_model.to(device)\n",
    "    \n",
    "    print(f'Fine-tuning model parameters: {get_parameters(gender_model):.3f}M')\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found. Please complete DINO training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a6faf",
   "metadata": {},
   "source": [
    "Define Optimizer and Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e873f9ca9af34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs=5\n",
    "init_lr= 1e-4\n",
    "optimizer = optim.AdamW(gender_model.parameters(), lr=init_lr, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be7829-9da4-4751-93fc-01717c1036a0",
   "metadata": {},
   "source": [
    "Start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3747eb-8fb9-46e9-b3f5-9223d18e51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    gender_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]')\n",
    "\n",
    "    for x_i, labels in train_pbar:\n",
    "        images = x_i[0]\n",
    "        if images.size()[0] < batch_size:\n",
    "            ##ignore the last batch, if it did not fit\n",
    "            break\n",
    "        # Move images to device\n",
    "        images = images.to(device).float()\n",
    "\n",
    "        y = np.zeros(len(labels))\n",
    "        y[np.asarray(labels)=='m'] = 0\n",
    "        y[np.asarray(labels)=='f'] = 1\n",
    "        y = torch.tensor(y)\n",
    "        y = y.to(torch.int64)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = gender_model(images)\n",
    "        out = out.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_train_loss: float = running_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc: float = 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261f63b-6e2f-48a1-9b2e-c68f1d7398e2",
   "metadata": {},
   "source": [
    "Evaluate the gender prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cdc8c-785a-4198-8b0c-e261fe21d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gender_model.eval()\n",
    "gender_model.to(device)\n",
    "\n",
    "predictions = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_pbar = tqdm(valid_loader, desc='Evaluation')\n",
    "\n",
    "    for x_i,labels in eval_pbar:\n",
    "        if x_i.size()[0] < batch_size:\n",
    "            ##ignore the last batch, if it did not fit \n",
    "            break\n",
    "        x_i = x_i.to(device)\n",
    "        out = gender_model(x_i)\n",
    "        if isinstance(out, tuple):\n",
    "            out = out[0]\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        predictions.append(predicted.cpu())\n",
    "        label_list.append(labels)\n",
    "\n",
    "predictions = np.asarray(predictions).flatten()\n",
    "label_list = np.asarray(label_list).flatten()\n",
    "labels = np.zeros(len(label_list))\n",
    "labels[label_list=='f'] = 1\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fb76537ec5bf5",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 1. Plot the attention maps on a random AgeDB image (see Tutorial 4.1 (ViT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cefc718d714a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d286cf579c7c1b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Change the encoder network to ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad55b28bca259ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92626edfc00ef44",
   "metadata": {},
   "source": [
    "### 3. Use t-SNE to see how gender is represented in the latent representation from the online network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41e7e2b3e52bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5f539bebef92e",
   "metadata": {},
   "source": [
    "### 4. Change the augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70244aa2bdf994aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
