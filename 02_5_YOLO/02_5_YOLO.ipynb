{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f9cd62-39a4-4903-8975-f05f2ce89d0f",
   "metadata": {},
   "source": [
    "# Tutorial 2.5: **Y**ou **O**nly **L**ook **O**nce (YOLO)\n",
    "\n",
    "Author: [Ren√© Larisch](mailto:rene.larisch@informatik.tu-chemnitz.de). This implementation based on the implementation of [Jeffrey Tan](https://github.com/tanjeffreyz/yolo-v1) with modifications.\n",
    "\n",
    "\n",
    "Introduced by [Redmon et al. (2015)](https://arxiv.org/pdf/1506.02640), YOLO (**Y**ou **O**nly **L**ook **O**nce) understands object detection as a regression problem rather than a classification problem.\n",
    "\n",
    "Previous object detection methods, like region-based convolution neural networks (R-CNN), used multiple region proposals, feed them into a CNN to extract features and uses a support vector machine (SVM) to identify if there is an object in the region or not. Regions are optimized during training to fit the ground truth bounding boxes. This is called the [selective search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf) algorithm. \n",
    "The drawback of this approach is, that each region proposal has to be processed by the CNN separately, slowing down the detection.\n",
    "\n",
    "YOLO solves that problem by predicting multiple bounding boxes at once and assigning each bounding box to a small part of the input image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b2c77",
   "metadata": {},
   "source": [
    "Let's start with the usual initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53ae5c-7a5d-4a9d-87bb-bb5b777f339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "\n",
    "from Utils.dataloaders import PascalVocDataset\n",
    "from Utils.plotting import plot_boxes\n",
    "from Utils.little_helpers import set_seed, get_parameters, get_overlap, get_bboxes\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834f015-3124-478f-93a9-d671e57a7156",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "YOLO follows the following algorithm:\n",
    "1. The input image is divided into an $S \\times S$ grid. If an object is in the center of the grid, the grid should detect the object. At the end, each grid cell should only predict one class. \n",
    "2. Each grid predicts $B$ many bounding boxes. A bounding box is defined by four parameters: \n",
    "$(x,y)$ defines the center of the bounding box, relative to the corresponding grid cell. \n",
    "The parameters $(h,w)$ define the height and width of the box and are relative to the size of the image. Thus, a bounding box can span over multiple grid cells.\n",
    "3. Each bounding box predicts also a **confidence score**, which is formally defined as $P(Object) * IoU^{truth}_{pred}$, with $P(Object)$ the probability that there is an object in the grid cell and $IoU^{truth}_{pred}$ the **Intersection over the Union** between the predicted bounding box and the ground truth. Or to say it differently: If there is no object in the grid cell, the **confidence score** is zero, otherwise, it is equal to the $IoU^{truth}_{pred}$.\n",
    "4. Each grid cell predicts conditional class probabilities ($P(Class_i|Object)$) for each of the $C$ many classes. Despite multiple bounding boxes, only one conditional probability per class is calculated for each grid cell.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/Yolo.gif\"/>\n",
    "    <p><i>Figure 1: Schematic view of the functionality of YOLO . From Redmon et al. (2015)</i></p>\n",
    "</div>\n",
    "\n",
    "In summary, the number of parameters to be predicted depends on the numbers of grid cells ($S \\times S$), the number of classes ($C$), and the five parameters (4 coordinates and 1 confidence score) for each of the bounding boxes ($B * 5$).\n",
    "Or in total:\n",
    "\n",
    "$$\n",
    "S \\times S \\times (B *5 + C)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f080d-52cf-46be-9dc8-8b65cef06698",
   "metadata": {},
   "source": [
    "## Intersection over Union\n",
    "\n",
    "The Intersection over Union (IoU) (sometimes called the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)) is calculated as the proportion between the overlapping area (or intersection) between two bounding boxes and the area of the union between the boxes. It calculates how well two bounding boxes match each other.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/IOU.png\" width=\"350\"/>\n",
    "    <p><i>Figure 2: Visualization of Intersection of Union. From https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</i></p>\n",
    "</div>\n",
    "\n",
    "If the area of overlap is small, the area of union will be bigger than the area of overlap, leading to a higher denominator and an IoU value closer to zero. In contrast to that, when the area of overlap increases, the size of both areas approaches each other so that the IoU gets closer to one. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/boxes.gif\" width=\"450\"/>\n",
    "    <p><i>Figure 3: Calculating the intersection of two boxes.</i></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "To calculate the overlapping area between two rectangular boxes $A$ and $B$, we need $\\left(x_{1}^A, y_{1}^A\\right)$ as the lower corner positions of box A and $\\left(x_{2}^A, y_{2}^A\\right)$ as the upper corner positions of box A, and the corresponding parameters for box B ($\\left(x_{1}^B, y_{1}^B\\right), \\left(x_{2}^B, y_{2}^B\\right)$).\n",
    "Next, we define the highest lower left point ($tl$) from the two boxes, which is computed by $x_{tl} = \\max(x_{1}^A, x_{1}^B)$ and $y_{tl} = \\max(y_{1}^A, y_{1}^B)$. Additionally, we compute the lowest high right point ($br$) with $x_{br} = \\min\\left(x_{2}^A, x_{2}^B\\right)$ and $y_{br} = \\min\\left(y_{2}^A, y_{2}^B\\right)$. With the points $\\left(x_{tl}, y_{tl}\\right)$ and $\\left(x_{br}, y_{br}\\right)$ we can define the intersection and calculate its area.\n",
    "\n",
    "To calculate the union, we simply add the areas of the two boxes and subtract the area of the intersection.\n",
    "\n",
    "We implement an extra function to calculate the $IoU$ between the predicted bounding box (_p_) and the ground truth (_a_).\n",
    "The function _get_iou_ gets also the number of boxes (_B_) and the number of classes (_C_) as parameters together with a small value that we use instead of zeros, in the case that there is no object in the box (_epsilon_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f6b1e-fada-49b8-9857-1ad70bb8146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_attr(data, i, C):\n",
    "    \"\"\"Returns the Ith attribute of each bounding box in data.\"\"\"\n",
    "\n",
    "    attr_start = C + i\n",
    "    return data[..., attr_start::5]\n",
    "    \n",
    "def bbox_to_coords(t, C):\n",
    "    \"\"\"Changes format of bounding boxes from [x, y, width, height] to ([x1, y1], [x2, y2]).\"\"\"\n",
    "    \n",
    "    width = bbox_attr(t, 2, C)\n",
    "    x = bbox_attr(t, 0, C)\n",
    "    x1 = x - width / 2.0\n",
    "    x2 = x + width / 2.0\n",
    "\n",
    "    height = bbox_attr(t, 3, C)\n",
    "    y = bbox_attr(t, 1, C)\n",
    "    y1 = y - height / 2.0\n",
    "    y2 = y + height / 2.0\n",
    "\n",
    "    return torch.stack((x1, y1), dim=4), torch.stack((x2, y2), dim=4)\n",
    "\n",
    "def get_iou(p, a, B, C, epsilon):\n",
    "\n",
    "    p_tl, p_br = bbox_to_coords(p, C)          # (batch, S, S, B, 2)\n",
    "    a_tl, a_br = bbox_to_coords(a, C)\n",
    "\n",
    "    # Largest top-left corner and smallest bottom-right corner give the intersection\n",
    "    coords_join_size = (-1, -1, -1, B, B, 2)\n",
    "    tl = torch.max(\n",
    "        p_tl.unsqueeze(4).expand(coords_join_size),         # (batch, S, S, B, 1, 2) -> (batch, S, S, B, B, 2)\n",
    "        a_tl.unsqueeze(3).expand(coords_join_size)          # (batch, S, S, 1, B, 2) -> (batch, S, S, B, B, 2)\n",
    "    )\n",
    "    br = torch.min(\n",
    "        p_br.unsqueeze(4).expand(coords_join_size),\n",
    "        a_br.unsqueeze(3).expand(coords_join_size)\n",
    "    )\n",
    "\n",
    "    intersection_sides = torch.clamp(br - tl, min=0.0)\n",
    "    intersection = intersection_sides[..., 0] \\\n",
    "                   * intersection_sides[..., 1]       # (batch, S, S, B, B)\n",
    "\n",
    "    p_area = bbox_attr(p, 2, C) * bbox_attr(p, 3, C)                  # (batch, S, S, B)\n",
    "    p_area = p_area.unsqueeze(4).expand_as(intersection)        # (batch, S, S, B, 1) -> (batch, S, S, B, B)\n",
    "\n",
    "    a_area = bbox_attr(a, 2, C) * bbox_attr(a, 3, C)                  # (batch, S, S, B)\n",
    "    a_area = a_area.unsqueeze(3).expand_as(intersection)        # (batch, S, S, 1, B) -> (batch, S, S, B, B)\n",
    "\n",
    "    union = p_area + a_area - intersection\n",
    "\n",
    "    # Catch division-by-zero\n",
    "    zero_unions = (union == 0.0)\n",
    "    union[zero_unions] = epsilon\n",
    "    intersection[zero_unions] = 0.0\n",
    "\n",
    "    return intersection / union\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47f8a6-0813-4b94-a08d-a5b0d9ee250d",
   "metadata": {},
   "source": [
    "## YOLO - Loss function\n",
    "\n",
    "In order to optimize our network to perform the prediction, the loss function consists of three terms.\n",
    "\n",
    "### 1. Localization loss\n",
    "\n",
    "The first loss we need is the localization loss, which is calculated as the MSE between the coordinates of the ground truth box $\\left(\\hat{x_i}, \\hat{y_i}, \\hat{w_i}, \\hat{h_i}\\right)$ and the coordinates of the predicted box $\\left(x_i, y_i, w_i, h_i\\right)$. It ensures that the spatial parameters of the bounding box are trained:\n",
    "\n",
    "$$\n",
    " \\mathcal{L}_{loc}(\\theta) = \\lambda_{coord}\\;\\left(\\; \\sum^{S^2}_{i=0} \\sum^B_{j=0}1^{obj}_{i,j} \\left[(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2\\right] \\; + \\;\n",
    "  \\sum^{S^2}_{i=0} \\sum^B_{j=0}1^{obj}_{i,j} \\left[(\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h} - \\sqrt{\\hat{h}_i})^2\\right] \\; \\right)\n",
    "$$\n",
    "\n",
    "The term $1^{obj}_{i,j}$ is one if there is an object in the bounding box. Therefore, $\\mathcal{L}_{loc}$ is zero if there is no object in the bounding box. For images with a lot of background information, this can lead to a lower average loss and a network that does not predict the bounding boxes correctly. To avoid this, we use the $\\lambda_{coord} = 5$ parameter to increase the loss if there is an object in the box, forcing the network to predict the bounding boxes correctly.\n",
    "\n",
    "### 2. Confidence loss\n",
    "\n",
    "The second loss is to learn the confidence score of each bounding box $j$, by minimizing the MSE between the predicted confidence score ($C_i$) of a grid cell $i$, and the IoU between the ground truth bounding box and the predicted bounding box ($\\hat{C}_i$).\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_{conf}(\\theta) = \\sum^{S^2}_{i=0} \\sum^B_{j=0}1^{obj}_{i,j}(C_{i,j} - \\hat{C}_{i,j})^2 + \\lambda_{noobj} \\sum^{S^2}_{i=0} \\sum^B_{j=0}1^{noobj}_{i,j}(C_{i,j} - \\hat{C}_{i,j})^2 \n",
    "$$\n",
    "\n",
    "To distinguish between the object and the background, the confidence should be fully updated if there is an actual object in the grid cell, i.e. $1^{obj}_{i,j} = 1$. If there is no object in the grid cell (i.e. $1^{noobj}_{i,j}$ = 1), the confidence should be updated only moderately (thus, $\\lambda_{noobj} = 0.5$). Since in many images the number of grid cells without an object is higher than the number of cells with an object, the value for $\\lambda_{noobj}$ should be low to avoid instability during training.\n",
    " \n",
    "### 3. Classification loss\n",
    "\n",
    "The last loss function is the classification loss. \n",
    "It is calculated as the mean square error (MSE) between the one-hot encoded class labels ($\\hat{p}_i(c)$) and the predicted class probabilities ($p_i(c)$), related to a grid cell $i$:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_{class}(\\theta) = \\sum^{S^2}_{i=0}1^{obj}_{i} \\sum_{c \\in classes}(p_i(c) - \\hat{p}_i(c))^2 \n",
    "$$\n",
    "\n",
    "where $1^{obj}_{i}$ is 1 when there actually is an object in grid cell $i$, 0 otherwise. \n",
    "\n",
    "\n",
    "### The final loss function\n",
    "\n",
    "The complete loss function can be written as:\n",
    "\n",
    "$$\n",
    " \\mathcal{L}(\\theta) = \\mathcal{L}_{loc}(\\theta) + \\mathcal{L}_{conf}(\\theta) + \\mathcal{L}_{class}(\\theta) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ba858-d608-47ca-8124-b32acb4a3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(a, b):\n",
    "    flattened_a = torch.flatten(a, end_dim=-2)\n",
    "    flattened_b = torch.flatten(b, end_dim=-2).expand_as(flattened_a)\n",
    "    return nn.functional.mse_loss(\n",
    "        flattened_a,\n",
    "        flattened_b,\n",
    "        reduction='sum'\n",
    "    )\n",
    "\n",
    "class SumSquaredErrorLoss(nn.Module):\n",
    "    def __init__(self, epsilon = 1e-6, B=3, C = 20, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.l_coord = 5\n",
    "        self.l_noobj = 0.5\n",
    "        self.epsilon = epsilon\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def forward(self, p, a):\n",
    "        # Calculate IoU of each predicted bbox against the ground truth bbox\n",
    "        iou = get_iou(p, a, self.B, self.C, self.epsilon)                     # (batch, S, S, B, B)\n",
    "        max_iou = torch.max(iou, dim=-1)[0]     # (batch, S, S, B)\n",
    "\n",
    "        # Get masks\n",
    "        bbox_mask = bbox_attr(a, 4, self.C) > 0.0\n",
    "        p_template = bbox_attr(p, 4, self.C) > 0.0\n",
    "        \n",
    "        # Use masks to see if there is an object in the grid or not\n",
    "        obj_i = bbox_mask[..., 0:1]         # 1 if grid I has any object at all\n",
    "        responsible = torch.zeros_like(p_template).scatter_(       # (batch, S, S, B)\n",
    "            -1,\n",
    "            torch.argmax(max_iou, dim=-1, keepdim=True),                # (batch, S, S, B)\n",
    "            value=1                         # 1 if bounding box is \"responsible\" for predicting the object\n",
    "        )\n",
    "        obj_ij = obj_i * responsible        # 1 if object exists AND bbox is responsible\n",
    "        noobj_ij = ~obj_ij                  # Otherwise, confidence should be 0\n",
    "\n",
    "        \n",
    "        ####\n",
    "        # Localization losses\n",
    "        ####\n",
    "        # XY position losses\n",
    "        x_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 0, self.C),\n",
    "            obj_ij * bbox_attr(a, 0, self.C)\n",
    "        )\n",
    "        y_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 1, self.C),\n",
    "            obj_ij * bbox_attr(a, 1, self.C)\n",
    "        )\n",
    "        pos_losses = x_losses + y_losses\n",
    "        \n",
    "\n",
    "        # Bbox dimension losses\n",
    "        p_width = bbox_attr(p, 2, self.C)\n",
    "        a_width = bbox_attr(a, 2, self.C)\n",
    "        width_losses = mse_loss(\n",
    "            obj_ij * torch.sign(p_width) * torch.sqrt(torch.abs(p_width) + self.epsilon),\n",
    "            obj_ij * torch.sqrt(a_width)\n",
    "        )\n",
    "        p_height = bbox_attr(p, 3, self.C)\n",
    "        a_height = bbox_attr(a, 3, self.C)\n",
    "        height_losses = mse_loss(\n",
    "            obj_ij * torch.sign(p_height) * torch.sqrt(torch.abs(p_height) + self.epsilon),\n",
    "            obj_ij * torch.sqrt(a_height)\n",
    "        )\n",
    "        dim_losses = width_losses + height_losses\n",
    "        # print('dim_losses', dim_losses.item())\n",
    "\n",
    "        ####\n",
    "        # Confidence losses (target confidence is IoU)\n",
    "        ####\n",
    "        # there is an object in the cell\n",
    "        obj_confidence_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 4, self.C),\n",
    "            obj_ij * torch.ones_like(max_iou)\n",
    "        )\n",
    "        \n",
    "        # there is no object in the cell\n",
    "        noobj_confidence_losses = mse_loss(\n",
    "            noobj_ij * bbox_attr(p, 4, self.C),\n",
    "            torch.zeros_like(max_iou)\n",
    "        )\n",
    "\n",
    "        ####\n",
    "        # Classification losses\n",
    "        ####\n",
    "        class_losses = mse_loss(\n",
    "            obj_i * p[..., :self.C],\n",
    "            obj_i * a[..., :self.C]\n",
    "        )\n",
    "        # print('class_losses', class_losses.item())\n",
    "\n",
    "        total = self.l_coord * (pos_losses + dim_losses) \\\n",
    "                + obj_confidence_losses \\\n",
    "                + self.l_noobj * noobj_confidence_losses \\\n",
    "                + class_losses\n",
    "        return total / self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab43cf-8f8d-4102-9ed2-d5391d7d394b",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Once the loss function is written, assembling the network is straightforward. While it is possible to perform end-to-end training with the entire network, we will use a pre-trained backbone network (e.g. a ResNet trained on ImageNet or a subset of it), with the YOLO detection head placed on top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d2f65-6b11-45e4-a2c2-17587e2e8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self, backbone, back_channels, num_classes=20, num_anchors=3, grid_size=7, pre_trained = True):\n",
    "        super(YOLO, self).__init__()\n",
    "        self.num_classes = num_classes #(C)\n",
    "        self.num_anchors = num_anchors # number of  anchor boxes (B)\n",
    "        self.grid_size = grid_size # (S)\n",
    "        self.backbone = backbone # backbone network\n",
    "        self.back_channels = back_channels\n",
    "        if pre_trained:\n",
    "            self.backbone.requires_grad_(False) # Freeze backbone weights\n",
    "\n",
    "        ## Replace the last layers with Identity layers and attach detection layers\n",
    "        self.backbone.avgpool = nn.Identity()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "                \n",
    "        ## detection Head\n",
    "        self.detector = nn.Sequential(\n",
    "            nn.Conv2d(self.back_channels ,1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024,1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),            \n",
    "            nn.ReLU(),        \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*1024, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, grid_size*grid_size*(num_anchors * 5 + num_classes))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.reshape(features, (-1, self.back_channels , 14, 14))\n",
    "        predictions = self.detector(features)\n",
    "        # reshape the predictions to match the label dimension (batch_size, S, S, B*5 + C)\n",
    "        return predictions.view(-1, self.grid_size, self.grid_size, self.num_anchors * 5 + self.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9332e2f-7663-4dcd-af33-a4eb001f082a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To fine-tune our YOLO network, we use the [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset in its version from 2012. It consists of 20 classes, providing multiple bounding boxes for multiple objects in a scene. It also contains scenes with objects of multiple classes.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/pascalvoc_samples.png\" width=\"650\"/>\n",
    "    <p><i>Figure 4: Samples from the first four classes of the PascalVOC dataset. Image taken from Everingham et al. (2010)</i></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3b753-3411-4976-a641-b925d8121240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n",
    "              'dog', 'horse', 'motorbike','person','pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "n_classes = len(class_list)\n",
    "C = n_classes # number of classes\n",
    "B = 3 # number of anchor boxes\n",
    "S = 7 # number of grid cells\n",
    "image_size = (448,448)\n",
    "\n",
    "train_set = PascalVocDataset('train', class_list, num_anchors = B, grid_size=S, image_size=image_size, normalize=True, augment=True)\n",
    "val_set = PascalVocDataset('test', class_list, num_anchors = B, grid_size=S, image_size=image_size, normalize=False, augment=False)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, \n",
    "                                           shuffle=True, num_workers=4, persistent_workers=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, \n",
    "                                           shuffle=False, num_workers=4, persistent_workers=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1df074",
   "metadata": {},
   "source": [
    "Let's have a look how an image from the test set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f90c96-77b0-436a-ac33-e3c6089efa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label, _ = val_set[3]\n",
    "plot_boxes(img, label, class_list, max_overlap=float('inf'),image_size = image_size, min_confidence=0.2, file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705db2f-1288-4d93-808c-decdf0019285",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a2823",
   "metadata": {},
   "source": [
    "At first, we create the YOLO model including the backbone model. As backbone model, we use a ResNet50 trained on ImageNet from PyTorch. Of course, you can use any network from any platform which seems sensible, even the models implemented by yourself in the previous tutorials. The better the backbone model is trained, the better the YOLO model will perform. Additionally, it makes sense to have the backbone model pre-trained on a similar dataset like the one given for the detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc378ff-7a29-4c4f-a57e-25efef53b8e3",
   "metadata": {},
   "source": [
    "As we need the number of feature maps (or out_channels in PyTorch) of the pre-trained ResNet, we first print its architecture to find the layer we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa912d5-6947-4332-899e-3ea60eb280a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "## backbone model\n",
    "# Pre-trained ResNet50 from PyTorch\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb18847-304a-4d13-ac9c-ce662a885c97",
   "metadata": {},
   "source": [
    "We want the number of feature maps from the last convolutional layer. To access this, we need to get the last sequential part (called _layer4_), want the third Bottleneck - module and there the third (as it is the last) convolutional layer. So the Python code line would be\n",
    "```\n",
    "ResNet.layer4[2].conv3.out_channels\n",
    "```\n",
    "In our example it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3a532-f7c2-4552-8536-10073b03a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_outchannels = backbone.layer4[2].conv3.out_channels\n",
    "print(back_outchannels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707b78e-bc34-4669-9199-6626167ca5ef",
   "metadata": {},
   "source": [
    "Keep in mind, that if you change the backbone model, you also have to change how to access the out_channels. \n",
    "\n",
    "Now we can initialize the YOLO network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd365fc6-2d61-4490-b7fc-462293bad11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOLO\n",
    "model = YOLO(backbone, back_outchannels, num_classes = C, num_anchors=B, grid_size=S, pre_trained=True)\n",
    "\n",
    "print('Trainable Parameters in YoloNetwork: %.3fM' % get_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff02f0e",
   "metadata": {},
   "source": [
    "Second, we define the optimizer and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a658cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001,)\n",
    "loss_function = SumSquaredErrorLoss(epsilon = 1e-6, B=B, C = C, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ae344",
   "metadata": {},
   "source": [
    "And finally, we start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0881e74-f684-4957-abd2-020963d8d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "model.to(device)\n",
    "num_epochs = 10#50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]')\n",
    "    \n",
    "    for data, labels, _ in train_pbar:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.forward(data)\n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    print('Loss after epoch: ', train_loss)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# save model\n",
    "results_folder = 'yolo_model/'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "torch.save(model.state_dict(), f'{results_folder}model_dict.pt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377e699",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69e67b-1dc4-4f6b-b0bf-a227b2ea9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use no_grad as we did not want to train further\n",
    "model = model = YOLO(backbone, back_outchannels, num_classes = C, num_anchors=B, grid_size=S, pre_trained=True)\n",
    "model.load_state_dict(torch.load('yolo_model/model_dict.pt', weights_only=True))\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, _ in val_loader:\n",
    "        data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        outputs = outputs.to('cpu')\n",
    "        plot_boxes(data[5].to('cpu'), outputs[5], class_list, \n",
    "                   max_overlap=float('inf'),image_size = image_size, min_confidence=0.8, file=None)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f92c68-3de5-4892-9744-c590af8f4893",
   "metadata": {},
   "source": [
    "### Mean average precision\n",
    "\n",
    "Besides of looking on images and how well objects have been detected, a very common method to evaluate object detection and also object segmentation models is to calculate the mean average precision value (mAP). \n",
    "\n",
    "To calculate the mAP, we first introduce how to measure the correctness of a binary classificator. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/conf_mat.png\" width=\"500\"/>\n",
    "    <p><i>Figure 5: Confusion matrix</i></p>\n",
    "</div>\n",
    "\n",
    "As depicted inf Figure 5, for a binary classificator there are four combinations between the model prediction and the ground truth value:\n",
    "1. **True positive (TP)**: The model predicts correctly an object, that exists.\n",
    "2. **False positive (FP)**: The model predicts falsly an object, which does not exist.\n",
    "3. **False negative (FN)**: The model, falsly, does not predict an object, which exists.\n",
    "4. **True negative (TN)**: The model, correctly, does not predict an object, which does not exist.\n",
    "\n",
    "These combinations can be used to quantize how well the model predicts an object correctly in relation to all the predicted objects (called **precision**) \n",
    "\n",
    "$$\n",
    " P = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "\n",
    "and how well the model predicts an object correctly in relation to all ground truth objects (called **recall**)\n",
    "\n",
    "$$\n",
    " R = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d861977-27cf-4548-9639-33c190693168",
   "metadata": {},
   "source": [
    "To calculate the mAP, we now have to define, if a model predicts an object or not with the help of the $IoU$.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/mAP-basic.png\" width=\"750\"/>\n",
    "    <p><i>Figure 6: The IoU is used to determine if a bounding box is a falsly predicted box (FP) or a correctly predicted box (TP) </i></p>\n",
    "</div>\n",
    "\n",
    "To do so, our algorithm has to do the following steps:\n",
    "1. Measure for each predicted bounding box, if it is a correct one (TP) or a false one (FP)\n",
    "2. For each class:\n",
    "   - Sort all predicted bounding box via their confidence scores in a descending order\n",
    "   - Calculate the precision and recall value for each predicted bounding box, starting by the first box (with the highest confidence score). Accumulate the detected FP and TP boxes for the following boxes.\n",
    "   - Calculate the average precision score with: $AP = \\sum_n (R_n - R_{n-1})P_n$\n",
    "3. After calculating the $AP$ for each class, calculate the mean average precision (mAP):\n",
    "\n",
    "$$\n",
    " mAP = \\frac{1}{N_{classses}} \\sum_{i=1}^{N_{classes}} AP_i\n",
    "$$\n",
    "\n",
    "\n",
    "More information about the (mean) average precision value can be found [here](https://github.com/rafaelpadilla/Object-Detection-Metrics) or [here](https://www.v7labs.com/blog/mean-average-precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82f668-3d3e-485a-9b83-de7a490c12fa",
   "metadata": {},
   "source": [
    "#### Step 1: Get the measurement for each bounding box\n",
    "\n",
    "A predicted bounding box is considered true positive (TP) if the $IoU$ is above a minimum threshold and if it predicts the correct class.\n",
    "It is considered a false positive (FP) if the $IoU$ is below the threshold or a wrong class is predicted.\n",
    "\n",
    "We use a minimum threshold for the confidence value to sort out bounding boxes with very low confidence, which will create a lot of false positive boxes.\n",
    "This means that the minimum confidence value is an additional hyperparameter that needs to be tuned after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac65d1-2b64-4be6-9808-3df50a258317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortout_redudancy(bboxes):\n",
    "    bboxes = [ee for n,ee in enumerate(bboxes) if ee not in bboxes[:n]]\n",
    "    return bboxes\n",
    "    \n",
    "def measure_prediction(data, gt, p, class_list, min_confidence, min_iou, pred_list, gt_dict):\n",
    "    ## data -> image input data\n",
    "    ## gt -> ground truth boxes\n",
    "    ## p -> predictions    \n",
    "    ## class_list -> list of all classes\n",
    "    ## min_confidence -> minimum confidence score a box must have to be considered\n",
    "    ## min_iou -> IoU threshold that we use to distinguish between TP(>= min_iou) and FP(< min_iou) boxes\n",
    "    ## pred_list -> list of already evaluated predicted boxes, and where we add more evaluated boxes to it\n",
    "    ## gt_dict -> dictionary to count the total number of ground truth boxes for each class  \n",
    "    ## return -> updated pred_list and updated gt_dict\n",
    "\n",
    "    \n",
    "    images_size = (data.size(dim=2),data.size(dim=3))\n",
    "    n_samples, s_x, s_y, n_points = np.shape(p)\n",
    "    num_classes = len(class_list)\n",
    "\n",
    "    grid_size_x = data.size(dim=3) / s_x\n",
    "    grid_size_y = data.size(dim=2) / s_y\n",
    "\n",
    "    ## iterate over all images that we have\n",
    "    for d in range(len(data)):\n",
    "        ## get the ground truth boxes\n",
    "        gt_box = get_bboxes(s_x, s_y, gt[d], grid_size_x, grid_size_y, num_classes, min_confidence=0.5, image_size = images_size)\n",
    "        gt_box = sortout_redudancy(gt_box)    \n",
    "        num_boxes_gt = len(gt_box)\n",
    "\n",
    "        ## update gt_dict\n",
    "        for b in gt_box:\n",
    "            if b[-1] in gt_dict:\n",
    "                gt_dict[b[-1]] = gt_dict[b[-1]] +1\n",
    "            else:\n",
    "                gt_dict[b[-1]] = 1\n",
    "        \n",
    "        ## get only the prediced bboxes of predictions over the min_confidence score\n",
    "        p_box = get_bboxes(s_x, s_y, p[d], grid_size_x, grid_size_y, num_classes, min_confidence=min_confidence, image_size = images_size)\n",
    "        # Sort by highest to lowest confidence\n",
    "        p_box = sorted(p_box, key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "        num_boxes_p = len(p_box)\n",
    "\n",
    "        ## initialize the array to save the IoU values\n",
    "        iou_p_gt = [[0 for _ in range(num_boxes_gt)] for _ in range(num_boxes_p)]\n",
    "\n",
    "        ## for each predicted bounding box, we use the highest IoU to link it to one\n",
    "        ## ground truth box and to check, if both boxes predicting the same class\n",
    "        \n",
    "        ## additional list to track, if a ground truth box is already linked with\n",
    "        ## a bounding box with a higher confidence score\n",
    "        list_max_gt_arg = []\n",
    "\n",
    "        ## iterate over all predicted boxes in the current sample\n",
    "        for pb in range(num_boxes_p):\n",
    "            \n",
    "            # calculate the IoU of the predicted bbox to all ground truth bboxes\n",
    "            for j in range(num_boxes_gt):\n",
    "                iou_p_gt[pb][j] = get_overlap(p_box[pb], gt_box[j])\n",
    "            ## take the ground truth bbox with the highest IoU and check, if its the same class\n",
    "            max_gt_arg = np.argmax(iou_p_gt[pb])\n",
    "            ## if a p_bbox with a higher confidence score is already linked to same gt_bbox\n",
    "            ## ignore the actual p_bbox\n",
    "            if max_gt_arg in list_max_gt_arg:\n",
    "                continue\n",
    "            else:\n",
    "                list_max_gt_arg.append(max_gt_arg)   \n",
    "                ## check if the predicted classes are the same\n",
    "                if p_box[pb][-1] == gt_box[max_gt_arg][-1]:\n",
    "                    ##check if iou < or > than min_iou\n",
    "                    if iou_p_gt[pb][max_gt_arg] >= min_iou:\n",
    "                        # TP\n",
    "                        class_i = p_box[pb][-1]\n",
    "                        conf_score = p_box[pb][-2]\n",
    "                        pred_list.append([class_i, conf_score,1])\n",
    "                    else:\n",
    "                        # FP\n",
    "                        class_i = p_box[pb][-1]\n",
    "                        conf_score = p_box[pb][-2]\n",
    "                        pred_list.append([class_i, conf_score,0])\n",
    "                #if classes are different\n",
    "                else:\n",
    "                    # FP\n",
    "                    class_i = p_box[pb][-1]\n",
    "                    conf_score = p_box[pb][-2]\n",
    "                    pred_list.append([class_i, conf_score,0])\n",
    "            \n",
    "    ## return a list for predicted bounding boxes, \n",
    "    ## the class, the confidence score, and is it TP or FP\n",
    "    ## TP are indicated with 1 and FP with 0\n",
    "    ## and also return the number of ground truth boxes per class\n",
    "    return(pred_list, gt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6fe74-8797-4588-9d26-04b481a60f18",
   "metadata": {},
   "source": [
    "Use the pre-trained network and iterate over the entire validation set to gather the necessary information for each predicted bounding box.\n",
    "To save computation time, we iterate over the validation set only once. As we compute the AP for each class, we also store the predicted class index for each bounding box, along with its confidence score and whether it is a TP or FP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85c7ef-e561-4b00-9add-e2d716fa93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = model = YOLO(backbone, back_outchannels, num_classes = C, num_anchors=B, grid_size=S, pre_trained=True)\n",
    "model.load_state_dict(torch.load('yolo_model/model_dict.pt', weights_only=True))\n",
    "model = model.to(device)\n",
    "\n",
    "## we go through the test set\n",
    "## save for every predicted bounding box > min_confidence\n",
    "## the class, confidence value, and if its TP or FP\n",
    "## TP = 1 / FP = 0\n",
    "## we also save the total number of ground truth bboxes per class\n",
    "\n",
    "## list to save the each p_bbox, \n",
    "## its class, its confidence score,\n",
    "## and if TP(1) or FP(0)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "## list to save number of ground truth\n",
    "## bboxes per class\n",
    "gt_dict = {}\n",
    "\n",
    "val_pbar =  tqdm(val_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, _ in val_pbar:\n",
    "        data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        outputs = outputs.to('cpu')\n",
    "        pred_list, gt_dict = measure_prediction(data = data, p = outputs, \n",
    "                                                gt = labels, class_list = class_list, \n",
    "                                                min_confidence=0.15, min_iou = 0.2, pred_list = pred_list, gt_dict = gt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29465099-8497-4ef4-a5e9-7c3a7f6296d7",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate the average precision for each class\n",
    "After we have collected all the necessary data, we create a dataframe from the data to have easier data management.\n",
    "Then we iterate over all the classes and calculate the AP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae90c0-22bf-446e-8fc4-6799c50a7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# create a pandas data frame to have better access to the values\n",
    "df = pd.DataFrame(pred_list, columns = ['class', 'confScore', 'TP/FP'])\n",
    "\n",
    "## iterate over all classes and calculate the average precision AP\n",
    "all_ap = np.zeros(len(gt_dict))\n",
    "\n",
    "for c in range(len(gt_dict)):\n",
    "    # number of all ground truth bboxes\n",
    "    gt_total = gt_dict[c]\n",
    "\n",
    "    # get entries only for the current class\n",
    "    df_c = df.loc[df['class'] == c]\n",
    "\n",
    "    # sort them over the confidence score\n",
    "    df_c = df_c.sort_values(by = 'confScore', ascending = False)\n",
    "    ## no go through all predictions and calculate Precision and Recall\n",
    "    list_precision = np.zeros(len(df_c))\n",
    "    list_recall = np.zeros(len(df_c))\n",
    "    acc_tp = 0\n",
    "    acc_fp = 0\n",
    "    ap = 0\n",
    "    for d in range(len(df_c)):\n",
    "        row = df_c.iloc[d]\n",
    "        if row['TP/FP'] == 1:\n",
    "            acc_tp +=1\n",
    "        if row['TP/FP'] == 0:\n",
    "            acc_fp +=1\n",
    "        list_precision[d] = acc_tp/(acc_tp + acc_fp)\n",
    "        list_recall[d] = acc_tp/(gt_dict[c]) # gt_dict[c] == TP + FN\n",
    "        if d > 0:\n",
    "            ap += (list_recall[d] - list_recall[d-1]) * list_precision[d]\n",
    "    \n",
    "    all_ap[c] = ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6d459-3af8-4249-ac47-bacc39b1c4cb",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the mean average precision\n",
    "By simply calculating the average over all AP values, we can calculate the mAP score.\n",
    "Since the $IoU$ threshold is crucial to discriminate between TP and FP samples, and thus can strongly influence the mAP score, it is conventional to mention the value in percent.\n",
    "For example, if we use a $min\\_iou = 0.2$, we specify the mAP value with $mAP_{20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f7b19-60ba-45af-86b5-2a11ba2f530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mAP_{20} = ', np.mean(all_ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb785944-2358-4fbb-9e23-fc3288747252",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d302398",
   "metadata": {},
   "source": [
    "### 1. Parameters for the mAP\n",
    "As mentioned above, the mAP value depends on two parameters (in addition to network performance): The lower threshold for the confidence score and the threshold for the $IoU$ parameter.\n",
    "Change both of them and see how the mAP value changes. Find out which parameter configuration gives you the highest mAP value, and plot some samples to see if the predictions make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a865ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1eb9e0",
   "metadata": {},
   "source": [
    "### 2. Change backbone model\n",
    "\n",
    "Switch from the pre-trained ResNet50 from PyTorch to an other backbone model. You can list all available models with `torchvision.models.list_models()`. Note, that all PyTorch models are solely trained on ImageNet. For more information, see [here](https://docs.pytorch.org/vision/main/models.html).\n",
    "Of course, you can use other resources, e.g. `timm`.\n",
    "Do not forget to make certain changes at the YOLO model class, depending on your chosen backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04729041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search for available pre-trained models\n",
    "from torchvision import models\n",
    "torchvision_models = models.list_models()\n",
    "print(f\"torchvision provides {len(torchvision_models)} pre-trained models.\")\n",
    "# filter for specific model type, e.g. EfficientNet\n",
    "torchvision_models_effNet = models.list_models(include='efficientnet*')\n",
    "print(f\"Available pretrained EfficientNet models ({len(torchvision_models_effNet)}):\")\n",
    "for model in torchvision_models_effNet:\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "import timm\n",
    "timm_models = timm.list_models(pretrained=True)\n",
    "print(f\"\\ntimm provides {len(timm_models)} pretrained models.\")\n",
    "# filter for specific model type, e.g. EfficientNet\n",
    "timm_models_effNet = timm.list_models(filter='efficientnet*', pretrained=True)\n",
    "print(f\"Available pretrained EfficientNet models ({len(timm_models_effNet)}):\")\n",
    "for model in timm_models_effNet:\n",
    "    print(f\"  - {model}\")\n",
    "    \n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2924d",
   "metadata": {},
   "source": [
    "### 3. Training YOLO from end-to-end\n",
    "\n",
    "As seen above, YOLO can be trained with a pre-trained ResNet50 and frozen layers. However, it is also possible to train the entire network (end-to-end training), as end-to-end training can lead to better results than using a pre-trained network.\n",
    "\n",
    "You just have to define a backbone network (like a CNN from one of the previous notebooks) and make sure that the weights are trainable.\n",
    "Note that training the entire network will use more VRAM on the GPU and will take longer to train the same number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146622e-5724-4e17-a213-4278b9501474",
   "metadata": {},
   "source": [
    "### 4. Change from YOLOv1 to YOLOv2\n",
    "\n",
    "In this notebook, we demonstrated how to implement the first version of the YOLO network (called YOLOv1). Not long after its release, the developer introduced YOLOv2, with many different improvements. A good description of the changes and why they were made can be found [here](https://medium.com/@sachinsoni600517/yolo-v2-comprehensive-tutorial-building-on-yolo-v1-mistakes-aa7912292c1a) and in the original [paper](https://arxiv.org/pdf/1612.08242v1.pdf).\n",
    "\n",
    "Task: Implement YOLOv2, based on our YOLOv1 variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f4219-06f5-4272-bf03-2c2b4dc83b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
