{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1f43a2",
   "metadata": {},
   "source": [
    "# Tutorial 1: How to create a simple deep neural network with PyTorch\n",
    "\n",
    "In this first Tutorial we will cover the basic steps and elements to create a deep neural network in PyTorch.\n",
    "\n",
    "## Preperations\n",
    "Before we get started, make sure that PyTorch, Torchvision, and Torchaudio are properly installed. If not, follow the previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a9eaa",
   "metadata": {},
   "source": [
    "### Checking the available devices\n",
    "\n",
    "To optimize the processing graph of the neural network, PyTorch uses the \"device\" variable to define if the calculation of the neural network should be performed on a CUDA-device (a GPU) or the CPU. \n",
    "Is it recommend to use the GPU, as the training procedure will be calculated much faster than on the CPU.\n",
    "\n",
    "Therefore, after importing the main package torch, we check with `torch.cuda.is_available()` if a CUDA capable GPU is available. \n",
    "If not, we will run the network on the CPU, otherwise on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebb08d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Device is ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007e304",
   "metadata": {},
   "source": [
    "### Import training and test set\n",
    "We use the Torchvision package, as it contains many common datasets, to load the training and test set for the MNIST dataset.\n",
    "It contains images from handwritten digits with a resolution of $28 \\times 28$ pixels.\n",
    "\n",
    "With help of the `torchvision.transforms` sub package, we create a transform-object which we can use to normalize the pixel-value. Why data needs to be normailzed or be preprocessed otherwise, will be covered in a future tutorial.\n",
    "\n",
    "Additionally, we will create two `DataLoader`objects, one for the training and one for the test set, which we will use to load the datasets batchwise. Splitting the whole dataset in smaller batches often results in faster training and a stable convergence.\n",
    "\n",
    "`trainset.classes` gives us the labels of the classes, but we can define them as we like. It is technically irrelevant for the training. The output of the network will be matched with this list for human readabilty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12e1946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5),(0.5))])\n",
    "batch_size=4\n",
    "trainset = torchvision.datasets.MNIST(root='../Dataset/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../Dataset/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# list of class labels\n",
    "classes = trainset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a3898",
   "metadata": {},
   "source": [
    "### Plot some sample images\n",
    "\n",
    "We use the Matplotlib library to show the first four samples from the MNIST training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ffe4355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes are: \n",
      "1 - one 8 - eight 0 - zero 1 - one\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(imges):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        img = imges[i,0]\n",
    "        img = img / 2 + 0.5 #unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(npimg, cmap='gray')\n",
    "    plt.show\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images)\n",
    "\n",
    "print('Classes are: ')\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf6791",
   "metadata": {},
   "source": [
    "## Create the deep neural network\n",
    "\n",
    "### The network class\n",
    "\n",
    "To define a neural network in PyTorch, we create a class object which inherits from the `torch.nn.Module`class.\n",
    "Inside of this class, we 1) create the layer objects which will build the structure of our network and then 2) define the feedforward processing path through the network. \n",
    "\n",
    "#### Define the network structure\n",
    "\n",
    "This is done in the `__init__`function.\n",
    "As many common layers are predefined in PyTorch already, we use the `torch.nn` sub package to access them. We want to create a network with one input layer, a hidden layer and an output layer. Each fully-connected layer has the number of inputs and the number of outputs as parameters.\n",
    "\n",
    "The first layer gets the MNIST-images as input which is why the number equals $28x28$. The output parameter can be freely chosen and expiremented with. It corresponds to the number of neurons in its layer and is a hyperparameter to be tuned.\n",
    "\n",
    "`self.fc1 = nn.Linear(16*4*4,120)`\n",
    "\n",
    "Then we add a second fully-connected layer which needs the number of outputs of the previous layer as its number of inputs. The output, again, is to be experimented with.\n",
    "\n",
    "`self.fc2 = nn.Linear(120,84)`\n",
    "\n",
    "We end with the classification head (also a fully-connected layer) with 10 output neurons which correspond to our 10 MNIST-labels (0-9):\n",
    "\n",
    "`self.fc3 = nn.Linear(84,10)`\n",
    "\n",
    "Please note, that here we only define, which layers we want to use in our network. The order of defining them is not important at the moment.\n",
    "\n",
    "#### Define the feedforward path\n",
    "\n",
    "Now that the structure is defined, we 2) define the feedforward processing path through the network in the `forward(self, x)` function. \n",
    "Or to say it differently, we connect the single layers with each other by defining which layers output becomes which layers input.\n",
    "\n",
    "As `x` represents the input to our network, it is processed by the first fully connected layer. Additionally, we apply the ReLU function as activation function on the output of each layer except the output layer.\n",
    "\n",
    "`x = F.relu(self.fc1(x))`\n",
    "\n",
    "`x = F.relu(self.fc2(x))`\n",
    "\n",
    "Finally, we return the output of the classification head as the final output of our network:\n",
    "\n",
    "`x = self.fc3(x)`\n",
    "\n",
    "`return x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d46bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## define the network structure with the layers\n",
    "        self.fc1   = nn.Linear(28*28, 120) # in_channels, out_channels\n",
    "        self.fc2   = nn.Linear(120,84) # in_channels, out_channels\n",
    "        self.fc3   = nn.Linear(84,10) # in_channels, out_channels\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## define the functionality of each layer/between the layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575f972",
   "metadata": {},
   "source": [
    "#### Creating the network\n",
    "Now we can simply create a new network object and assign it to the device identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc12351",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e36219",
   "metadata": {},
   "source": [
    "### Set up loss and optimizer\n",
    "We use the `CrossEntropyLoss`as loss-function on the error between the network output and the correct labels and using the simple\n",
    "Stochastic-Gradient-Desced (`SGD`) optimizer.\n",
    "Please note, that the optimizer needs the parameters of our network `net.parameters()` as input to define which need to be tuned.\n",
    "\n",
    "Further available loss functions and optimizers can be found on the [official PyTorch website](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ce6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86361c19",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "To train the network, we need a double for-loop.\n",
    "The inner loop uses the trainloader object defined above to iterate over the batches of the training set. \n",
    "\n",
    "For each batch, we get the input data (the images) and the corresponding labels and assign them to the identified device due to performance reasons.\n",
    "    \n",
    "As we calculate the gradients of the weights for each batch independently, we set the gradients to zero:\n",
    "    \n",
    "`optimizer.zero_grad()`\n",
    "    \n",
    "Then we calculate the output of the network, depending on the inputs, calculating the loss and then propagating the error along the backward path automatically.\n",
    "    \n",
    "`outputs = net(inputs)`\n",
    "    \n",
    "`loss = criterion(outputs, labels)`\n",
    "    \n",
    "`loss.backward()`\n",
    "    \n",
    "We apply the gradients via the optimzer:\n",
    "    \n",
    "`optimizer.step()`\n",
    "    \n",
    "And printing out the loss from the current batch at the end.\n",
    "\n",
    "All this is repeated for 2 epochs (the outer loop).\n",
    "\n",
    "To pass the 28x28 image to the first layer of the neural network, we need to flatten the image to convert it from a 2D to a 1D-vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b726f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.816\n",
      "[1,  4000] loss: 0.381\n",
      "[1,  6000] loss: 0.333\n",
      "[1,  8000] loss: 0.278\n",
      "[1, 10000] loss: 0.244\n",
      "[1, 12000] loss: 0.217\n",
      "[1, 14000] loss: 0.209\n",
      "[2,  2000] loss: 0.172\n",
      "[2,  4000] loss: 0.157\n",
      "[2,  6000] loss: 0.165\n",
      "[2,  8000] loss: 0.152\n",
      "[2, 10000] loss: 0.159\n",
      "[2, 12000] loss: 0.151\n",
      "[2, 14000] loss: 0.137\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the input data and labels\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.flatten(start_dim=1)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimze\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch +1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79827af5",
   "metadata": {},
   "source": [
    "After training, we save the parameters of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9dc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './mnist_mlp_net.pth'\n",
    "torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ab52d",
   "metadata": {},
   "source": [
    "## Test the network performance\n",
    "We use the testloader object to see, how well the network will recognize the numbers in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fb7ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5aca3",
   "metadata": {},
   "source": [
    "We create a new network object and load the parameters from the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb1b789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(path, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23157e90",
   "metadata": {},
   "source": [
    "By passing the flattened first images as parameter to the `net()`, we compute the classification for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2f30fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  7 - seven 2 - two 1 - one 0 - zero\n"
     ]
    }
   ],
   "source": [
    "output = net(images.flatten(start_dim=1))\n",
    "\n",
    "_, predicted = torch.max(output, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb2673",
   "metadata": {},
   "source": [
    "As this looks very promising, we write a loop to get the data from the testload batchwise and pass them through the network. To compute the accuracy, we check how many images were correctly identified and devide it by the total number of images.\n",
    "\n",
    "To avoid further training and to reduce the computational costs, we deactivate the calculation of gradients with `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40128b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 96%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total   = 0\n",
    "\n",
    "# use no_grad as we do not want further training\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.flatten(start_dim=1)\n",
    "        outputs = net(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the test set: {100*correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb84b50",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement a multi-layer neural network for the FashionMNIST dataset\n",
    "\n",
    "Use the previous code as guidance for this task. The FashionMNIST is a little more complex dataset than the classic MNIST but only uses 10 classes of simple images of clothing items. Load them as before with `trainset = torchvision.datasets.FashionMNIST(root='./data/', train=True, download=True, transform=transform)`; test set accordingly.\n",
    "\n",
    "Experiment with layer sizes, number of epochs, number of layers and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45023c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# immport torch and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "774bf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d540b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network layer and feedforward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fa8dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network and define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8b72e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aa79143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a3d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a5141",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So in this tutorial we have covered:\n",
    "\n",
    "1. How to use DataLoader to initialize the training and test set to present them batchwise to the network.\n",
    "2. How to define the structure of the network and the feedforward path.\n",
    "3. How to define the loss function and optimizer.\n",
    "4. How to write the training loop to apply the gradients on the network parameters.\n",
    "5. How to measure the network performance on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
