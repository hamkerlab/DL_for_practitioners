{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afa7f6416545eea",
   "metadata": {},
   "source": [
    "# Tutorial 2.4: ResNet\n",
    "\n",
    "Author: [Erik Syniawa](mailto:erik.syniawa@informatik.tu-chemnitz.de)\n",
    "\n",
    "As introduced in \"Deep Residual Learning for Image Recognition\" by He et al. (2017) [[1](#6-references)]. The implementation is based on the implementaion of the  torchvision library [[2](#6-references)] with slight modifications to make the code more cohesive with later models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db1b4f-9e69-4616-b43c-e8a75bf36e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple, Type, Union\n",
    "\n",
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "    \n",
    "from Utils.little_helpers import timer, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005f16db3dcb3c7",
   "metadata": {},
   "source": [
    "## 1. The Degradation Problem\n",
    "As networks get deeper, they become harder to train, with both training and test accuracy getting worse. This is counterintuitive because a deeper network should be able to represent everything a shallower network can (by making some layers act as identity mappings).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/training_error.png\" width=\"750\"/>\n",
    "    <p><i>Figure 1: Training error for plain and residual networks. Source: [1]</i></p>\n",
    "</div>\n",
    "\n",
    "Figure 1 shows that in the plain network the deeper 34-layer network has a __higher__ error than the 18-layer network. In residual networks, the deeper 34-layer network has __lower__ error than the 18-layer network. This is a key insight of the paper [1] - residual connections help deeper networks to train better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd389371b38b447",
   "metadata": {},
   "source": [
    "## 2. Understanding Residual Learning\n",
    "\n",
    "The key insight of ResNet is to reformulate the layers as learning a residual mapping instead of the underlying mapping.\n",
    "Instead of hoping a stack of layers directly fits a desired mapping $H(x)$, we let these layers fit a residual function $F(x) = H(x) - x$.\n",
    "The original mapping becomes $F(x) + x$, which is implemented with a \"shortcut connection\" that performs identity mapping and element-wise addition (see Figure 2).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/residual_block.png\" width=\"750\"/>\n",
    "    <p><i>Figure 2: Comparison of residual and non-residual blocks</i></p>\n",
    "</div>\n",
    "\n",
    "1. If an identity mapping is optimal, it's easier to push the residual to zero than for stacked layers to learn the identity function\n",
    "2. If the optimal mapping is close to identity, small perturbations are easier to learn than learning from scratch\n",
    "3. Shortcut connections allow gradient to flow directly through the network, mitigating vanishing gradient problems\n",
    "\n",
    "### 2.1 The Loss Landscape of ResNets ([Li et al., 2018](https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html))\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/loss_landscape.png\" width=\"750\"/>\n",
    "    <p><i>Figure 3: The loss surfaces of ResNet-56 with/without skip connections. Source: Li et al. (2018)</i></p>\n",
    "</div>\n",
    "\n",
    "The visualization above reveals several critical insights:\n",
    "\n",
    "1. **Without skip connections** (left): The loss landscape has many sharp local minima and steep cliff-like regions. Such landscapes are extremely difficult to optimize because:\n",
    "   - Gradients can point in unhelpful directions\n",
    "   - Learning can easily get trapped in poor local minima\n",
    "   - Small perturbations in weights can cause dramatic changes in loss\n",
    "\n",
    "2. **With skip connections** (right): The loss landscape becomes smooth and convex, with a clear, wide basin leading to probable good solutions. Summa Summarum:\n",
    "   - Allows for more reliable gradient flow\n",
    "   - Makes optimization much less sensitive to initialization\n",
    "   - Creates wider minima that tend to generalize better\n",
    "\n",
    "This visualization helps explain why very deep networks become trainable with residual connections. Skip connections essentially provide \"highways\" for gradient flow, preventing the vanishing/exploding gradient problem while creating a more navigable optimization landscape.\n",
    "\n",
    "The dramatic smoothing effect becomes increasingly important as networks get deeper. For shallow networks, the benefits are less pronounced, but for deep architectures, skip connections transform what would be untrainable networks into highly effective models. This is not only important for ResNets but also for other architectures that use skip connections, such as Vision Transformers that we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe03a99469a069",
   "metadata": {},
   "source": [
    "## 3. Implementation of ResNet Building Blocks\n",
    "\n",
    "### 3.1 Key components\n",
    "\n",
    "#### 3.1.1 Batch Normalization ([Ioffe & Szegedy, 2015](https://proceedings.mlr.press/v37/ioffe15.html))\n",
    "\n",
    "Batch Normalization (BatchNorm) is a technique that normalizes the activations of each layer, which helps to stabilize and accelerate training. For each feature channel, BatchNorm:\n",
    "\n",
    "- Normalizes the activations to have zero mean and unit variance across the batch\n",
    "- Applies learnable scale ($\\gamma$) and shift ($\\beta$) parameters to preserve the network's representational capacity\n",
    "\n",
    "BatchNorm offers several benefits:\n",
    "\n",
    "- Reduces internal covariate shift (the distribution of activations changing during training)\n",
    "- Provides regularization, reducing the need for dropout and potentially allowing higher learning rates \n",
    "- Makes networks less sensitive to weight initialization\n",
    "\n",
    "In ResNet, BatchNorm is applied after each convolution and before the activation function:\n",
    "\n",
    "```python\n",
    "# Correct structure:\n",
    "self.conv = nn.Conv2d(...)\n",
    "self.bn = nn.BatchNorm2d(...)\n",
    "# In forward:\n",
    "out = F.relu(self.bn(self.conv(x)))  # BN before ReLU\n",
    "```\n",
    "\n",
    "#### 3.1.2 ReLU Activation Function\n",
    "The Rectified Linear Unit (ReLU) is defined as:\n",
    "\n",
    "$f(x) = \\max(0, x)$\n",
    "\n",
    "ReLU became the standard activation function in early modern CNNs for several reasons:\n",
    "\n",
    "- It allows faster convergence compared to sigmoid or tanh due to non-saturating behavior for positive values\n",
    "- It's computationally efficient (simple thresholding operation)\n",
    "- It induces sparsity in the network, as negative values become zero\n",
    "\n",
    "However, ReLU has important limitations:\n",
    "\n",
    "- Dying ReLU problem: Neurons can \"die\" during training when they consistently receive negative inputs, causing their gradients to become zero and preventing further learning\n",
    "- Non-zero centered: Unlike tanh, ReLU outputs are not zero-centered, which can lead to zig-zagging dynamics during gradient descent\n",
    "\n",
    "To address these issues, several improved activation functions have been developed:\n",
    "\n",
    "- **Leaky ReLU**: $f(x) = \\max(0, x) + \\alpha \\min(0, x)$ where $\\alpha$ is a small constant (e.g., 0.01)\n",
    "- **PReLU** (Parametric ReLU): Similar to Leaky ReLU but with learnable $\\alpha$\n",
    "- **ELU** (Exponential Linear Unit): $f(x) = x$ if $x > 0$ else $\\alpha(e^x - 1)$\n",
    "- **SiLU/Swish**: $f(x) = x \\cdot \\sigma(x)$ where $\\sigma$ is the sigmoid function\n",
    "- **GELU** (Gaussian Linear Error Unit): $f(x) = x \\cdot \\Phi(x)$ where $\\Phi$ is the cumulative distribution function of the standard normal distribution\n",
    "\n",
    "More recent architectures often use SiLU/Swish or GELU activations, which provide better gradient flow and typically lead to better performance, especially in deeper networks. However, the original ResNet architecture uses ReLU activations throughout.\n",
    "\n",
    "In ResNets, ReLU is typically applied after BatchNorm:\n",
    "\n",
    "```python\n",
    "out = F.relu(self.bn(self.conv(x)))  # BN before ReLU\n",
    "```\n",
    "\n",
    "#### 3.1.3 Weight Initialization\n",
    "\n",
    "Proper initialization is crucial for training very deep networks. ResNet typically uses He initialization (also known as Kaiming initialization). In practice, PyTorch will handle this automatically. For a detailed explanation over different initialization of different activation functions, see the [PyTorch documentation](https://pytorch.org/docs/stable/nn.init.html).\n",
    "\n",
    "```python\n",
    "# As used in the original paper:\n",
    "for m in self.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "```\n",
    "\n",
    "### 3.2 Residual Block (used in ResNet-18/34)\n",
    "\n",
    "This is the basic block in smaller ResNets. Each block contains:\n",
    "\n",
    "- Two 3×3 convolutional layers (each followed by BatchNorm and ReLU)\n",
    "- A skip connection that adds the input to the output of the convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce36c7aa7d7314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1  # Output channels = in_channels * expansion\n",
    "    \"\"\"Basic residual block for ResNet18/34\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 stride: int = 1,\n",
    "                 **kwargs):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=3,\n",
    "                               stride=stride, \n",
    "                               padding=1, \n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, \n",
    "                               kernel_size=3,\n",
    "                               stride=1, \n",
    "                               padding=1, \n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        if stride == 1 and in_channels == out_channels:\n",
    "            # Identity shortcut - no transformation needed\n",
    "            self.shortcut = nn.Identity()\n",
    "        else:\n",
    "            # Projection shortcut - transform input dimensions to match output\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, \n",
    "                          kernel_size=1,\n",
    "                          stride=stride, \n",
    "                          bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(identity)  # Skip connection\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae37301e9446de9",
   "metadata": {},
   "source": [
    "### 3.3 Bottleneck Block (used in ResNet-50/101/152)\n",
    "\n",
    "The Bottleneck Block is an alternative building block used in deeper ResNet architectures. The key idea is to reduce computational complexity while maintaining performance through a bottleneck design.\n",
    "\n",
    "#### Structure of the Bottleneck Block\n",
    "\n",
    "The Bottleneck Block contains three convolution layers (see Figure 4):\n",
    "\n",
    "- 1×1 Convolution (Dimensionality Reduction): Reduces the number of channels/feature maps, typically to 1/4 of the input channels\n",
    "- 3×3 Convolution (Spatial Feature Extraction): Processes spatial features with the reduced channel dimension\n",
    "- 1×1 Convolution (Dimensionality Expansion): Restores channel dimension to the desired output size (typically 4× the middle layer)\n",
    "\n",
    "This design creates a \"bottleneck\" in the middle where the representation has fewer channels, significantly reducing computation:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/bottleneck.png\" width=\"500\"/>\n",
    "    <p><i>Figure 4: Comparison of Basic Block vs. Bottleneck Block. Source [1].</i></p>\n",
    "</div>\n",
    "\n",
    "#### Computational Advantage\n",
    "\n",
    "To understand the computational benefit, consider a block with 256 input and output channels:\n",
    "\n",
    "- Basic Block: Two 3×3 convolutions, each with 256 channels\n",
    "    - Computation cost: $2 × (3×3×256×256) = 1,179,648$ operations per spatial location\n",
    "\n",
    "- Bottleneck Block: 1×1 → 3×3 → 1×1 convolutions, with the middle layer having 64 channels\n",
    "    - Computation cost: $(1×1×256×64) + (3×3×64×64) + (1×1×64×256) = 69,632$ operations per spatial location\n",
    "\n",
    "The bottleneck design reduces computation by about 17× while maintaining similar expressiveness!\n",
    "\n",
    "> Why do you think the bottleneck block is not used in ResNet-18/34?\n",
    "\n",
    "#### Expansion Factor\n",
    "The bottleneck block uses an expansion factor (typically 4) to define the relationship between the bottleneck width and output width. This is why we set `expansion = 4` in the implementation, meaning the output channels will be 4× the number of bottleneck channels. \n",
    "In the basic block the expansion factor is 1, meaning the output channels are equal to the input channels, but we leave it in as an attribute for consistency and a simpler implementation of the whole model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299931b0c93ee8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Bottleneck block for ResNet50/101/152\"\"\"\n",
    "    expansion = 4  # Output channels = in_channels * expansion\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 stride: int = 1, \n",
    "                 groups: int = 1, \n",
    "                 base_width: int = 64, \n",
    "                 dilation: int = 1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        width = int(out_channels * (base_width / 64.)) * groups\n",
    "        \n",
    "        # First 1x1 conv - dimensionality reduction\n",
    "        self.conv1 = nn.Conv2d(in_channels, width,\n",
    "                              kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        \n",
    "        # 3x3 conv - spatial feature extraction\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                              groups=groups, dilation=dilation,\n",
    "                              padding=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        \n",
    "        # Second 1x1 conv - dimensionality expansion\n",
    "        self.conv3 = nn.Conv2d(width, out_channels * self.expansion,\n",
    "                              kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if stride == 1 and in_channels == out_channels * self.expansion:\n",
    "            # Identity shortcut - no transformation needed\n",
    "            self.shortcut = nn.Identity()\n",
    "        else:\n",
    "            # Projection shortcut - transform input dimensions to match output\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        # Dimensionality reduction\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Spatial feature extraction\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Dimensionality expansion\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += self.shortcut(identity)  # Skip connection\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bc665bcd96a50",
   "metadata": {},
   "source": [
    "## 3.4 Projection Shortcuts vs. Identity Shortcuts\n",
    "\n",
    "There are three ways to handle shortcuts when dimensions change:\n",
    "- Option A: Zero-padding identity shortcuts (no extra parameters)\n",
    "- Option B: Projection shortcuts only when dimensions change\n",
    "- Option C: Projection shortcuts for all connections\n",
    "\n",
    "Let's examine the trade-offs:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/effect_shortcuts.png\" width=\"800\"/>\n",
    "    <p><i>Figure 5: Effect of Shortcut Types. Values taken from [1].</i></p>\n",
    "</div>\n",
    "\n",
    "[[1](#6-references)] concluded that projections provide small accuracy gains but identity shortcuts are sufficient for addressing the degradation problem and are more memory/computation efficient. \n",
    "\n",
    "> Looking at our code: What type of shortcut is used in our building blocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7e9036d3d98d",
   "metadata": {},
   "source": [
    "## 3.5 Regularization Techniques for Deep CNNs\n",
    "\n",
    "### 3.5.1 Stochastic Depth with DropPath ([Huang et al., 2016](https://arxiv.org/abs/1603.09382))\n",
    "\n",
    "DropPath is a regularization technique specifically designed for very deep residual networks. The core idea is beautifully simple: during training, randomly drop entire layers (by skipping the residual branch) with some probability. During inference, use the full network.\n",
    "\n",
    "#### How DropPath works\n",
    "\n",
    "1. During training, for each batch, randomly dropping the entire residual path (the non-identity branch) of certain blocks with a probability `p`\n",
    "2. When a path is dropped, the input simply passes through the identity connection\n",
    "3. When kept, the output is scaled by `1/(1-p)` to maintain the expected value of the output\n",
    "4. During inference (evaluation mode), no paths are dropped\n",
    "\n",
    "Here's the core implementation taken from [timm (torch image models)](https://github.com/huggingface/pytorch-image-models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e05624ea955248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to rwightman's timm package\n",
    "# github.com:rwightman/pytorch-image-models\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607da9151357493",
   "metadata": {},
   "source": [
    "\n",
    "#### Why it works\n",
    "\n",
    "Stochastic Depth improves deep network training in several ways:\n",
    "\n",
    "1. **Implicit Network Ensemble**: By randomly dropping different layers, the network effectively becomes an implicit ensemble of networks with varying depths, contributing to better generalization.\n",
    "\n",
    "2. **Reducing Vanishing Gradients**: With some layers dropped, gradients have shorter paths to flow back during backpropagation, which helps mitigate the vanishing gradient problem.\n",
    "\n",
    "3. **Improved Information Flow**: By sometimes skipping residual blocks, the network can maintain better information flow from earlier layers to later ones.\n",
    "\n",
    "4. **Regularization Effect**: The randomness introduced during training prevents the network from relying too heavily on specific layers, forcing it to learn more robust features.\n",
    "\n",
    "#### Implementation Pattern\n",
    "\n",
    "A common implementation pattern is to gradually increase the drop probability for deeper layers:\n",
    "\n",
    "- Earlier layers are dropped with lower probability (or not at all)\n",
    "- Deeper layers are dropped with higher probability\n",
    "\n",
    "This makes intuitive sense: earlier layers extract more fundamental features that shouldn't be dropped as often, while deeper layers focus on more specialized feature refinement.\n",
    "\n",
    "With the standard linear scaling approach, if `p` is the maximum drop probability:\n",
    "\n",
    "```\n",
    "drop_rate_for_block[i] = p * i / (total_blocks - 1)\n",
    "```\n",
    "\n",
    "\n",
    "### 3.5.2 DropBlock ([Ghiasi et al., 2018](https://proceedings.neurips.cc/paper/2018/hash/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Abstract.html))\n",
    "\n",
    "Standard dropout randomly drops individual activations during training, which works well for fully connected layers. However, it's less effective for convolutional layers due to the inherent spatial correlation within feature maps:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/dropblock.png\" width=\"750\"/>\n",
    "    <p><i>Figure 6: (a) Input image to a CNN. The green regions in (b) and (c) show activations containing semantic information. (b) Standard dropout randomly removes individual activations, but nearby units still preserve semantic information. (c) DropBlock drops contiguous regions, forcing the network to use other features for classification. Source: Ghiasi et al. (2018)</i></p>\n",
    "</div>\n",
    "\n",
    "As illustrated above, in convolutional networks:\n",
    "- Features are spatially correlated - adjacent units in a feature map contain similar information\n",
    "- Dropping random individual activations isn't effective because neighboring activations can still propagate the same information\n",
    "- The network doesn't learn to rely on diverse features because the same semantic information can flow through multiple pathways\n",
    "\n",
    "#### How it works\n",
    "\n",
    "DropBlock addresses this problem by dropping entire contiguous regions of feature maps, rather than individual units:\n",
    "\n",
    "1. **Block-wise dropping**: Instead of dropping individual activations, DropBlock zeros out square regions of activations\n",
    "2. **Contiguous regions**: By removing spatially contiguous areas, entire semantic concepts are blocked\n",
    "3. **Feature diversification**: The network is forced to learn from other regions and features\n",
    "\n",
    "The method has two main parameters:\n",
    "- `block_size`: The size of the square blocks to drop (e.g., 7×7)\n",
    "- `drop_prob`: The probability of dropping a feature unit\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Below is a PyTorch implementation of DropBlock that you can be used in your models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98808892d045e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DropBlock2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements DropBlock2D: structured form of dropout for convolutional layers. \n",
    "    \n",
    "    :param drop_prob: Probability of activations to be dropped.\n",
    "    :param block_size: Size of the blocks to drop (quadratic).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 drop_prob: float, \n",
    "                 block_size: int):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # If not in training phase or drop probability is 0, return input\n",
    "        if not self.training or self.drop_prob == 0:\n",
    "            return x\n",
    "                \n",
    "        assert x.dim() == 4 # shape: (B, C, H, W)\n",
    "        # Get dimensions\n",
    "        _, _, height, width = x.shape\n",
    "        \n",
    "        # Calculate gamma (sampling rate)\n",
    "        # Equation (1) from the paper\n",
    "        gamma = self.drop_prob / (self.block_size ** 2) * (\n",
    "            (height * width) / ((height - self.block_size + 1) * (width - self.block_size + 1))\n",
    "        )\n",
    "        \n",
    "        # sample mask from Bernoulli distribution \n",
    "        mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "        mask = mask.to(x.device)\n",
    "        \n",
    "        # compute block mask and apply it\n",
    "        block_mask = self._compute_block_mask(mask)\n",
    "        out = x * block_mask[:, None, :, :]\n",
    "\n",
    "        # Normalize by keeping the sum of the input the same\n",
    "        out = out * (block_mask.numel() / (block_mask.sum() + 1e-8))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _compute_block_mask(self, mask):\n",
    "        # https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html\n",
    "        block_mask = F.max_pool2d(\n",
    "            input=mask[:, None, :, :],\n",
    "            kernel_size=(self.block_size, self.block_size),\n",
    "            stride=(1, 1),\n",
    "            padding=self.block_size // 2\n",
    "        )\n",
    "        \n",
    "        # if block size is even, trim the last row and column due to padding\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "        \n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "        \n",
    "        return block_mask",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62639b8b3763a7",
   "metadata": {},
   "source": [
    "#### Tips\n",
    "\n",
    "- Like DropPath DropBlock is most effective in the later layers of the network where features are more semantic.\n",
    "- Choose appropriate block_size:\n",
    "    - For feature maps with high resolution (early layers): smaller block_size (e.g., 3×3)\n",
    "    - For feature maps with low resolution (later layers): larger block_size (e.g., 5×5 or 7×7)\n",
    "- Schedule the drop probability: Start with a low probability and gradually increase it during training\n",
    "\n",
    "\n",
    "### 3.4.3 DropPath vs. DropBlock: Complementary Approaches\n",
    "\n",
    "| Aspect                        | DropPath                                                          | DropBlock                                         |\n",
    "|-------------------------------|-------------------------------------------------------------------|---------------------------------------------------|\n",
    "| **What is dropped**           | Entire residual branches                                          | Contiguous blocks in feature maps                 |\n",
    "| **Applicability**             | Networks with branching paths (e.g., ResNets, Vision Transformer) | Any convolutional layer                           |\n",
    "| **Main benefit**              | Creates implicit ensemble of networks with varying depths         | Forces learning of spatially distributed features |\n",
    "| **Level of operation**        |  Network architecture level                                        |Feature map level                                 |\n",
    "\n",
    "**Key differences and complementary benefits**:\n",
    "\n",
    "1. **Scope of operation**: \n",
    "   - DropPath works at the architectural level by disabling entire branches\n",
    "   - DropBlock works at the feature map level by masking contiguous regions\n",
    "\n",
    "2. **Feature diversity**:\n",
    "   - DropPath encourages the network to not rely too heavily on any particular residual path\n",
    "   - DropBlock encourages the network to use diverse spatial features within a layer\n",
    "\n",
    "3. **Combining both techniques**:\n",
    "   - DropPath helps with network-level robustness\n",
    "   - DropBlock helps with feature-level robustness\n",
    "   - Together, they provide complementary regularization effects\n",
    "\n",
    "\n",
    "> Before you implement either DropPath or DropBlock in your ResNet, you should first run the code without them to see how the model performs. This will give you a baseline to compare against when you add these techniques. After that, you can experiment with different configurations of DropPath and DropBlock to see how they affect the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191c5ed1f3c04b5",
   "metadata": {},
   "source": [
    "## 4. ResNet implementation\n",
    "\n",
    "Here we bring it all together. Note that we use the `ResidualBlock` and `Bottleneck` classes defined above to create the ResNet architecture without `DropPath` or `DropBlock` (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f8640e0530f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 block: Type[Union[ResidualBlock, Bottleneck]],  # If you define another Block architecture eg. with DropPath or DropBlock insert them here\n",
    "                 layers: List[int],  # number of blocks in each layer\n",
    "                 num_classes: int = 1000,  # if <= 0 head will be identity\n",
    "                 in_channels: int = 3,  # input channels (RGB or grayscale)\n",
    "                 zero_init_residual: bool = False,  # zero-initialize the last BN in each residual branch\n",
    "                 groups: int = 1,  # number of groups for group convolution\n",
    "                 width_per_group: int = 64,  # width of each group\n",
    "                 replace_stride_with_dilation: Optional[List[bool]] = None, \n",
    "                 return_features: bool = False):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.return_features = return_features\n",
    "        self.in_channels = 64\n",
    "        self.dilation = 1\n",
    "        \n",
    "        if replace_stride_with_dilation is None:\n",
    "            # Create a list of False values with same length as layers\n",
    "            replace_stride_with_dilation = [False] * len(layers)\n",
    "        else:\n",
    "            assert len(replace_stride_with_dilation) == len(layers), \\\n",
    "                f\"replace_stride_with_dilation should be of length {len(layers)}, but has length {len(replace_stride_with_dilation)}\"\n",
    "            \n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        # Initial layers - can be adjusted based on input size\n",
    "        if in_channels == 1:  # grayscale\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, \n",
    "                                   kernel_size=3, \n",
    "                                   stride=1, \n",
    "                                   padding=1, \n",
    "                                   bias=False)\n",
    "        else:  # RGB - For larger inputs like Imagenette\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, \n",
    "                                   kernel_size=7, \n",
    "                                   stride=2, \n",
    "                                   padding=3, \n",
    "                                   bias=False)\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        if in_channels != 1:  # For larger inputs, add maxpool\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.maxpool = nn.Identity()  # No maxpool for small inputs\n",
    "        \n",
    "        # Define channel sizes for each layer\n",
    "        channels = [64, 128, 256, 512, 1024, 2048]  # Support for more layers if needed\n",
    "        \n",
    "        # Create ResNet layers using ModuleList for flexibility\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i, num_blocks in enumerate(layers):\n",
    "            # Only apply stride=2 from the second layer onwards\n",
    "            stride = 1 if i == 0 else 2\n",
    "            \n",
    "            layer = self._make_layer(\n",
    "                block=block, \n",
    "                out_channels=channels[i], \n",
    "                num_blocks=num_blocks, \n",
    "                stride=stride,\n",
    "                dilate=replace_stride_with_dilation[i]\n",
    "            )\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate final feature dimension based on architecture\n",
    "        final_dim = channels[len(layers)-1] * block.expansion\n",
    "            \n",
    "        # Classifier head. If num_classes is 0, return features only to use it as a bockbone\n",
    "        self.head = nn.Linear(final_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        # Zero-initialize the last BN in each residual branch\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, ResidualBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, \n",
    "                    block: Type[Union[ResidualBlock, Bottleneck]],\n",
    "                    out_channels: int, \n",
    "                    num_blocks: int, \n",
    "                    stride: int = 1, \n",
    "                    dilate: bool = False) -> nn.Sequential:\n",
    "        previous_dilation = self.dilation\n",
    "        \n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "            \n",
    "        layers = []\n",
    "        \n",
    "        # First block may have stride > 1\n",
    "        layers.append(block(\n",
    "            in_channels=self.in_channels, \n",
    "            out_channels=out_channels, \n",
    "            stride=stride, \n",
    "            groups=self.groups,\n",
    "            base_width=self.base_width, \n",
    "            dilation=previous_dilation\n",
    "        ))\n",
    "        \n",
    "        # Update in_channels for subsequent blocks\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        # Remaining blocks\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(\n",
    "                in_channels=self.in_channels, \n",
    "                out_channels=out_channels, \n",
    "                groups=self.groups,\n",
    "                base_width=self.base_width, \n",
    "                dilation=self.dilation\n",
    "            ))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Apply all ResNet layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        features = torch.flatten(x, 1)\n",
    "        logits = self.head(features)\n",
    "        \n",
    "        if self.return_features:\n",
    "            return logits, features\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228f8e7f59f2545",
   "metadata": {},
   "source": [
    "### 4.1 Creating ResNet Models\n",
    "\n",
    "Here we define a function to create ResNet models with different configurations so you don't have to do it manually below. You can specify the model type (e.g., `resnet18`, `resnet50`, etc.) or provide custom parameters for the number of blocks and channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086d1641e186bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create ResNet models with different configurations\n",
    "def create_resnet(model_type: str = None, \n",
    "                  block_type: str = 'basic', \n",
    "                  num_blocks: List[int] = None, \n",
    "                  num_classes: int = 1000,\n",
    "                  in_channels: int = 3, \n",
    "                  return_features: bool = False, \n",
    "                  **kwargs) -> ResNet:\n",
    "    # Predefined configurations | Add more if you create more block types\n",
    "    configs = {\n",
    "        'resnet18': (ResidualBlock, [2, 2, 2, 2]),\n",
    "        'resnet34': (ResidualBlock, [3, 4, 6, 3]),\n",
    "        'resnet50': (Bottleneck, [3, 4, 6, 3]),\n",
    "        'resnet101': (Bottleneck, [3, 4, 23, 3]),\n",
    "        'resnet152': (Bottleneck, [3, 8, 36, 3]),\n",
    "        # Smaller ResNets for smaller datasets (they are not in the original paper)\n",
    "        'resnet20': (ResidualBlock, [3, 3, 3]),\n",
    "        'resnet32': (ResidualBlock, [5, 5, 5]),\n",
    "        'resnet44': (ResidualBlock, [7, 7, 7]),\n",
    "        'resnet56': (ResidualBlock, [9, 9, 9]),\n",
    "        'resnet_mnist': (ResidualBlock, [2, 2, 2]),\n",
    "    }\n",
    "    \n",
    "    # If model_type is provided, use predefined configuration\n",
    "    if model_type is not None:\n",
    "        if model_type not in configs:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}. \"\n",
    "                           f\"Available types: {list(configs.keys())}\")\n",
    "        block, layers = configs[model_type]\n",
    "    \n",
    "    # Otherwise, use custom configuration\n",
    "    else:\n",
    "        if num_blocks is None:\n",
    "            raise ValueError(\"Either model_type or num_blocks must be provided\")\n",
    "        \n",
    "        if block_type.lower() == 'basic':\n",
    "            block = ResidualBlock\n",
    "        elif block_type.lower() == 'bottleneck':\n",
    "            block = Bottleneck\n",
    "        else:\n",
    "            raise ValueError(\"block_type must be 'basic' or 'bottleneck'\")\n",
    "        \n",
    "        layers = num_blocks\n",
    "    \n",
    "    return ResNet(block, layers, \n",
    "                  num_classes=num_classes, \n",
    "                  in_channels=in_channels,\n",
    "                  return_features=return_features,  # For returning the embeddings before classifier\n",
    "                  **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428143151753dd1",
   "metadata": {},
   "source": [
    "### 4.2 Channel Width and Feature Map Progression\n",
    "\n",
    "In the standard ResNet architecture [[2](#6-references)], the number of feature channels follows a specific progression through the network stages:\n",
    "\n",
    "1. **Initial Convolution**: Starts with 64 channels\n",
    "2. **Four Stages**: Each subsequent stage doubles the number of channels\n",
    "   - Stage 1: 64 channels (or 64 × block.expansion for bottleneck blocks)\n",
    "   - Stage 2: 128 channels (or 128 × block.expansion)\n",
    "   - Stage 3: 256 channels (or 256 × block.expansion)\n",
    "   - Stage 4: 512 channels (or 512 × block.expansion)\n",
    "\n",
    "For standard ResNets with basic blocks (ResNet-18/34), the `expansion` factor is 1, resulting in 64, 128, 256, and 512 channels. For bottleneck architectures (ResNet-50/101/152), the `expansion` factor is 4, resulting in 256, 512, 1024, and 2048 channels at the output of each stage.\n",
    "\n",
    "This doubling of channels is coordinated with spatial downsampling (via strided convolutions), which halves the feature map resolution between stages (hence bottleneck). This design maintains a roughly consistent computational load across stages while increasing representational capacity.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/resnet_channels.png\" width=\"500\"/>\n",
    "    <p><i>Figure 7: Channel width progression in ResNet with layers = [2, 2, 2]</i></p>\n",
    "</div>\n",
    "\n",
    "#### 4.2.1 ResNet Variants with Modified Channel Widths\n",
    "\n",
    "Several ResNet variants modify the standard channel progression to achieve specific goals:\n",
    "\n",
    "1. **Wide ResNet**: Increases the base width of the network by using a wider channel multiplier\n",
    "   - For example, Wide-ResNet-50-2 doubles the internal width of bottleneck blocks\n",
    "   - The outer channels remain the same (256, 512, 1024, 2048), but the internal 3×3 convolution width increases (128→256, 256→512, etc.)\n",
    "   - This increases model capacity while maintaining the same depth\n",
    "\n",
    "2. **ResNeXt**: Introduces grouped convolutions with expanded width\n",
    "   - ResNeXt models are parameterized by their cardinality (number of groups) and width\n",
    "   - For example, ResNeXt-50 32×4d has 32 groups and base width of 4d\n",
    "   - This enables wider networks with controlled parameter growth\n",
    "\n",
    "3. **EfficientNet** (see later section): Some adaptations use channel width multipliers to scale the entire network\n",
    "   - These typically scale all channel dimensions by a constant factor\n",
    "   - Helps balance model size and computational requirements\n",
    "\n",
    "In the torchvision implementation, channel width modification is controlled by two parameters:\n",
    "- `groups`: Controls the number of groups in 3×3 convolutions (default=1 for standard ResNets)\n",
    "- `width_per_group`: Base width for each group (default=64 for standard ResNets)\n",
    "\n",
    "The effective width of the internal bottleneck is then calculated as:\n",
    "```python\n",
    "width = int(planes * (base_width / 64.0)) * groups\n",
    "```\n",
    "\n",
    "For standard ResNets, this formula simplifies to `width = planes` (since base_width=64 and groups=1). For Wide ResNet-50-2, it becomes `width = planes * 2`, and for ResNeXt-50 32×4d, it's `width = 4 * 32 = 128`.\n",
    "\n",
    "### 4.3 Loading Pre-trained ResNet Models\n",
    "\n",
    "Torchvision provides pre-trained ResNet models that can be easily loaded and used:\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pretrained ResNet model (you can choose from several variants)\n",
    "# Common options: resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "pretrained_resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Print the model architecture to see the structure\n",
    "print(\"Model structure summary:\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in pretrained_resnet.parameters())}\")\n",
    "print(\"First few layers:\")\n",
    "for name, module in list(pretrained_resnet.named_children())[:4]:\n",
    "    print(f\"{name}: {module}\")\n",
    "```\n",
    "\n",
    "You can also load a pre-trained models from `timm`, which is a popular library for image models. We discussed it in the [DropPath section](#3-implementation-of-resnet-building-blocks). Also you can download different models from [huggingface.co/models](https://huggingface.co/models) or directly from [github](https://github.com/KaimingHe/deep-residual-networks) if the authors provide it.\n",
    "Although there are guidelines how to call different attributes or instances of the model, the best way is to check the documentation of the library you are using or to print them out like above to assure that you reference the right part (for example `head` for the classifier).\n",
    "\n",
    "### 4.4 Transfer Learning with ResNet\n",
    "\n",
    "ResNet is commonly used as a backbone for transfer learning in computer vision tasks (see YOLO notebook). Here's how you can use a pre-trained ResNet for a new classification task:\n",
    "\n",
    "```python\n",
    "def create_transfer_model(num_classes):\n",
    "    # Load pre-trained ResNet\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Freeze all the parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace the last fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "> As mentioned above: Please check if the instances you are using are the right ones. For example, in the code above we use `model.fc` to access the classifier head, but in other libraries it might be called `head` (as in our implementation), `classifier` or something else entirely. Sometimes the model is also wrapped in a other module, so you have to access it like this `model.module.fc`! `print(model)` is your friend here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0453071bfe09782",
   "metadata": {},
   "source": [
    "## 5. Beyond ResNet: Efficient Model Scaling\n",
    "\n",
    "After exploring ResNet architecture and its implementation, let's look at a more recent advancement in convolutional neural networks: **EfficientNet**. Introduced in 2019 by Tan and Le [[3](#6-references)], EfficientNet addresses a key question in neural network design:\n",
    "\n",
    "> How do we effectively scale models to achieve better accuracy with limited computational resources?\n",
    "\n",
    "ResNet showed that deeper networks can be successfully trained through residual connections. However, EfficientNet demonstrates that we need to carefully balance **depth**, **width**, and **resolution** to achieve optimal performance.\n",
    "\n",
    "### The Problem with Traditional Scaling Methods\n",
    "\n",
    "Traditional methods for scaling neural networks typically focus on one dimension:\n",
    "\n",
    "1. **Depth Scaling**: Adding more layers (like going from ResNet-18 to ResNet-50, ResNet-101, etc.)\n",
    "2. **Width Scaling**: Increasing the number of channels in each layer\n",
    "3. **Resolution Scaling**: Using higher resolution input images\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/scaling.PNG\" width=\"900\"/>\n",
    "    <p><i>Figure 8: Model scaling methods. Source: [3]</i></p>\n",
    "</div>\n",
    "\n",
    "While each approach improves accuracy, scaling any single dimension quickly reaches diminishing returns. The key insight of EfficientNet was that these dimensions are not independent\n",
    "\n",
    "### Compound Scaling: The Key Insight of EfficientNet\n",
    "\n",
    "EfficientNet's major contribution is **compound scaling**, which uniformly scales on these dimensions:\n",
    "\n",
    "- Depth: $d = \\alpha^\\phi$\n",
    "- Width: $w = \\beta^\\phi$\n",
    "- Resolution: $r = \\gamma^\\phi$\n",
    "\n",
    "Where $\\alpha$, $\\beta$, $\\gamma$ are constants determined by a small grid search, and $\\phi$ is the compound coefficient that controls the overall scaling.\n",
    "\n",
    "The intuition behind compound scaling is straightforward:\n",
    "\n",
    "- If input resolution increases, we need more layers (depth) to increase the receptive field\n",
    "- With higher resolution and more layers, we need more channels (width) to capture fine-grained patterns\n",
    "- These dimensions should be balanced together rather than scaled independently\n",
    "\n",
    "### ResNet vs. EfficientNet\n",
    "\n",
    "Compared to the ResNet-50, the EfficientNet-B4 utilize similar FLOPs, yet the EfficientNet-B4 improves the top-1 accuracy on ImageNet from 76% of the ResNet to around 83% (see Figure 9). This suggest not only better performance in terms of accuracy but also in computational efficiency to CNNs. Please consider this for your own UTKFace age task.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/effnet_resnet.png\" width=\"500\"/>\n",
    "    <p><i>Figure 9: FLOPS vs. ImageNet Accuracy with different architectures. Source: [3]</i></p>\n",
    "</div>\n",
    "\n",
    "You can use pre-trained EfficientNets from [`timm`](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/efficientnet.py).\n",
    "\n",
    "```python \n",
    "\n",
    "import timm\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Load a pretrained EfficientNet-B0 model\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "# If you want to modify the classifier for a different number of classes\n",
    "# model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
    "\n",
    "# For evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Or adjust the learning rate of your backbone \n",
    "lr_params = [\n",
    "    {'params': model.heads.parameters(), 'lr': base_lr},\n",
    "    {'params': model.encoder.parameters(), 'lr': backbone_lr}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.SGD(lr_params, weight_decay=0.01, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9e9ec65ae26da",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "[1] [He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)\n",
    "\n",
    "[2] [torchvision/models/resnet.py](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)\n",
    "\n",
    "[3] [Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR.](https://arxiv.org/pdf/1905.11946.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a574da6158415cd",
   "metadata": {},
   "source": [
    "# Training and evaluation of a ResNet on the Imagenette\n",
    "\n",
    "So let's get to action and train a ResNet on the Imagenette dataset. We will use the `create_resnet` function to create a ResNet-18 model and train it on the Imagenette dataset. Depending on your hardware, you can choose to train larger ResNet model (e.g., ResNet-34). But first, let's load the dataset and define the training and evaluation functions with the image augmentation and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa5a11b34c2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "from Utils.dataloaders import prepare_imagenette\n",
    "\n",
    "# define hyperparameters\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "transform_augm = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    # Core transformations\n",
    "    v2.RandomResizedCrop(size=224, scale=(0.75, 1.0), ratio=(0.9, 1.05)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),  # People can face either direction\n",
    "    v2.RandomRotation(degrees=(-10, 10)),  # Small rotations\n",
    "    \n",
    "    # Lighting and appearance variations\n",
    "    v2.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.05),\n",
    "    v2.RandomAutocontrast(p=0.2),\n",
    "    \n",
    "    # Occasional realistic variations - with proper probability handling\n",
    "    v2.RandomApply([v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),\n",
    "    v2.RandomPerspective(distortion_scale=0.15, p=0.3),\n",
    "    v2.RandomErasing(p=0.1, scale=(0.02, 0.08), ratio=(0.3, 3.3)),\n",
    "    \n",
    "    # Normalization\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_norm = transforms.Compose(\n",
    "[    v2.ToImage(),\n",
    "     v2.ToDtype(torch.float32, scale=True),\n",
    "     v2.Resize(size=(224,224)),\n",
    "     v2.Normalize(mean = [0.485, 0.456, 0.406], std=[0.229,0.224,0.225]) , \n",
    "])\n",
    "\n",
    "# Load the Imagenette dataset\n",
    "train_loader, val_loader, classes = prepare_imagenette(train_compose=transform_augm, \n",
    "                                                       test_compose=transform_norm, \n",
    "                                                       save_path='../Dataset/',\n",
    "                                                       batch_size=batch_size, \n",
    "                                                       num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d7d270b3772a6",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd75243746048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_type = 'resnet18'\n",
    "num_classes = len(classes)\n",
    "resnet_model = create_resnet(model_type=resnet_type, \n",
    "                             num_classes=num_classes, \n",
    "                             in_channels=3, \n",
    "                             return_features=True)  # We will later visualize the features\n",
    "\n",
    "# model summary\n",
    "print(resnet_model)\n",
    "\n",
    "# learnable parameters\n",
    "from Utils.little_helpers import get_parameters\n",
    "\n",
    "print(f\"Number of trainable parameters: {get_parameters(resnet_model):.3f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c234817cf2dd52",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168dc7f72841e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our optimizer and loss function. Normally ResNets are trained with SGD, but Adam is also a good choice.\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "init_lr = 1e-2\n",
    "w_decay = 1e-3\n",
    "optimizer = optim.SGD(resnet_model.parameters(), lr=init_lr, momentum=0.9, weight_decay=w_decay)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)  # Reduce lr by a factor `gamma` every `step_size` epochs\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "# Training loop\n",
    "from Utils.functions import train_model\n",
    "\n",
    "results_folder = 'resnet_model/'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "with timer(\"Training process\"):\n",
    "    history = train_model(model=resnet_model, \n",
    "                          train_loader=train_loader, \n",
    "                          val_loader=val_loader, \n",
    "                          criterion=loss_fn,\n",
    "                          optimizer=optimizer,\n",
    "                          scheduler=scheduler,\n",
    "                          device=device,\n",
    "                          num_epochs=num_epochs,\n",
    "                          checkpoint_path=results_folder,\n",
    "                          patience=5)\n",
    "    \n",
    "torch.save(resnet_model.state_dict(), f'{results_folder}{resnet_type}_dict.pth')\n",
    "\n",
    "# save history + predictions\n",
    "np.save(f'{results_folder}history.npy', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0247f548067935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plotting import visualize_training_results\n",
    "\n",
    "visualize_training_results(train_losses=history['train_loss'],\n",
    "                           train_accs=history['train_acc'],\n",
    "                           test_losses=history['val_loss'],\n",
    "                           test_accs=history['val_acc'],\n",
    "                           output_dir=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e627d6d23a91425",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db2e2dd291fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functions import test_model\n",
    "from Utils.plotting import visualize_test_results\n",
    "\n",
    "# evaluate model on validation set\n",
    "with timer(\"Evaluating process\"):\n",
    "    aggregate_df, per_image_df, overall_accuracy, embeddings = test_model(model=resnet_model,\n",
    "                                                                          test_loader=val_loader,\n",
    "                                                                          device=device,\n",
    "                                                                          class_names=classes,\n",
    "                                                                          print_per_class_summary=True,\n",
    "                                                                          collect_embeddings=True,)\n",
    "\n",
    "# save dataframes as parquet (requires pyarrow and fastparquet)\n",
    "try:\n",
    "    aggregate_df.to_parquet(os.path.join(results_folder, 'aggregate_df.parquet'))\n",
    "    per_image_df.to_parquet(os.path.join(results_folder, 'per_image_df.parquet'))\n",
    "except ImportError:\n",
    "    aggregate_df.to_pickle(os.path.join(results_folder, 'aggregate_df.pkl'))\n",
    "    per_image_df.to_pickle(os.path.join(results_folder, 'per_image_df.pkl'))\n",
    "    \n",
    "visualize_test_results(aggregate_df=aggregate_df,\n",
    "                       per_image_df=per_image_df,\n",
    "                       overall_accuracy=overall_accuracy,\n",
    "                       output_dir=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede2ae1599d9a2c",
   "metadata": {},
   "source": [
    "### Visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887a327d3d0c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plotting import visualize_embeddings_tsne, visualize_embeddings_pca\n",
    "\n",
    "# Visualize the embeddings using t-SNE\n",
    "visualize_embeddings_tsne(embeddings=embeddings['all_embeddings'],\n",
    "                          labels=embeddings['all_labels'],\n",
    "                          class_names=classes,\n",
    "                          output_dir=None)\n",
    "\n",
    "# Visualize the embeddings using PCA\n",
    "visualize_embeddings_pca(embeddings=embeddings['all_embeddings'],\n",
    "                          labels=embeddings['all_labels'],\n",
    "                          class_names=classes,\n",
    "                          output_dir=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956093e816993de5",
   "metadata": {},
   "source": [
    "## 1. Exercise: Implement DropPath and DropBlock in ResNet\n",
    "\n",
    "As mentioned in the text, our implementation of ResNet does not include any regularization techniques like DropPath or DropBlock. Your task is to implement these techniques in the ResNet architecture. You can choose to implement either one or both of them. Choose either the `ResidualBlock` or `Bottleneck` class to implement them. You can also use the `DropPath` and `DropBlock` classes provided above. Make sure to test your implementation on the Imagenette dataset and compare the results with the baseline ResNet model **without** these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f0e92d3129bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace the following code with your implementation of DropPath and DropBlock in the Bottleneck or ResidualBlock class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4328c2fbeb8ab6",
   "metadata": {},
   "source": [
    "## 2. Exercise: Experiment with a pretrained ResNet model\n",
    "\n",
    "As mentioned in the text, you can use a pretrained ResNet model from torchvision or timm. Your task is to load a pretrained ResNet model and fine-tune it on the Imagenette dataset. You can choose to freeze some layers of the model or train all layers best. Compare the results with the baseline ResNet model and the one with DropPath or DropBlock.\n",
    "\n",
    "> Note that you can add **Layer-specific learning rates**: The more semantic the information is (later layers), the higher the learning rate should be. Early layers capture generic features like edges and textures, while later layers capture more dataset-specific features.\n",
    "\n",
    "```python\n",
    "# Define different learning rates for different parts of the network (for a pretrained torchvision model)\n",
    "# Lower learning rate for early layers (backbone)\n",
    "backbone_lr = 1e-5\n",
    "# Medium learning rate for middle layers\n",
    "middle_lr = 5e-5\n",
    "# Higher learning rate for the classifier (final layers)\n",
    "head_lr = 1e-3\n",
    "\n",
    "# Group parameters by their position in the network (example)\n",
    "param_groups = [\n",
    "    # Layer1 (early features) - lowest learning rate\n",
    "    {'params': model.layer1.parameters(), 'lr': backbone_lr},\n",
    "    # Layer2 (early-mid features)\n",
    "    {'params': model.layer2.parameters(), 'lr': backbone_lr * 2},\n",
    "    # Layer3 (mid features)\n",
    "    {'params': model.layer3.parameters(), 'lr': middle_lr},\n",
    "    # Layer4 (semantic features)\n",
    "    {'params': model.layer4.parameters(), 'lr': middle_lr * 2},\n",
    "    # Classifier (task-specific) - highest learning rate\n",
    "    {'params': model.fc.parameters(), 'lr': head_lr}\n",
    "]\n",
    "\n",
    "optimizer = optim.SGD(param_groups, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346adbb853027774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
