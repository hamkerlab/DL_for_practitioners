{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cacc58cd232e091",
   "metadata": {},
   "source": [
    "# Tutorial 4.1: Vision Transformer (ViT)\n",
    "\n",
    "Author: [Erik Syniawa](mailto:erik.syniawa@informatik.tu-chemnitz.de)\n",
    "\n",
    "Published by Dosovitskiy et al. (2020) in [[1](#6-references)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    print(f\"Added {root_path} to sys.path\")\n",
    "    \n",
    "from Utils.little_helpers import timer, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'PyTorch version: {torch.__version__} running on {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dda8d882cdcbb",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Vision Transformers (ViT) represent a significant shift in computer vision approaches by applying the Transformer architecture - originally designed for NLP - directly to image classification tasks.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/vit_animation.gif\" width=\"700\"/>\n",
    "    <p><i>Figure 1: ViT architecture. Source: [2]</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c87061469ab59",
   "metadata": {},
   "source": [
    "The general architecture of ViT consists of the following components:\n",
    "\n",
    "### 1. Patch Embedding\n",
    "\n",
    "Since Transformers have a quadratic computational complexity due to self-attention, it is important to reduce the input dimensionality via a patch embedding.\n",
    "- The input image is divided into fixed-size patches (e.g., 16x16 pixels)\n",
    "- Each patch is flattened into a vector\n",
    "- A linear or a convolutional projection maps each flattened patch to a __learnable__ lower-dimensional embedding vector\n",
    "- This creates a sequence of patch embeddings that serve as the input to the Transformer\n",
    "\n",
    "\n",
    "Let's have a look how this looks on an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5aef795a2dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load and preprocess image\n",
    "image_url = \"https://images.unsplash.com/photo-1501854140801-50d01698950b\"\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Apply transformations and convert to tensor\n",
    "x = transform(img).unsqueeze(0)  # [1, C, H, W]\n",
    "\n",
    "# Extract patches directly using unfold\n",
    "B, C, H, W = x.shape\n",
    "patches = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "patches = patches.contiguous().view(B, C, -1, patch_size, patch_size)\n",
    "patches = patches.permute(0, 2, 1, 3, 4)  # [B, num_patches, C, patch_size, patch_size]\n",
    "\n",
    "# Reshape for visualization\n",
    "patches_for_grid = patches.squeeze(0)  # [num_patches, C, patch_size, patch_size]\n",
    "\n",
    "# Create grid visualization\n",
    "grid = make_grid(patches_for_grid, nrow=img_size//patch_size, padding=1)\n",
    "grid_np = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Create linear layout (all patches in one row)\n",
    "num_display_patches = 64\n",
    "linear_grid = make_grid(patches_for_grid, nrow=num_display_patches, padding=1)\n",
    "linear_grid_np = linear_grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Display original, grid, and linear layout\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "ax1.imshow(x.squeeze(0).permute(1, 2, 0).numpy())\n",
    "ax1.set_title(f'Original Image ({img_size}×{img_size})')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(grid_np)\n",
    "ax2.set_title(f'Patches in Grid ({patch_size}×{patch_size})')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3.imshow(linear_grid_np)\n",
    "ax3.set_title(f'Patches in Linear Sequence: {num_display_patches} in one row')\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868ebb77e809387",
   "metadata": {},
   "source": [
    "Now we understand how the image is divided into patches. The next step is to convert these patches into a lower-dimensional embedding space via `torch.nn.Module`. This will be done using a convolutional layer with kernel size and stride equal to the patch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c6f01a1118726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" \n",
    "    Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size: int = 224, # 224x224\n",
    "                 patch_size: int = 16,  # 16x16 \n",
    "                 in_chans: int = 3,  # color channels \n",
    "                 embed_dim: int = 768):  # embedding dimension\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        \n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.num_patches_w = self.patch_shape[0]\n",
    "        self.num_patches_h = self.patch_shape[1]\n",
    "        \n",
    "        # Initialize weights with truncated normal from the ViT paper\n",
    "        fan_in = in_chans * patch_size[0] * patch_size[1]\n",
    "        nn.init.trunc_normal_(self.proj.weight, std=np.sqrt(1 / fan_in))\n",
    "        if self.proj.bias is not None:\n",
    "            nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "    def _get_h_w(self):\n",
    "        return self.num_patches_h, self.num_patches_w\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [B, C, H, W]\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == W, f\"Input image must be square, got {H}x{W}\"\n",
    "        \n",
    "        # Apply patch embedding conv\n",
    "        # (B, C, H, W) -> (B, embed_dim, H//patch_size, W//patch_size)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Flatten and transpose: (B, embed_dim, H', W') -> (B, num_patches, embed_dim)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848016e08ee317c",
   "metadata": {},
   "source": [
    "Implementation from [[3](#6-references)].\n",
    "\n",
    "> What would be the computational costs of calculating the self-attention of a 224x224 image with and without patches? Assuming the image is RGB, the embedding dimension is 768 and the patch size is 16x16. \n",
    "\n",
    "\n",
    "> What could be the advantage of using a convolutional layer (like in the code) instead of a linear projection of each patch?\n",
    "\n",
    "#### 1.1 Patch Embedding Filters after training\n",
    "\n",
    "Figure 2 shows the top 28 principal components of the learned patch embedding filters from ViT-L/32. These visualizations reveal:\n",
    "\n",
    "- These filters act as basis functions for representing the fine structure within each patch\n",
    "- They capture different frequency patterns and edge orientations\n",
    "- Some filters detect color variations (RGB channels), while others focus on texture and structural elements\n",
    "- These learned embedding filters transform raw pixel data into meaningful feature representations\n",
    "- The diversity of these filters suggests that the model learns to extract various visual features at the patch level before self-attention processing\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/rgb_filter_embeddings.PNG\" width=\"500\"/>\n",
    "    <p><i>Figure 2: Filters of the patch embedding of RGB values of ViT-L/32 model. Source: [1]</i></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c9be4c9f3c642",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Positional Embedding\n",
    "\n",
    "Since Transformers don't inherently understand spatial relationships, positional information must be added. In contrast to Transformers in NLP, the initial ViT from [[1](#6-references)] uses learnable positional embeddings that are initialized randomly.\n",
    "\n",
    "- 1D positional embeddings are added to the patch embeddings (Patch Embedding + Positional Embedding)\n",
    "- These embeddings encode the 2D position of patches within the original image\n",
    "- The model learns to represent spatial relationships between patches during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75ce0cf6500dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable positional embeddings for the transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_patches: int, \n",
    "                 embed_dim: int, \n",
    "                 dropout: float = 0.1):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize positional embeddings\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Standard initialization strategy from the ViT paper\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x shape: [B, num_patches, embed_dim]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b31540a95e470",
   "metadata": {},
   "source": [
    "Implementation from [[3](#6-references)]. \n",
    "\n",
    "We will implement the positional embedding directly in the ViT class for simplicity. A sidenote on positional embeddings: There are implementations like the DeiT (Data-efficient image Transformers) or some variants of the SwinTransformer that use fixed sinusoidal positional embeddings instead of learnable ones. But this is not the focus of this notebook as we will focus on [[1](#6-references)].\n",
    "\n",
    "#### 2.1 Positional Encoding after training\n",
    "\n",
    "The heatmap of Figure 3 visualizes how similar the learned positional embeddings are to each other after training:\n",
    "\n",
    "- Each tile shows the cosine similarity between one patch positional embedding and all others\n",
    "- You can clearly see that patches close to each other (diagonally, horizontally, or vertically) have more similar embeddings\n",
    "- The grid-like structure that emerges shows that the model has learned 2D spatial relationships even though it uses 1D positional embeddings\n",
    "- This explains why the authors found that using more complex 2D positional embeddings didn't improve performance - the model already learned to represent 2D structure\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/position_embeddings.PNG\" width=\"500\"/>\n",
    "    <p><i>Figure 3: Positional encoding after training. Source: [1]</i></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd62ed5fcc663ca",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Class Token [CLS] ([Devlin et al., 2019](https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email))\n",
    "\n",
    "- Following BERT's approach, a special learnable embedding called the \"cls token\" is prepended to the sequence\n",
    "- This token aggregates information from all patches through self-attention\n",
    "- The final state of this token at the output of the Transformer serves as the image representation\n",
    "- A classification head (MLP) is attached to this token for image classification (see section [5.4](#54-final-architecture))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239903d8c3c55c8",
   "metadata": {},
   "source": [
    "### 4. Layer Normalization ([Ba et al., 2016](https://arxiv.org/abs/1607.06450))\n",
    "\n",
    "Unlike in ResNets where BatchNorm is used, the ViT uses Layer Normalization. \n",
    "Contrary to BatchNorm, which normalizes across the batch dimension, LayerNorm normalizes across the feature dimension for each individual sample.\n",
    "\n",
    "For an input tensor $X \\in \\mathbb{R}^{B \\times d}$ where $B$ is the batch size and $d$ is the feature dimension, LayerNorm operates as follows:\n",
    "\n",
    "1. Compute the mean and variance **across the feature dimension** for each sample in the batch:\n",
    "   - $\\mu_i = \\frac{1}{d} \\sum_{j=1}^{d} x_{i,j}$\n",
    "   - $\\sigma_i^2 = \\frac{1}{d} \\sum_{j=1}^{d} (x_{i,j} - \\mu_i)^2$\n",
    "\n",
    "2. Normalize the features:\n",
    "   - $\\hat{x}_{i,j} = \\frac{x_{i,j} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$\n",
    "\n",
    "3. Apply learnable scale ($\\gamma$) and shift ($\\beta$) parameters:\n",
    "   - $y_{i,j} = \\gamma \\cdot \\hat{x}_{i,j} + \\beta$\n",
    "\n",
    "Where $\\epsilon$ is a small constant added for numerical stability. Let's see how this looks in code:\n",
    "\n",
    "```python\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization module\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_dim: int, \n",
    "                 eps: float = 1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [B, ..., d]\n",
    "        # Calculate mean and variance along the last dimension (features)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        y = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return y\n",
    "```\n",
    "\n",
    "We don't need to implement this in the ViT class, as PyTorch already has a built-in LayerNorm module, which we will use.\n",
    "\n",
    "#### 4.1 Why LayerNorm works better for Transformers\n",
    "\n",
    "As demonstrated in ([Shen et al., 2020](https://proceedings.mlr.press/v119/shen20e.html)), BatchNorm performs poorly in Transformer architectures for several reasons:\n",
    "\n",
    "1. **High Variance in Batch Statistics**: NLP data exhibits much higher variance in batch statistics compared to CV data, which causes instability when using BatchNorm.\n",
    "\n",
    "2. **Variable Sequence Lengths**: NLP tasks often have variable-length inputs, making BatchNorm statistics inconsistent across batches.\n",
    "\n",
    "3. **Token Independence**: LayerNorm treats each token independently, which aligns better with the self-attention mechanism's design.\n",
    "\n",
    "4. **Gradient Flow**: LayerNorm helps recenter and rescale backward gradients, which is particularly important for deep Transformer networks.\n",
    "\n",
    "In ViT, LayerNorm is applied twice in each Transformer block:\n",
    "- Before the Multi-Head Self-Attention (MSA) module\n",
    "- Before the MLP block\n",
    "\n",
    "This \"pre-norm\" configuration helps stabilize training by ensuring that the input to each sub-layer has a consistent distribution, which is crucial for the convergence of deep Transformer networks.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/batchvslayer.png\" width=\"500\"/>\n",
    "    <p><i>Figure 4: Difference between BatchNorm and LayerNorm. The colored sections show the entries on which the statistics are calculated on. Source: Shen et al., 2020</i></p>\n",
    "</div>\n",
    "\n",
    "#### 4.2 Implementation Considerations\n",
    "\n",
    "When implementing LayerNorm in Vision Transformers, several considerations are important:\n",
    "\n",
    "1. **Epsilon Value**: A small constant ($\\epsilon = 10^{-6}$ is common) is added to the variance for numerical stability.\n",
    "\n",
    "2. **Parameter Initialization**: The scale parameter $\\gamma$ is typically initialized to 1, and the shift parameter $\\beta$ to 0.\n",
    "\n",
    "3. **Pre-norm vs Post-norm**: In the original Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), normalization was applied *after* each sub-layer (post-norm). In ViT and many modern Transformer variants, normalization is applied *before* each sub-layer (pre-norm), which improves training stability.\n",
    "\n",
    "4. **Dimensionality**: LayerNorm is applied to the entire embedding dimension in ViT, unlike in some other applications where it might be applied to only a subset of features.\n",
    "\n",
    "#### 4.3 \"Recent\" Advances\n",
    "\n",
    "- **RMSNorm** ([Zhang & Sennrich, 2019](https://arxiv.org/abs/1910.07467)): Simplifies LayerNorm by removing the mean-centering step and only normalizing by the root mean square.\n",
    "\n",
    "- **PowerNorm** ([Shen et al., 2020](https://proceedings.mlr.press/v119/shen20e.html)): Addresses the issues of BatchNorm in Transformers by relaxing zero-mean normalization and using running statistics for the quadratic mean.\n",
    "\n",
    "- **T-Fixup** ([Huang et al., 2020](https://proceedings.mlr.press/v119/huang20f)): An initialization technique that enables training deep Transformers without any normalization layers (and the need for warm up phases).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89278ad0dc7342b8",
   "metadata": {},
   "source": [
    "### 5. Transformer (Encoder-only)\n",
    "\n",
    "The standard Transformer encoder consists of alternating Multi-Head Self-Attention (MSA) and MLP blocks. Each block is preceded by LayerNorm and uses residual and skip connections like in ResNet.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/transformer_block.PNG\" width=\"200\"/>\n",
    "    <p><i>Figure 5: Structure of a Transformer block. Source: [1]</i></p>\n",
    "</div>\n",
    "\n",
    "#### 5.1 Multi-Head Self-Attention (MSA)\n",
    "\n",
    "- Self-attention allows each patch to attend to all other patches, enabling global information integration\n",
    "- We will use the MSA from the PyTorch package, so the code below is just for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c741c05c187ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # Each head gets a fraction of the total embedding dimension\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        # Single linear projection for queries, keys, and values (more efficient)\n",
    "        # Projects from dim → dim*3 (to create queries, keys, and values at once)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        # Dropout applied to attention weights for regularization\n",
    "        self.attn_drop = nn.Dropout(attention_dropout)\n",
    "        # Final projection to mix information from different heads\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        # Dropout applied to the output projection\n",
    "        self.proj_drop = nn.Dropout(projection_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract dimensions from input tensor\n",
    "        B, N, C = x.shape  # Batch size, sequence length, embedding dimension\n",
    "        \n",
    "        # Step 1: Project input to queries, keys, and values all at once\n",
    "        # - Apply linear projection to get [B, N, 3*C] tensor\n",
    "        # - Reshape to [B, N, 3, num_heads, head_dim]\n",
    "        # - Permute to [3, B, num_heads, N, head_dim] to separate q, k, v\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # Split into separate q, k, v tensors, each with shape [B, num_heads, N, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Step 2: Compute attention scores (dot product of queries and keys)\n",
    "        # - Transpose keys to align dimensions for matrix multiplication\n",
    "        # - Scale to prevent extremely small gradients in softmax\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [B, num_heads, N, N]\n",
    "        \n",
    "        # Step 3: Apply softmax to get attention probabilities\n",
    "        # Each row sums to 1, representing probability distribution over the sequence\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Step 4: Apply dropout for regularization\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Step 5: Apply attention weights to values\n",
    "        # - Matrix multiply attention weights with values\n",
    "        # - Transpose and reshape back to original dimensions\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        # Step 6: Final projection and dropout\n",
    "        # This mixes information from different heads\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffea9a616941bf",
   "metadata": {},
   "source": [
    "The key feature is that the attention computation is split across multiple \"heads\". Each head can focus on different parts of the sequence simultaneously.\n",
    "This allows the model to capture various types of relationships in parallel (see section [5.6](#56-preview)):\n",
    "- Some heads might focus on local patterns\n",
    "- Others might capture long-range dependencies\n",
    "- Others might specialize in specific linguistic or semantic features\n",
    "\n",
    "The $num_{heads}$ parameter determines how many parallel attention mechanisms to use. The $head_{dim} = dim / num_{heads}$ determines the dimension of each attention head.\n",
    "At the end, outputs from all heads are concatenated and projected back to the original dimension.\n",
    "\n",
    "> **Note**: As mentioned above, self-attention has quadratic computational complexity with respect to sequence length. For a $224\\times 224$ image with $16\\times 16$ patches, we have 196 tokens, resulting in $\\sim$ 38K attention operations per layer per head. This becomes a significant bottleneck as image resolution or the number of patches increases.\n",
    "Several approaches can optimize this computation:\n",
    "\n",
    "- **Flash attention**: [Flash Attention](https://arxiv.org/abs/2205.14135) and [Flash Attention-2](https://arxiv.org/abs/2307.08691) significantly speed up attention computation through IO-aware implementation, tiling, and recomputation strategies. \n",
    "\n",
    "- **Linear attention**: Linear complexity approximations of attention have been successfully applied to ViTs, including Performer's [FAVOR+](https://arxiv.org/abs/2009.14794) and [Nyströmformer](https://arxiv.org/abs/2102.03902) (used in ViTGAN).\n",
    "\n",
    "- **Hierarchical designs**: Models like [Swin Transformer](https://arxiv.org/abs/2103.14030) and [PVT](https://arxiv.org/abs/2102.12122) apply hierarchical attention with progressive spatial reduction, combining the benefits of CNNs multiscale feature maps with transformer architecture.\n",
    "\n",
    "- **Sparse attention**: Approaches like [Longformer](https://arxiv.org/abs/2004.05150) and [Reformer](https://arxiv.org/abs/2001.04451) use sparse attention patterns to reduce complexity, allowing for longer sequences without quadratic scaling. Applications in computer vision include [Swin Transformer](https://arxiv.org/abs/2103.14030) and [Linformer](https://arxiv.org/abs/2006.04768).\n",
    "\n",
    "- **Memory-efficient implementations**: Libraries like [xformers](https://github.com/facebookresearch/xformers) provide memory-efficient Multi-Head Attention implementations for PyTorch that reduce both memory footprint and computational requirements. \n",
    "\n",
    "> When implementing your own ViT and struggle with limited computational resources, please consider these optimizations to improve efficiency without sacrificing performance. \n",
    "\n",
    "#### 5.2 MLP\n",
    "\n",
    "The MLP contains two layers with a GeLU non-linearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1d126dcbaa33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP block as used in Vision Transformer with GeLU activation\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_features: int, \n",
    "                 hidden_features: int, \n",
    "                 out_features: int,\n",
    "                 dropout: float = 0.0,\n",
    "                 activation: nn.Module = nn.GELU(),\n",
    "                 weight_init: bool = True):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            activation,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_features, out_features),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        if weight_init:\n",
    "            self._init_weights()\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply MLP block\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # Initialize weights using Xavier uniform distribution (from torchvision vit.py)\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.normal_(layer.bias, std=1e-6)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2951e461cf39",
   "metadata": {},
   "source": [
    "#### 5.3 Encoder Block\n",
    "\n",
    "The encoder block is a combination of MSA and MLP with layer normalization (see Figure 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c249cc1d800d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block consisting of Multi-Head Self-Attention followed by MLP\n",
    "    Using pre-norm architecture from TorchVision\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_dim: int, \n",
    "                 num_heads: int, \n",
    "                 mlp_ratio: float = 4.0, \n",
    "                 dropout: float = 0.0, \n",
    "                 attention_dropout: float = 0.0):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        \n",
    "        # Layer Normalization before self-attention (pre-norm architecture)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Multi-Head Self-Attention using PyTorch's built-in implementation\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=attention_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Dropout after attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Normalization before MLP (pre-norm architecture)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # MLP block\n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=embed_dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            out_features=embed_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "                \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply Layer Norm -> Self-Attention -> Residual\n",
    "        attn_output, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Apply Layer Norm -> MLP -> Residual\n",
    "        x = x + self.mlp(self.norm2(x))  # Note: MLP already includes dropout so we don't need to apply it again\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606818009ca7c7e8",
   "metadata": {},
   "source": [
    "> **Note**: PyTorch's `nn.LayerNorm` initializes its parameters as follows:\n",
    "- The scale parameter weight ($\\gamma$) is initialized to 1.0\n",
    "- The shift parameter bias ($\\beta$) is initialized to 0.0\n",
    "- So we just have to initialize $\\epsilon$ to 1e-6\n",
    "\n",
    "#### 5.4 Final Architecture\n",
    "\n",
    "Now we have to put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfb198bf30f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) model\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size: int = 224, \n",
    "                 patch_size: int = 16, \n",
    "                 in_chans: int = 3, \n",
    "                 num_classes: int = 1000,\n",
    "                 embed_dim: int = 768, \n",
    "                 depth: int = 12, \n",
    "                 num_heads: int = 12, \n",
    "                 mlp_ratio: float = 4.0,  # hidden layer size = embed_dim * mlp_ratio\n",
    "                 dropout: float = 0.0,\n",
    "                 attention_dropout: float = 0.0,):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        # Patch Embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, \n",
    "            patch_size=patch_size, \n",
    "            in_chans=in_chans, \n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.normal_(self.class_token, std=0.02)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Dropout\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                embed_dim=embed_dim, \n",
    "                num_heads=num_heads, \n",
    "                mlp_ratio=mlp_ratio, \n",
    "                dropout=dropout,\n",
    "                attention_dropout=attention_dropout\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Layer Norm before classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification head if valid number of classes is provided\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Convert image to patch embeddings\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Prepend class token\n",
    "        cls_token = self.class_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Return class token representation\n",
    "        return x[:, 0]\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get features from the class token\n",
    "        x = self.forward_features(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        folder_name, _ = os.path.split(path)\n",
    "        if folder_name:\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3114d947e8581af",
   "metadata": {},
   "source": [
    "#### 5.5 Overview \n",
    "\n",
    "There are different variants of the ViT regarding their depth and other hyperparameters. The most common you can find below.\n",
    "\n",
    "\n",
    "| Model     | Patch Size | Embedding Dim | Heads | Blocks | MLP Size | Parameters |\n",
    "|-----------|------------|---------------|-------|--------|----------|------------|\n",
    "| ViT-S/8   | 8×8        | 384           | 6     | 12     | 1536     | 21M        |\n",
    "| ViT-S/16  | 16×16      | 384           | 6     | 12     | 1536     | 21M        |\n",
    "| ViT-S/32  | 32×32      | 384           | 6     | 12     | 1536     | 21M        |\n",
    "| ViT-B/8   | 8×8        | 768           | 12    | 12     | 3072     | 85M        |\n",
    "| ViT-B/16  | 16×16      | 768           | 12    | 12     | 3072     | 85M        |\n",
    "| ViT-B/32  | 32×32      | 768           | 12    | 12     | 3072     | 85M        |\n",
    "| ViT-L/8   | 8×8        | 1024          | 16    | 24     | 4096     | 307M       |\n",
    "| ViT-L/16  | 16×16      | 1024          | 16    | 24     | 4096     | 307M       |\n",
    "| ViT-L/32  | 32×32      | 1024          | 16    | 24     | 4096     | 307M       |\n",
    "| ViT-H/8   | 8×8        | 1280          | 16    | 32     | 5120     | 632M       |\n",
    "| ViT-H/16  | 16×16      | 1280          | 16    | 32     | 5120     | 632M       |\n",
    "| ViT-H/32  | 32×32      | 1280          | 16    | 32     | 5120     | 632M       |\n",
    "\n",
    "#### 5.6 Preview\n",
    "\n",
    "Understanding how information flows through Vision Transformers compared to CNNs is crucial for interpreting their behavior. The concept of \"attention distance\" in ViTs is analogous to receptive field size in CNNs but shows fundamentally different patterns of information integration.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/attention_metrics.PNG\" width=\"500\"/>\n",
    "    <p><i>Figure 6: Attention distance of ViT-L/32 model. Source: [1]</i></p>\n",
    "</div>\n",
    "\n",
    "Unlike CNNs, where receptive fields grow gradually through the network hierarchy, attention heads in ViTs display a unique pattern of information processing (as shown in Figure 6):\n",
    "\n",
    "- **Parallel local-global processing**: Even in early layers, ViT has some attention heads with very large attention distances (processing global context) while others focus locally - enabling simultaneous processing of both broader context and fine details.\n",
    "- **Heterogeneous attention specialization**: Different attention heads within the same layer specialize in different spatial relationships, with some focusing on adjacent patches and others connecting distant regions of the image.\n",
    "- **Rapid transition to global processing**: By the middle layers, most attention heads operate globally, unlike CNNs which require many more layers to achieve the same receptive field size.\n",
    "\n",
    "This multi-scale parallel processing appears to be a key advantage of the Transformer architecture, allowing it to build rich representations that combine local features with global context in ways fundamentally different from the strictly hierarchical processing in CNNs. For a detailed overview, check out [\"Do Vision Transformers See Like Convolutional Neural Networks?\"](https://proceedings.neurips.cc/paper_files/paper/2021/hash/652cf38361a209088302ba2b8b7f51e0-Abstract.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f00a4ebd24afe",
   "metadata": {},
   "source": [
    "#### 5.7 Summary\n",
    "\n",
    "> **Are Vision Transformers inherently better than CNNs?**\n",
    "\n",
    "When we examine the original ViT [[1](#6-references)], we find that the answer isn't straightforward. If ViT is initialized randomly (without pre-training), it requires an enormous dataset to achieve better performance than CNNs. As shown in Figure 7, only when trained on the JFT dataset (which contains over 100M images and is not publicly available), ViT does slightly outperform CNNs. For \"smaller\" datasets, CNNs still maintain an advantage.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/comparison_CNNs.png\" width=\"1000\"/>\n",
    "    <p><i>Figure 7: Scaling of performance between ResNets (=BiT) and ViTs with dataset size. Source: [1]</i></p>\n",
    "</div>\n",
    "\n",
    "> **But why is the ViT so widely used nowadays?**\n",
    "\n",
    "There are several key factors that have driven ViT adoption despite the data-hungry nature of the architecture:\n",
    "\n",
    "1. **Pre-trained foundation models**: Large tech companies like Google and Meta have released pre-trained ViT models (trained on massive datasets like JFT-300M or ImageNet-21k). These models demonstrate exceptional transfer learning capabilities - they can be fine-tuned on smaller datasets with significantly better performance than training from scratch. Research shows ViTs can outperform CNNs in transfer learning scenarios, particularly as model size increases ([Steiner et al., 2021](https://arxiv.org/abs/2106.10270)).\n",
    "\n",
    "\n",
    "2. **Architectural improvements**: Many variants of the original ViT have been developed to improve efficiency and performance on smaller datasets:\n",
    "   - **DeiT** (Data-efficient image Transformers): Uses knowledge distillation to train more efficiently\n",
    "   - **Swin Transformer**: Introduces hierarchical structure and local attention to improve efficiency (see [section 5.1](#51-multi-head-self-attention-msa))\n",
    "   - **CCT** (Compact Convolutional Transformer): Combines convolutional layers with Transformers for better inductive bias. See our Tutorial 4.2 for detailed information!\n",
    "   - Check out [Han et al. (2022)](https://ieeexplore.ieee.org/abstract/document/9716741) for a comprehensive survey on Vision Transformers\n",
    "\n",
    "3. **Interpretability**: ViTs provide a more transparent view of their decision-making process through attention maps, allowing to better understand which parts of an image influence classification decisions. In [[1](#6-references)] the authors determine the attention maps of the ViT-L/32 model via Attention Rollouts [[4](#6-references)]. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figures/attention_map.PNG\" width=\"250\"/>\n",
    "    <p><i>Figure 8: Attention maps of ViT-L/32 model. Source: [1]</i></p>\n",
    "</div>\n",
    "\n",
    "Implementing attention rollouts is straightforward if you have access to the attention weights of the model. So this will be a nice exercise for you to implement it yourself. \n",
    "\n",
    "> **But I want to learn from scratch!**\n",
    "\n",
    "If you are interested in learning how to train a ViT from scratch, check out [\"How to train your ViT?\"](https://arxiv.org/abs/2106.10270) for appropriate data augmentation and other tricks. For example you can also use DropPath like in our ResNet Tutorial (2.4) as a regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c599b96f81c6e81",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "[1] [Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *International Conference on Learning Representations (ICLR)*.](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "[2] [https://www.v7labs.com/blog/vision-transformer-guide/](https://www.v7labs.com/blog/vision-transformer-guide/)\n",
    "\n",
    "[3] [https://medium.com/@sanjithkumar986/vision-transformers-1-19e4c052aab9](https://medium.com/@sanjithkumar986/vision-transformers-1-19e4c052aab9)\n",
    "\n",
    "[4] [Abnar, S., & Zuidema, W. (2020). Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928.](https://arxiv.org/abs/2005.00928)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a792d13c70bf6",
   "metadata": {},
   "source": [
    "## Training and evaluation of ViT on the Imagenette\n",
    "\n",
    "So let's get to action and train the ViT on a dataset. You can adjust hyperparameters like the learning rate, batch size, and number of epochs in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1e95619593777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "from Utils.dataloaders import prepare_imagenette\n",
    "\n",
    "# define hyperparameters\n",
    "batch_size = 256\n",
    "patch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "transform_augm = transforms.Compose([\n",
    "    v2.ToImage(),\n",
    "    # Core transformations\n",
    "    v2.RandomResizedCrop(size=224, scale=(0.75, 1.0), ratio=(0.9, 1.05)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),  # People can face either direction\n",
    "    v2.RandomRotation(degrees=(-10, 10)),  # Small rotations\n",
    "    \n",
    "    # Lighting and appearance variations\n",
    "    v2.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.05),\n",
    "    v2.RandomAutocontrast(p=0.2),\n",
    "    \n",
    "    # Occasional realistic variations - with proper probability handling\n",
    "    v2.RandomApply([v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),\n",
    "    v2.RandomPerspective(distortion_scale=0.15, p=0.3),\n",
    "    v2.RandomErasing(p=0.1, scale=(0.02, 0.08), ratio=(0.3, 3.3)),\n",
    "    \n",
    "    # Normalization\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_norm = transforms.Compose(\n",
    "[    v2.ToImage(),\n",
    "     v2.ToDtype(torch.float32, scale=True),\n",
    "     v2.Resize(size=(224,224)),\n",
    "     v2.Normalize(mean = [0.485, 0.456, 0.406], std=[0.229,0.224,0.225]) , \n",
    "])\n",
    "\n",
    "# Load the Imagenette dataset\n",
    "train_loader, val_loader, classes = prepare_imagenette(train_compose=transform_augm, \n",
    "                                                       test_compose=transform_norm, \n",
    "                                                       save_path='../Dataset',\n",
    "                                                       batch_size=batch_size, \n",
    "                                                       num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc1892164a196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "num_classes = len(classes)\n",
    "img_size = images.shape[3]\n",
    "in_chans = images.shape[1]\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Image size: {img_size}\")\n",
    "print(f\"Number of channels: {in_chans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bf799c4d3d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init ViT model\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    in_chans=in_chans,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=384,  # ViT-B/16 uses standard 768\n",
    "    depth=12,  # ViT-B/16 uses standard 12\n",
    "    num_heads=6,  # ViT-B/16 uses standard 12\n",
    "    mlp_ratio=4.0,  # ViT-B/16 uses standard 4.0\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    ")\n",
    "\n",
    "# model summary\n",
    "print(vit_model)\n",
    "\n",
    "# print number of trainable parameters\n",
    "from Utils.little_helpers import get_parameters\n",
    "print(f\"Number of trainable parameters: {get_parameters(vit_model):.3f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effdc63eed6aee2",
   "metadata": {},
   "source": [
    "Like in [[1](#6-references)] we will use a linear warm up and cosine annealing decay schedule. The learning rate is increased linearly from `min_lr` to the initial learning rate over the amount of `warmup_epochs`. After that, it is decreased using cosine annealing. See notebook '04_1_WarmUpSchedular.ipynb' for a detailed explanation of the learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed39a439441917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR, ChainedScheduler\n",
    "\n",
    "warmup_epochs = 4  # 5-10% of training epochs\n",
    "num_epochs = 40 \n",
    "\n",
    "warmup_lr_init = 1e-6\n",
    "base_lr = 1e-4\n",
    "min_lr = 1e-7\n",
    "\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=base_lr, weight_decay=0.05)  # test different values for weight decay (0.05-0.1) and learning rate (1e-3 - 1e-4)\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=warmup_lr_init/base_lr,\n",
    "    end_factor=1.0, \n",
    "    total_iters=warmup_epochs\n",
    ")\n",
    "\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=num_epochs, \n",
    "    eta_min=min_lr\n",
    ")\n",
    "\n",
    "# combine schedulers with ChainedScheduler\n",
    "scheduler = ChainedScheduler(\n",
    "    schedulers=(warmup_scheduler, cosine_scheduler),\n",
    "    optimizer=optimizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16a8528b325928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functions import train_model\n",
    "\n",
    "# save results + model checkpoints into ...\n",
    "results_folder = f'vit_results/'\n",
    "\n",
    "# train model\n",
    "with timer(\"Training process\"):\n",
    "    history = train_model(model=vit_model,\n",
    "                          train_loader=train_loader,\n",
    "                          val_loader=val_loader,\n",
    "                          criterion=nn.CrossEntropyLoss(label_smoothing=0.0),  # label smoothing to reduce overfitting. Proper values are 0.01 to 0.2\n",
    "                          optimizer=optimizer,\n",
    "                          scheduler=scheduler,\n",
    "                          patience=5,  # number of epochs to wait before early stopping \n",
    "                          monitor='val_loss',  # metric to monitor for early stopping. If this metric doesn't improve for the last `patience` epochs, early stopping is triggered\n",
    "                          device=device,\n",
    "                          num_epochs=num_epochs,\n",
    "                          checkpoint_path=results_folder,  # path to save the \"best\" model with best value of `monitor`\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bc9f95defc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plotting import visualize_training_results\n",
    "\n",
    "# save history\n",
    "np.save(os.path.join(results_folder, 'history.npy'), history, allow_pickle=True)\n",
    "\n",
    "visualize_training_results(train_losses=history['train_loss'],\n",
    "                           train_accs=history['train_acc'],\n",
    "                           test_losses=history['val_loss'],\n",
    "                           test_accs=history['val_acc'],\n",
    "                           output_dir=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936ecae1bf99f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.functions import test_model\n",
    "from Utils.plotting import visualize_test_results\n",
    "\n",
    "# evaluate model on validation set\n",
    "with timer(\"Evaluating process\"):\n",
    "    aggregate_df, per_image_df, overall_accuracy = test_model(model=vit_model,\n",
    "                                                              test_loader=val_loader,\n",
    "                                                              device=device,\n",
    "                                                              class_names=classes,\n",
    "                                                              print_per_class_summary=False)\n",
    "\n",
    "\n",
    "# save dataframes as parquet (requires pyarrow and fastparquet)\n",
    "try:\n",
    "    aggregate_df.to_parquet(os.path.join(results_folder, 'aggregate_df.parquet'))\n",
    "    per_image_df.to_parquet(os.path.join(results_folder, 'per_image_df.parquet'))\n",
    "except ImportError:\n",
    "    aggregate_df.to_pickle(os.path.join(results_folder, 'aggregate_df.pkl'))\n",
    "    per_image_df.to_pickle(os.path.join(results_folder, 'per_image_df.pkl'))\n",
    "    \n",
    "visualize_test_results(aggregate_df=aggregate_df,\n",
    "                       per_image_df=per_image_df,\n",
    "                       overall_accuracy=overall_accuracy,\n",
    "                       max_classes_display=10,\n",
    "                       output_dir=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244664c6be4f2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show patch embeddings\n",
    "from Utils.plotting import visualize_patch_embeddings\n",
    "\n",
    "visualize_patch_embeddings(model=vit_model,\n",
    "                           num_components=28,\n",
    "                           output_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b2d6d0488ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show position embeddings\n",
    "from Utils.plotting import visualize_position_embeddings\n",
    "\n",
    "visualize_position_embeddings(model=vit_model, output_dir=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45283dbf7a291c77",
   "metadata": {},
   "source": [
    "As we can see from our results, the ViT performs worse than ResNet and even simple CNNs when trained from scratch on our dataset. When we examine the patch and positional embeddings, we can see clear differences between those shown in [[1](#6-references)] and our implementation. The author of this notebook wants to make it clear that this is not due to a flawed implementation but simply due to the fact that our dataset wasn't big and diverse enough to learn meaningful representations.\n",
    "\n",
    "This observation aligns with [[1](#6-references)]: Vision Transformers lack the inductive biases that are inherently built into CNNs. Convolutional architectures have translation equivariance and locality baked into their design, which helps them learn efficiently from limited data. In contrast, our ViT must learn these spatial relationships entirely from the data itself.\n",
    "\n",
    "Without these built-in structural priors, Transformers require substantially more training examples to achieve comparable performance. This is why the original ViT paper demonstrated that these models only outperform CNNs when pre-trained on very large datasets like JFT-300M. On smaller datasets, the lack of inductive bias becomes a disadvantage, resulting in poorer generalization. \n",
    "\n",
    "This trade-off represents a fundamental principle in deep learning: models with fewer inductive biases can potentially learn more flexible representations, but only when provided with sufficient data to compensate for the absence of built-in priors. Pre-trained ViT models leverage this by learning general visual representations from massive datasets before being fine-tuned on smaller, task-specific datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd5b6c99e70004",
   "metadata": {},
   "source": [
    "## 1. Exercise: Visualize the patch and positional embeddings of a pre-trained ViT\n",
    "\n",
    "In this exercise, you will visualize the patch and positional embeddings of a pre-trained Vision Transformer (ViT) model from `timm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d5139b835b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all references to the model and optimizer\n",
    "del vit_model, optimizer\n",
    "if torch.device == 'cuda':\n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary())\n",
    "\n",
    "# Garbage collect to free memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c90c30939bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# Load a pretrained ViT model\n",
    "# You can change this to any other ViT variant. Have a look at https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py or\n",
    "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/tiny_vit.py\n",
    "model_name = 'vit_base_patch32_224.orig_in21k'  \n",
    "\n",
    "# initialize the model\n",
    "pretrained_model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "# TODO: Implement training and eval pipelines. Tip: Use the `train_model` and `test_model` functions from the Utils folder, as well as the already initialized `train_loader` and `val_loader` from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4b1dc001b12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plotting import visualize_patch_embeddings, visualize_position_embeddings\n",
    "\n",
    "# TODO: Visualize the patch and position embeddings with the `visualize_patch_embeddings` and `visualize_position_embeddings` functions from the Utils folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344384e496d1095",
   "metadata": {},
   "source": [
    "> Note: These functions are using predefined attributes to access the patch and positional embeddings (like `model.patch_embed` and `model.pos_embed`). These attributes might be different for the model you are using (like in `torchvision`). So check the model architecture and adapt the code accordingly if you are using another library than `timm`.\n",
    "\n",
    "> After the first run, try increasing the weight decay in the optimizer, initialize the pre-trained model and train again. Look at the positional embeddings. What do you observe?\n",
    "\n",
    "> Imagenette is a subset of ImageNet on which your pre-trained ViT model was probably trained on. Most of the pre-trained model achieve an accuracy of around 85% on ImageNet. But why do you start with a lower accuracy than that? What could be the reasons for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f94f45590366b",
   "metadata": {},
   "source": [
    "## 2. Exercise: Plot the attention maps of the pre-trained ViT for a random Imagenette Picture\n",
    "\n",
    "In this exercise, you will implement the attention rollout technique to visualize how a pre-trained Vision Transformer (ViT) attends to different parts of an image. This technique, introduced by [[4](#6-references)], helps us understanding how information propagates through the self-attention layers of a Transformer. \n",
    "\n",
    "You will:\n",
    "\n",
    "- Use the pre-trained ViT model from `timm` from Exercise 1\n",
    "- Recompute the attention from all layers (use the `attention_hook` function). Note that you can also use the `AttentionExtractor` class from [`timm`](https://github.com/huggingface/pytorch-image-models/blob/main/timm/utils/attention_extract.py).\n",
    "- Implement the attention rollout algorithm **yourself** (see [[4](#6-references)] for details)\n",
    "- Visualize attention maps on random Imagenette images\n",
    "- Compare raw attention to attention rollout\n",
    "\n",
    "### Function to extract attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7df1069d1d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(model: nn.Module, \n",
    "                  img: torch.Tensor,) -> List[torch.Tensor]:\n",
    "    \n",
    "    attention_maps = []\n",
    "    def attention_hook(module: nn.Module, \n",
    "                       input: torch.Tensor, \n",
    "                       output: torch.Tensor) -> torch.Tensor:\n",
    "        # This is executed during the forward pass through an attention layer\n",
    "        # We need to add this inside an attention module to capture when attn is computed\n",
    "        \n",
    "        # Get the input to the attention module\n",
    "        x = input[0]\n",
    "        \n",
    "        # Get qkv projections\n",
    "        qkv = module.qkv(x)\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Reshape qkv to separate heads\n",
    "        qkv = qkv.reshape(B, N, 3, module.num_heads, C // module.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)  # Unbind along first dimension\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Store the attention weights\n",
    "        attention_maps.append(attn.detach())\n",
    "        \n",
    "        # Let the original forward pass continue\n",
    "        return output\n",
    "    \n",
    "    # Register hooks for all attention modules\n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hooks.append(block.attn.register_forward_hook(attention_hook))\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        model(img)\n",
    "    \n",
    "    # Remove hooks after forward pass\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Post-process attention maps\n",
    "    # TODO: This is your task now!\n",
    "    # 1. Average or Max over the attention heads (have look how the approaches differ)\n",
    "    # 2. Add residual connection (0.5 * attention + 0.5 * identity)\n",
    "    # 3. Normalize the attention maps\n",
    "    \n",
    "    processed_maps = []\n",
    "    for attn in attention_maps:\n",
    "        pass\n",
    "    return processed_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa9f35e2d4c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(attention_maps: List[torch.Tensor],) -> torch.Tensor:\n",
    "    # TODO: Recursively compute rollout through all layers\n",
    "    # A_roll(l) = A(l) * A_roll(l-1)\n",
    "    # normalize the final attention map\n",
    "    rollout = None\n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce2f25da2114bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(img_tensor: torch.Tensor, \n",
    "                        attention_map: List[torch.Tensor], \n",
    "                        save_path: Optional[str] = None):\n",
    "    \n",
    "    # TODO: Implement the visualization of attention maps\n",
    "    # Remember to resize the attention map to the original image size. Use a bilinear interpolation or something similar\n",
    "    # Also remember to denormalize the image for visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716d588eb4727e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get a random image from the dataset\n",
    "# For example, you can use the first image in the batch\n",
    "\n",
    "# Extract attention matrices\n",
    "pretrained_model.to(\"cpu\") # Move model to CPU to make data processing easier\n",
    "\n",
    "# TODO: Implement post-processing of attention matrices\n",
    "attention_matrices = get_attention(pretrained_model, img)\n",
    "\n",
    "# TODO: Implement attention rollout\n",
    "rollout = attention_rollout(attention_matrices)\n",
    "\n",
    "# Visualize raw attention vs attention rollout\n",
    "# First, visualize raw attention from the last layer\n",
    "raw_attention = attention_matrices[-1]\n",
    "visualize_attention(img, raw_attention, save_path=None)\n",
    "\n",
    "# Then, visualize attention rollout\n",
    "visualize_attention(img, rollout, save_path=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764d390f79fda42",
   "metadata": {},
   "source": [
    "Your main task is to complete the `get_attention`, `attention_rollout` and `visualize_attention` functions. \n",
    "\n",
    "For a ViT with L layers, attention rollout is computed as:\n",
    "```\n",
    "A_rollout(0) = A(0)  # First layer's attention\n",
    "A_rollout(i) = A(i) @ A_rollout(i-1)  # For i=1 to L-1\n",
    "```\n",
    "\n",
    "Where @ represents matrix multiplication. The idea is to track how attention propagates from the input tokens through all layers of the Transformer.\n",
    "\n",
    "In [[4](#6-references)] the authors also propose attention flow, which is a different way to visualize attention. If you are interested in this, you can implement it as well. The main difference is that attention flow treats the attention graph as a flow network with capacities, using maximum flow algorithms from graph theory instead of matrix multiplication. While attention rollout multiplies attention weights along paths (treating them as proportion factors), attention flow finds the maximum possible flow from any layer to input tokens by treating attention weights as capacity constraints. This results in more distributed attention patterns that better correlate with ground truth importance scores, though at higher computational cost ($O(d^2 \\cdot n^4)$ vs $O(d \\cdot n^2)$ for rollout)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
