{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8eb81cea149cfe",
   "metadata": {},
   "source": [
    "# Tutorial-Extra 4.1: Learning Rate Warmup in Transformer Training\n",
    "\n",
    "Author: [Erik Syniawa](mailto:erik.syniawa@informatik.tu-chemnitz.de)\n",
    "\n",
    "## 1. Understanding Learning Rate Warmup\n",
    "\n",
    "Learning rate warmup is a critical technique in training Transformer models, including Vision Transformers (ViT). It involves starting with a very small learning rate and gradually increasing it to a predefined \"base\" learning rate over a specified number of steps or epochs before applying any decay schedule.\n",
    "\n",
    "### Why Warmup is important for Transformers\n",
    "\n",
    "Transformers are particularly sensitive to the learning rate during initial training for several reasons:\n",
    "\n",
    "1. **Random Initialization Sensitivity**: Transformers start with randomly initialized attention mechanisms that need time to form meaningful patterns.\n",
    "\n",
    "2. **Gradient Instability**: Early in training, large gradients combined with high learning rates can cause the model to diverge.\n",
    "\n",
    "3. **Self-Attention Stabilization**: The self-attention mechanism needs time to learn which features are relevant before taking large optimization steps.\n",
    "\n",
    "4. **Complex Loss Landscape**: Transformer architectures create a complex loss landscape where early optimization steps need to be more cautious.\n",
    "\n",
    "5. **High Parameter Count**: With millions of parameters, Transformers benefit from a more controlled start to training.\n",
    "\n",
    "## 2. Learning Rate Dynamics: Within and Across Epochs\n",
    "\n",
    "Let's visualize how learning rates change throughout training with warmup and subsequent scheduling.\n",
    "\n",
    "### Key Components of a Learning Rate Schedule\n",
    "\n",
    "1. **Warmup Phase**: Gradual increase from a small initial learning rate to the base learning rate\n",
    "2. **Decay Phase**: Gradual decrease following a specific schedule (e.g., cosine, linear, step)\n",
    "3. **Minimum Learning Rate**: The lower bound below which the learning rate won't decrease\n",
    "\n",
    "### Visualization of Learning Rate Changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ca5be9d050fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:38:24.260788Z",
     "start_time": "2025-04-22T13:38:20.452119Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, ChainedScheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a model and optimizer\n",
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = DummyModel()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define parameters\n",
    "total_epochs = 100\n",
    "warmup_epochs = 20\n",
    "steps_per_epoch = 50\n",
    "base_lr = 1e-3\n",
    "min_lr = 1e-6\n",
    "warmup_start_lr = 1e-6\n",
    "\n",
    "# Create schedulers\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=warmup_start_lr/base_lr,\n",
    "    end_factor=1.0, \n",
    "    total_iters=warmup_epochs\n",
    ")\n",
    "\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=total_epochs, \n",
    "    eta_min=min_lr\n",
    ")\n",
    "\n",
    "if torch.__version__ >= '2.6':\n",
    "    print(\"Using ChainedScheduler for PyTorch 2.6 and onward\")\n",
    "    \n",
    "    # ChainedScheduler is available in PyTorch 2.6 and onward\n",
    "    # It allows you to combine multiple schedulers\n",
    "    # Note: ChainedScheduler is available in earlier versions of PyTorch but takes different arguments\n",
    "    \n",
    "    scheduler = ChainedScheduler(\n",
    "        schedulers=(warmup_scheduler, cosine_scheduler),\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "\n",
    "# if you are using an older version of PyTorch, you may need to use the following instead:\n",
    "elif torch.__version__ < '2.6':\n",
    "    from torch.optim.lr_scheduler import SequentialLR\n",
    "    print(\"Using SequentialLR for older PyTorch versions\")\n",
    "    \n",
    "    cosine_scheduler = CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=total_epochs - warmup_epochs,  # adjust T_max for the remaining epochs \n",
    "        eta_min=min_lr)\n",
    "    \n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_epochs],)\n",
    "# SequentialLR is deprecated in PyTorch 2.6 and onward due to epoch-based scheduling (scheduler.step(epoch) instead of scheduler.step())\n",
    "\n",
    "# Track learning rates across epochs\n",
    "epoch_lrs = []\n",
    "epoch_lrs.append(warmup_start_lr)\n",
    "\n",
    "for epoch in range(1, total_epochs):  # Start from 1 since we already recorded epoch 0\n",
    "    dummy_input = torch.randn(32, 10)\n",
    "    dummy_output = model(dummy_input)\n",
    "    dummy_loss = dummy_output.sum()\n",
    "    optimizer.zero_grad()\n",
    "    dummy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_lrs.append(scheduler.get_last_lr()[0])  # Get the current learning rate\n",
    "\n",
    "# Function to simulate learning rate changes within an epoch\n",
    "def lr_within_epoch(epoch, steps_per_epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        # For the warmup phase\n",
    "        if epoch == 0:\n",
    "            # For epoch 0, start at exactly warmup_start_lr\n",
    "            start_lr = warmup_start_lr\n",
    "            end_lr = epoch_lrs[0]\n",
    "        else:\n",
    "            # For other warmup epochs, interpolate between recorded values\n",
    "            start_lr = epoch_lrs[epoch-1]\n",
    "            end_lr = epoch_lrs[epoch]\n",
    "            \n",
    "        return [start_lr + (end_lr - start_lr) * (step / steps_per_epoch) \n",
    "                for step in range(steps_per_epoch)]\n",
    "    else:\n",
    "        # Constant within an epoch after warmup\n",
    "        return [epoch_lrs[epoch]] * steps_per_epoch\n",
    "\n",
    "# Plot learning rate across epochs\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Learning Rate across all epochs\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(total_epochs), epoch_lrs)\n",
    "plt.axvline(x=warmup_epochs, color='r', linestyle='--', \n",
    "           label=f'End of Warmup ({warmup_epochs} epochs)')\n",
    "plt.title('Learning Rate Schedule Across Training', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.ylim(warmup_start_lr * 0.8, base_lr * 1.2)  # Set limits to ensure we see the full range\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "\n",
    "# Add explicit tick at warmup_start_lr to confirm the value\n",
    "plt.yticks(list(plt.yticks()[0]) + [warmup_start_lr])\n",
    "\n",
    "# Annotate the starting point to make it clear\n",
    "plt.annotate(f'Start: {warmup_start_lr:.1e}', \n",
    "             xy=(0, warmup_start_lr),\n",
    "             xytext=(5, warmup_start_lr*2),\n",
    "             arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Learning Rate within specific epochs\n",
    "plt.subplot(2, 1, 2)\n",
    "within_epochs = [0, 5, warmup_epochs, 50, total_epochs-1]  # Selected epochs\n",
    "for epoch in within_epochs:\n",
    "    lrs = lr_within_epoch(epoch, steps_per_epoch)\n",
    "    label = 'Warmup' if epoch < warmup_epochs else 'Decay'\n",
    "    if epoch == warmup_epochs:\n",
    "        label = 'Transition'\n",
    "    plt.plot(range(steps_per_epoch), lrs,   \n",
    "             label=f'Epoch {epoch} ({label})')\n",
    "\n",
    "plt.title('Learning Rate Within Selected Epochs', fontsize=14)\n",
    "plt.xlabel('Step within Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ee6df8c304daa",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Different Types of Warmup and Decay Schedules\n",
    "\n",
    "### Common Warmup Strategies\n",
    "\n",
    "1. **Linear Warmup**: The most common approach, where the learning rate increases linearly from initial to base value (`LinearLR`). You can also use `StepLR` for a stepwise increase with a given ratio ($\\gamma > 1.0$).\n",
    "2. **Exponential Warmup**: Learning rate increases exponentially, starting slow and accelerating (`LambdaLR`).\n",
    "\n",
    "### Common Decay Strategies After Warmup\n",
    "\n",
    "1. **Cosine Decay**: Follows a cosine curve, decreasing more slowly at the beginning and end (`CosineAnnealingLR`).\n",
    "2. **Linear Decay**: Linear decrease from base learning rate to minimum (`LinearLR`).\n",
    "3. **Step Decay**: Learning rate drops by a factor at specific milestones (`StepLR` with $\\gamma < 1.0$).\n",
    "4. **Exponential Decay**: Learning rate decreases by a multiplicative factor each step (`ExponentialLR`).\n",
    "\n",
    "Let's visualize some of these combinations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148abd99751fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:20:58.526191Z",
     "start_time": "2025-04-22T11:20:58.363243Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr_schedule(schedule_type: str, \n",
    "                    total_steps: int, \n",
    "                    warmup_steps: int, \n",
    "                    base_lr: float = 1.0, \n",
    "                    min_lr: float = 1e-7, \n",
    "                    warmup_start_lr: float = 1e-6):\n",
    "    \n",
    "    lrs = []\n",
    "    for step in range(total_steps):\n",
    "        if step < warmup_steps:\n",
    "            # Warmup phase\n",
    "            if 'linear_warmup' in schedule_type:\n",
    "                lr = warmup_start_lr + (base_lr - warmup_start_lr) * (step / warmup_steps)\n",
    "            elif 'exp_warmup' in schedule_type:\n",
    "                # Exponential warmup\n",
    "                warmup_factor = np.exp(np.log(base_lr/warmup_start_lr) * step / warmup_steps)\n",
    "                lr = warmup_start_lr * warmup_factor\n",
    "        else:\n",
    "            # Decay phase\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            \n",
    "            if 'cosine' in schedule_type:\n",
    "                lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
    "            elif 'linear' in schedule_type:\n",
    "                lr = base_lr - (base_lr - min_lr) * progress\n",
    "            elif 'step' in schedule_type:\n",
    "                # Step decay at 1/3 and 2/3 of decay period\n",
    "                if progress < 1/3:\n",
    "                    lr = base_lr\n",
    "                elif progress < 2/3:\n",
    "                    lr = base_lr * 0.1\n",
    "                else:\n",
    "                    lr = base_lr * 0.01\n",
    "            elif 'exp' in schedule_type:\n",
    "                decay_rate = np.power(min_lr / base_lr, 1 / (total_steps - warmup_steps))\n",
    "                lr = base_lr * (decay_rate ** (step - warmup_steps))\n",
    "        \n",
    "        lrs.append(lr)\n",
    "    \n",
    "    return lrs\n",
    "\n",
    "# Parameters\n",
    "total_steps = 500\n",
    "warmup_steps = 50\n",
    "schedules = [\n",
    "    'linear_warmup_cosine',\n",
    "    'linear_warmup_linear',\n",
    "    'linear_warmup_step',\n",
    "    'exp_warmup_cosine'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for schedule in schedules:\n",
    "    lrs = get_lr_schedule(schedule, total_steps, warmup_steps)\n",
    "    plt.plot(lrs, label=schedule, alpha=0.7)\n",
    "\n",
    "plt.axvline(x=warmup_steps, color='r', linestyle='--', label='End of Warmup')\n",
    "plt.xlabel('Training Steps', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Different Warmup and Decay Schedules', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4016c490262cca",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Per-Step vs. Per-Epoch Updates\n",
    "\n",
    "It's important to understand when to update your learning rate scheduler:\n",
    "\n",
    "### Per-Epoch Updates\n",
    "- The scheduler is stepped once per epoch after all batches have been processed\n",
    "- More common in standard training setups\n",
    "- Easier to implement and track\n",
    "- Works well when epochs have consistent numbers of batches\n",
    "\n",
    "```python\n",
    "# Per-epoch update\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Training steps\n",
    "        ...\n",
    "    \n",
    "    # Update learning rate once per epoch\n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "### Per-Step Updates\n",
    "- The scheduler is stepped after each batch/optimization step\n",
    "- More precise control over learning rate changes\n",
    "- Better for very large datasets where epochs contain many batches\n",
    "- Common in large-scale Transformer pretraining (e.g., BERT, GPT)\n",
    "\n",
    "```python\n",
    "# Per-step update\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Training steps\n",
    "        ...\n",
    "        \n",
    "        # Update learning rate after each batch\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "> If you're using per-step updates, you'll need to adjust your warmup period to be in steps rather than epochs!\n",
    "\n",
    "## 5. Recommended Warmup Settings for Different Scenarios\n",
    "\n",
    "\n",
    "| Scenario | Warmup | Base LR | Min LR | Decay | See |\n",
    "|----------|--------|---------|--------|-------|-----|\n",
    "| **ViT on small datasets** | 5% of epochs | 1e-4 to 3e-4 | 1e-6 | Cosine | Steiner et al. (2021) |\n",
    "| **ViT on ImageNet** | 10-20 epochs | 3e-3 to 5e-3 | 1e-5 | Cosine | Dosovitskiy et al. (2020) |\n",
    "| **Self-supervised ViT** | 10% of epochs | 5e-4 | 1e-6 | Cosine | Caron et al. (2021) |\n",
    "| **Fine-tuning pretrained ViT** | 2-5% of epochs | 1e-5 to 5e-5 | 1e-7 | Linear | Touvron et al. (2021) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e41da",
   "metadata": {},
   "source": [
    "\n",
    "> Note: These are general guidelines and may need to be adjusted based on your specific dataset, model, and training setup.\n",
    "\n",
    "### 5.1 References\n",
    "\n",
    "1. Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" ICLR 2021\n",
    "2. Caron, M., et al. (2021). \"Emerging Properties in Self-Supervised Vision Transformers.\" ICCV 2021\n",
    "3. Touvron, H., et al. (2021). \"Training data-efficient image transformers & distillation through attention.\" ICML 2021\n",
    "4. Steiner, A., et al. (2021). \"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers.\" arXiv:2106.10270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2422c5ad76d5e24",
   "metadata": {},
   "source": [
    "# 6. Custom Warmup Scheduler Implementation \n",
    "\n",
    "Some architectures or training setups may require custom schedulers not just limited to the learning rate but also for other hyperparameters. For example DINO uses a scheduler for the temperature parameter for the teacher and for the weight decay regularization in the optimizer. Below is an example of a custom warmup scheduler that can be used in PyTorch for weight decay:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9b152facbd29d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:20:58.701318Z",
     "start_time": "2025-04-22T11:20:58.691984Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class CosineWeightDecayScheduler:\n",
    "    def __init__(self, \n",
    "                 optimizer: optim.Optimizer, \n",
    "                 initial_wd: float, \n",
    "                 final_wd: float, \n",
    "                 total_epochs: int):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.initial_wd = initial_wd\n",
    "        self.final_wd = final_wd\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def step(self):\n",
    "        # Cosine schedule from initial_wd to final_wd\n",
    "        factor = 0.5 * (1 + np.cos(np.pi * self.current_epoch / self.total_epochs))\n",
    "        current_wd = self.final_wd + (self.initial_wd - self.final_wd) * factor\n",
    "        \n",
    "        # Update weight decay in optimizer\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['weight_decay'] = current_wd\n",
    "            \n",
    "        self.current_epoch += 1\n",
    "        return current_wd\n",
    "    \n",
    "# Example usage\n",
    "lr = 1e-3\n",
    "initial_wd = 1e-3\n",
    "final_wd = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=initial_wd)\n",
    "lr_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "# Setup weight decay scheduler\n",
    "wd_scheduler = CosineWeightDecayScheduler(\n",
    "    optimizer,\n",
    "    initial_wd=initial_wd,\n",
    "    final_wd=final_wd,\n",
    "    total_epochs=num_epochs\n",
    ")\n",
    "\n",
    "# In your training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training code...\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Step both schedulers at the end of each epoch\n",
    "    lr_scheduler.step()\n",
    "    wd_scheduler.step()\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    current_wd = optimizer.param_groups[0]['weight_decay']\n",
    "    print(f\"Epoch {epoch}: LR={current_lr:.6f}, WD={current_wd:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a884d7e378d59",
   "metadata": {},
   "source": [
    "> What could be the benefits of using a custom warmup scheduler for weight decay in your training setup? And why might this approach just suitable in self-supervised learning setups?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
